window.pdocSearch = (function(){
/** elasticlunr - http://weixsong.github.io * Copyright (C) 2017 Oliver Nightingale * Copyright (C) 2017 Wei Song * MIT Licensed */!function(){function e(e){if(null===e||"object"!=typeof e)return e;var t=e.constructor();for(var n in e)e.hasOwnProperty(n)&&(t[n]=e[n]);return t}var t=function(e){var n=new t.Index;return n.pipeline.add(t.trimmer,t.stopWordFilter,t.stemmer),e&&e.call(n,n),n};t.version="0.9.5",lunr=t,t.utils={},t.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),t.utils.toString=function(e){return void 0===e||null===e?"":e.toString()},t.EventEmitter=function(){this.events={}},t.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop(),n=e;if("function"!=typeof t)throw new TypeError("last argument must be a function");n.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},t.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);-1!==n&&(this.events[e].splice(n,1),0==this.events[e].length&&delete this.events[e])}},t.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)},this)}},t.EventEmitter.prototype.hasHandler=function(e){return e in this.events},t.tokenizer=function(e){if(!arguments.length||null===e||void 0===e)return[];if(Array.isArray(e)){var n=e.filter(function(e){return null===e||void 0===e?!1:!0});n=n.map(function(e){return t.utils.toString(e).toLowerCase()});var i=[];return n.forEach(function(e){var n=e.split(t.tokenizer.seperator);i=i.concat(n)},this),i}return e.toString().trim().toLowerCase().split(t.tokenizer.seperator)},t.tokenizer.defaultSeperator=/[\s\-]+/,t.tokenizer.seperator=t.tokenizer.defaultSeperator,t.tokenizer.setSeperator=function(e){null!==e&&void 0!==e&&"object"==typeof e&&(t.tokenizer.seperator=e)},t.tokenizer.resetSeperator=function(){t.tokenizer.seperator=t.tokenizer.defaultSeperator},t.tokenizer.getSeperator=function(){return t.tokenizer.seperator},t.Pipeline=function(){this._queue=[]},t.Pipeline.registeredFunctions={},t.Pipeline.registerFunction=function(e,n){n in t.Pipeline.registeredFunctions&&t.utils.warn("Overwriting existing registered function: "+n),e.label=n,t.Pipeline.registeredFunctions[n]=e},t.Pipeline.getRegisteredFunction=function(e){return e in t.Pipeline.registeredFunctions!=!0?null:t.Pipeline.registeredFunctions[e]},t.Pipeline.warnIfFunctionNotRegistered=function(e){var n=e.label&&e.label in this.registeredFunctions;n||t.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},t.Pipeline.load=function(e){var n=new t.Pipeline;return e.forEach(function(e){var i=t.Pipeline.getRegisteredFunction(e);if(!i)throw new Error("Cannot load un-registered function: "+e);n.add(i)}),n},t.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){t.Pipeline.warnIfFunctionNotRegistered(e),this._queue.push(e)},this)},t.Pipeline.prototype.after=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i+1,0,n)},t.Pipeline.prototype.before=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i,0,n)},t.Pipeline.prototype.remove=function(e){var t=this._queue.indexOf(e);-1!==t&&this._queue.splice(t,1)},t.Pipeline.prototype.run=function(e){for(var t=[],n=e.length,i=this._queue.length,o=0;n>o;o++){for(var r=e[o],s=0;i>s&&(r=this._queue[s](r,o,e),void 0!==r&&null!==r);s++);void 0!==r&&null!==r&&t.push(r)}return t},t.Pipeline.prototype.reset=function(){this._queue=[]},t.Pipeline.prototype.get=function(){return this._queue},t.Pipeline.prototype.toJSON=function(){return this._queue.map(function(e){return t.Pipeline.warnIfFunctionNotRegistered(e),e.label})},t.Index=function(){this._fields=[],this._ref="id",this.pipeline=new t.Pipeline,this.documentStore=new t.DocumentStore,this.index={},this.eventEmitter=new t.EventEmitter,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},t.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},t.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},t.Index.load=function(e){e.version!==t.version&&t.utils.warn("version mismatch: current "+t.version+" importing "+e.version);var n=new this;n._fields=e.fields,n._ref=e.ref,n.documentStore=t.DocumentStore.load(e.documentStore),n.pipeline=t.Pipeline.load(e.pipeline),n.index={};for(var i in e.index)n.index[i]=t.InvertedIndex.load(e.index[i]);return n},t.Index.prototype.addField=function(e){return this._fields.push(e),this.index[e]=new t.InvertedIndex,this},t.Index.prototype.setRef=function(e){return this._ref=e,this},t.Index.prototype.saveDocument=function(e){return this.documentStore=new t.DocumentStore(e),this},t.Index.prototype.addDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.addDoc(i,e),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));this.documentStore.addFieldLength(i,n,o.length);var r={};o.forEach(function(e){e in r?r[e]+=1:r[e]=1},this);for(var s in r){var u=r[s];u=Math.sqrt(u),this.index[n].addToken(s,{ref:i,tf:u})}},this),n&&this.eventEmitter.emit("add",e,this)}},t.Index.prototype.removeDocByRef=function(e){if(e&&this.documentStore.isDocStored()!==!1&&this.documentStore.hasDoc(e)){var t=this.documentStore.getDoc(e);this.removeDoc(t,!1)}},t.Index.prototype.removeDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.hasDoc(i)&&(this.documentStore.removeDoc(i),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));o.forEach(function(e){this.index[n].removeToken(e,i)},this)},this),n&&this.eventEmitter.emit("remove",e,this))}},t.Index.prototype.updateDoc=function(e,t){var t=void 0===t?!0:t;this.removeDocByRef(e[this._ref],!1),this.addDoc(e,!1),t&&this.eventEmitter.emit("update",e,this)},t.Index.prototype.idf=function(e,t){var n="@"+t+"/"+e;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var i=this.index[t].getDocFreq(e),o=1+Math.log(this.documentStore.length/(i+1));return this._idfCache[n]=o,o},t.Index.prototype.getFields=function(){return this._fields.slice()},t.Index.prototype.search=function(e,n){if(!e)return[];e="string"==typeof e?{any:e}:JSON.parse(JSON.stringify(e));var i=null;null!=n&&(i=JSON.stringify(n));for(var o=new t.Configuration(i,this.getFields()).get(),r={},s=Object.keys(e),u=0;u<s.length;u++){var a=s[u];r[a]=this.pipeline.run(t.tokenizer(e[a]))}var l={};for(var c in o){var d=r[c]||r.any;if(d){var f=this.fieldSearch(d,c,o),h=o[c].boost;for(var p in f)f[p]=f[p]*h;for(var p in f)p in l?l[p]+=f[p]:l[p]=f[p]}}var v,g=[];for(var p in l)v={ref:p,score:l[p]},this.documentStore.hasDoc(p)&&(v.doc=this.documentStore.getDoc(p)),g.push(v);return g.sort(function(e,t){return t.score-e.score}),g},t.Index.prototype.fieldSearch=function(e,t,n){var i=n[t].bool,o=n[t].expand,r=n[t].boost,s=null,u={};return 0!==r?(e.forEach(function(e){var n=[e];1==o&&(n=this.index[t].expandToken(e));var r={};n.forEach(function(n){var o=this.index[t].getDocs(n),a=this.idf(n,t);if(s&&"AND"==i){var l={};for(var c in s)c in o&&(l[c]=o[c]);o=l}n==e&&this.fieldSearchStats(u,n,o);for(var c in o){var d=this.index[t].getTermFrequency(n,c),f=this.documentStore.getFieldLength(c,t),h=1;0!=f&&(h=1/Math.sqrt(f));var p=1;n!=e&&(p=.15*(1-(n.length-e.length)/n.length));var v=d*a*h*p;c in r?r[c]+=v:r[c]=v}},this),s=this.mergeScores(s,r,i)},this),s=this.coordNorm(s,u,e.length)):void 0},t.Index.prototype.mergeScores=function(e,t,n){if(!e)return t;if("AND"==n){var i={};for(var o in t)o in e&&(i[o]=e[o]+t[o]);return i}for(var o in t)o in e?e[o]+=t[o]:e[o]=t[o];return e},t.Index.prototype.fieldSearchStats=function(e,t,n){for(var i in n)i in e?e[i].push(t):e[i]=[t]},t.Index.prototype.coordNorm=function(e,t,n){for(var i in e)if(i in t){var o=t[i].length;e[i]=e[i]*o/n}return e},t.Index.prototype.toJSON=function(){var e={};return this._fields.forEach(function(t){e[t]=this.index[t].toJSON()},this),{version:t.version,fields:this._fields,ref:this._ref,documentStore:this.documentStore.toJSON(),index:e,pipeline:this.pipeline.toJSON()}},t.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},t.DocumentStore=function(e){this._save=null===e||void 0===e?!0:e,this.docs={},this.docInfo={},this.length=0},t.DocumentStore.load=function(e){var t=new this;return t.length=e.length,t.docs=e.docs,t.docInfo=e.docInfo,t._save=e.save,t},t.DocumentStore.prototype.isDocStored=function(){return this._save},t.DocumentStore.prototype.addDoc=function(t,n){this.hasDoc(t)||this.length++,this.docs[t]=this._save===!0?e(n):null},t.DocumentStore.prototype.getDoc=function(e){return this.hasDoc(e)===!1?null:this.docs[e]},t.DocumentStore.prototype.hasDoc=function(e){return e in this.docs},t.DocumentStore.prototype.removeDoc=function(e){this.hasDoc(e)&&(delete this.docs[e],delete this.docInfo[e],this.length--)},t.DocumentStore.prototype.addFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&(this.docInfo[e]||(this.docInfo[e]={}),this.docInfo[e][t]=n)},t.DocumentStore.prototype.updateFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&this.addFieldLength(e,t,n)},t.DocumentStore.prototype.getFieldLength=function(e,t){return null===e||void 0===e?0:e in this.docs&&t in this.docInfo[e]?this.docInfo[e][t]:0},t.DocumentStore.prototype.toJSON=function(){return{docs:this.docs,docInfo:this.docInfo,length:this.length,save:this._save}},t.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[^aeiou]",i="[aeiouy]",o=n+"[^aeiouy]*",r=i+"[aeiou]*",s="^("+o+")?"+r+o,u="^("+o+")?"+r+o+"("+r+")?$",a="^("+o+")?"+r+o+r+o,l="^("+o+")?"+i,c=new RegExp(s),d=new RegExp(a),f=new RegExp(u),h=new RegExp(l),p=/^(.+?)(ss|i)es$/,v=/^(.+?)([^s])s$/,g=/^(.+?)eed$/,m=/^(.+?)(ed|ing)$/,y=/.$/,S=/(at|bl|iz)$/,x=new RegExp("([^aeiouylsz])\\1$"),w=new RegExp("^"+o+i+"[^aeiouwxy]$"),I=/^(.+?[^aeiou])y$/,b=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,E=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,D=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,F=/^(.+?)(s|t)(ion)$/,_=/^(.+?)e$/,P=/ll$/,k=new RegExp("^"+o+i+"[^aeiouwxy]$"),z=function(n){var i,o,r,s,u,a,l;if(n.length<3)return n;if(r=n.substr(0,1),"y"==r&&(n=r.toUpperCase()+n.substr(1)),s=p,u=v,s.test(n)?n=n.replace(s,"$1$2"):u.test(n)&&(n=n.replace(u,"$1$2")),s=g,u=m,s.test(n)){var z=s.exec(n);s=c,s.test(z[1])&&(s=y,n=n.replace(s,""))}else if(u.test(n)){var z=u.exec(n);i=z[1],u=h,u.test(i)&&(n=i,u=S,a=x,l=w,u.test(n)?n+="e":a.test(n)?(s=y,n=n.replace(s,"")):l.test(n)&&(n+="e"))}if(s=I,s.test(n)){var z=s.exec(n);i=z[1],n=i+"i"}if(s=b,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+e[o])}if(s=E,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+t[o])}if(s=D,u=F,s.test(n)){var z=s.exec(n);i=z[1],s=d,s.test(i)&&(n=i)}else if(u.test(n)){var z=u.exec(n);i=z[1]+z[2],u=d,u.test(i)&&(n=i)}if(s=_,s.test(n)){var z=s.exec(n);i=z[1],s=d,u=f,a=k,(s.test(i)||u.test(i)&&!a.test(i))&&(n=i)}return s=P,u=d,s.test(n)&&u.test(n)&&(s=y,n=n.replace(s,"")),"y"==r&&(n=r.toLowerCase()+n.substr(1)),n};return z}(),t.Pipeline.registerFunction(t.stemmer,"stemmer"),t.stopWordFilter=function(e){return e&&t.stopWordFilter.stopWords[e]!==!0?e:void 0},t.clearStopWords=function(){t.stopWordFilter.stopWords={}},t.addStopWords=function(e){null!=e&&Array.isArray(e)!==!1&&e.forEach(function(e){t.stopWordFilter.stopWords[e]=!0},this)},t.resetStopWords=function(){t.stopWordFilter.stopWords=t.defaultStopWords},t.defaultStopWords={"":!0,a:!0,able:!0,about:!0,across:!0,after:!0,all:!0,almost:!0,also:!0,am:!0,among:!0,an:!0,and:!0,any:!0,are:!0,as:!0,at:!0,be:!0,because:!0,been:!0,but:!0,by:!0,can:!0,cannot:!0,could:!0,dear:!0,did:!0,"do":!0,does:!0,either:!0,"else":!0,ever:!0,every:!0,"for":!0,from:!0,get:!0,got:!0,had:!0,has:!0,have:!0,he:!0,her:!0,hers:!0,him:!0,his:!0,how:!0,however:!0,i:!0,"if":!0,"in":!0,into:!0,is:!0,it:!0,its:!0,just:!0,least:!0,let:!0,like:!0,likely:!0,may:!0,me:!0,might:!0,most:!0,must:!0,my:!0,neither:!0,no:!0,nor:!0,not:!0,of:!0,off:!0,often:!0,on:!0,only:!0,or:!0,other:!0,our:!0,own:!0,rather:!0,said:!0,say:!0,says:!0,she:!0,should:!0,since:!0,so:!0,some:!0,than:!0,that:!0,the:!0,their:!0,them:!0,then:!0,there:!0,these:!0,they:!0,"this":!0,tis:!0,to:!0,too:!0,twas:!0,us:!0,wants:!0,was:!0,we:!0,were:!0,what:!0,when:!0,where:!0,which:!0,"while":!0,who:!0,whom:!0,why:!0,will:!0,"with":!0,would:!0,yet:!0,you:!0,your:!0},t.stopWordFilter.stopWords=t.defaultStopWords,t.Pipeline.registerFunction(t.stopWordFilter,"stopWordFilter"),t.trimmer=function(e){if(null===e||void 0===e)throw new Error("token should not be undefined");return e.replace(/^\W+/,"").replace(/\W+$/,"")},t.Pipeline.registerFunction(t.trimmer,"trimmer"),t.InvertedIndex=function(){this.root={docs:{},df:0}},t.InvertedIndex.load=function(e){var t=new this;return t.root=e.root,t},t.InvertedIndex.prototype.addToken=function(e,t,n){for(var n=n||this.root,i=0;i<=e.length-1;){var o=e[i];o in n||(n[o]={docs:{},df:0}),i+=1,n=n[o]}var r=t.ref;n.docs[r]?n.docs[r]={tf:t.tf}:(n.docs[r]={tf:t.tf},n.df+=1)},t.InvertedIndex.prototype.hasToken=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return!1;t=t[e[n]]}return!0},t.InvertedIndex.prototype.getNode=function(e){if(!e)return null;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return null;t=t[e[n]]}return t},t.InvertedIndex.prototype.getDocs=function(e){var t=this.getNode(e);return null==t?{}:t.docs},t.InvertedIndex.prototype.getTermFrequency=function(e,t){var n=this.getNode(e);return null==n?0:t in n.docs?n.docs[t].tf:0},t.InvertedIndex.prototype.getDocFreq=function(e){var t=this.getNode(e);return null==t?0:t.df},t.InvertedIndex.prototype.removeToken=function(e,t){if(e){var n=this.getNode(e);null!=n&&t in n.docs&&(delete n.docs[t],n.df-=1)}},t.InvertedIndex.prototype.expandToken=function(e,t,n){if(null==e||""==e)return[];var t=t||[];if(void 0==n&&(n=this.getNode(e),null==n))return t;n.df>0&&t.push(e);for(var i in n)"docs"!==i&&"df"!==i&&this.expandToken(e+i,t,n[i]);return t},t.InvertedIndex.prototype.toJSON=function(){return{root:this.root}},t.Configuration=function(e,n){var e=e||"";if(void 0==n||null==n)throw new Error("fields should not be null");this.config={};var i;try{i=JSON.parse(e),this.buildUserConfig(i,n)}catch(o){t.utils.warn("user configuration parse failed, will use default configuration"),this.buildDefaultConfig(n)}},t.Configuration.prototype.buildDefaultConfig=function(e){this.reset(),e.forEach(function(e){this.config[e]={boost:1,bool:"OR",expand:!1}},this)},t.Configuration.prototype.buildUserConfig=function(e,n){var i="OR",o=!1;if(this.reset(),"bool"in e&&(i=e.bool||i),"expand"in e&&(o=e.expand||o),"fields"in e)for(var r in e.fields)if(n.indexOf(r)>-1){var s=e.fields[r],u=o;void 0!=s.expand&&(u=s.expand),this.config[r]={boost:s.boost||0===s.boost?s.boost:1,bool:s.bool||i,expand:u}}else t.utils.warn("field name in user configuration not found in index instance fields");else this.addAllFields2UserConfig(i,o,n)},t.Configuration.prototype.addAllFields2UserConfig=function(e,t,n){n.forEach(function(n){this.config[n]={boost:1,bool:e,expand:t}},this)},t.Configuration.prototype.get=function(){return this.config},t.Configuration.prototype.reset=function(){this.config={}},lunr.SortedSet=function(){this.length=0,this.elements=[]},lunr.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},lunr.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},lunr.SortedSet.prototype.toArray=function(){return this.elements.slice()},lunr.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},lunr.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},lunr.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;){if(r===e)return o;e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o]}return r===e?o:-1},lunr.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;)e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o];return r>e?o:e>r?o+1:void 0},lunr.SortedSet.prototype.intersect=function(e){for(var t=new lunr.SortedSet,n=0,i=0,o=this.length,r=e.length,s=this.elements,u=e.elements;;){if(n>o-1||i>r-1)break;s[n]!==u[i]?s[n]<u[i]?n++:s[n]>u[i]&&i++:(t.add(s[n]),n++,i++)}return t},lunr.SortedSet.prototype.clone=function(){var e=new lunr.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},lunr.SortedSet.prototype.union=function(e){var t,n,i;this.length>=e.length?(t=this,n=e):(t=e,n=this),i=t.clone();for(var o=0,r=n.toArray();o<r.length;o++)i.add(r[o]);return i},lunr.SortedSet.prototype.toJSON=function(){return this.toArray()},function(e,t){"function"==typeof define&&define.amd?define(t):"object"==typeof exports?module.exports=t():e.elasticlunr=t()}(this,function(){return t})}();
    /** pdoc search index */const docs = [{"fullname": "NDNT", "modulename": "NDNT", "kind": "module", "doc": "<h2 id=\"welcome-to-the-documentation-for-ndnt\">Welcome to the documentation for NDNT!</h2>\n\n<h4 id=\"click-on-the-links-on-the-left-or-use-the-searchbar-to-navigate-through-the-documentation\">Click on the links on the left (or use the searchbar) to navigate through the documentation.</h4>\n\n<p>To generate the documentation, run the following command in the parent directory of the repository:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code>pdoc<span class=\"w\"> </span>-d<span class=\"w\"> </span>google<span class=\"w\"> </span>-o<span class=\"w\"> </span>NDNT/docs<span class=\"w\"> </span>NDNT\n</code></pre>\n</div>\n"}, {"fullname": "NDNT.NDN", "modulename": "NDNT.NDN", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.NDN.FFnets", "modulename": "NDNT.NDN", "qualname": "FFnets", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;normal&#x27;: &lt;class &#x27;NDNT.networks.FFnetwork&#x27;&gt;, &#x27;add&#x27;: &lt;class &#x27;NDNT.networks.FFnetwork&#x27;&gt;, &#x27;mult&#x27;: &lt;class &#x27;NDNT.networks.FFnetwork&#x27;&gt;, &#x27;scaffold&#x27;: &lt;class &#x27;NDNT.networks.ScaffoldNetwork&#x27;&gt;, &#x27;scaffold3d&#x27;: &lt;class &#x27;NDNT.networks.ScaffoldNetwork3D&#x27;&gt;, &#x27;readout&#x27;: &lt;class &#x27;NDNT.networks.ReadoutNetwork&#x27;&gt;}"}, {"fullname": "NDNT.NDN.NDN", "modulename": "NDNT.NDN", "qualname": "NDN", "kind": "class", "doc": "<p>Initializes an instance of the NDN (Neural Deep Network) class.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>ffnet_list (list):</strong>  A list of FFnetwork objects. If None, a single ffnet specified by layer_list will be used.</li>\n<li><strong>layer_list (list):</strong>  A list specifying the layers of the FFnetwork. Only used if ffnet_list is None.</li>\n<li><strong>external_modules (list):</strong>  A list of external modules to be used in the FFnetwork.</li>\n<li><strong>loss_type (str):</strong>  The type of loss function to be used. Default is 'poisson'.</li>\n<li><strong>ffnet_out (int or list):</strong>  The index(es) of the output layer(s) of the FFnetwork(s). If None, the last layer will be used.</li>\n<li><strong>optimizer_params (dict):</strong>  Parameters for the optimizer. If None, default parameters will be used.</li>\n<li><strong>model_name (str):</strong>  The name of the model. If None, a default name will be assigned.</li>\n<li><strong>seed (int):</strong>  The random seed to be used for reproducibility.</li>\n<li><strong>working_dir (str):</strong>  The directory to save checkpoints and other files.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "NDNT.NDN.NDN.__init__", "modulename": "NDNT.NDN", "qualname": "NDN.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">ffnet_list</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">layer_list</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">external_modules</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">loss_type</span><span class=\"o\">=</span><span class=\"s1\">&#39;poisson&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">ffnet_out</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">optimizer_params</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">model_name</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">seed</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">working_dir</span><span class=\"o\">=</span><span class=\"s1\">&#39;./checkpoints&#39;</span></span>)</span>"}, {"fullname": "NDNT.NDN.NDN.loss_type", "modulename": "NDNT.NDN", "qualname": "NDN.loss_type", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.NDN.NDN.ffnet_out", "modulename": "NDNT.NDN", "qualname": "NDN.ffnet_out", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.NDN.NDN.seed", "modulename": "NDNT.NDN", "qualname": "NDN.seed", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.NDN.NDN.ffnet_list", "modulename": "NDNT.NDN", "qualname": "NDN.ffnet_list", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.NDN.NDN.working_dir", "modulename": "NDNT.NDN", "qualname": "NDN.working_dir", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.NDN.NDN.loss", "modulename": "NDNT.NDN", "qualname": "NDN.loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.NDN.NDN.val_loss", "modulename": "NDNT.NDN", "qualname": "NDN.val_loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.NDN.NDN.trainer", "modulename": "NDNT.NDN", "qualname": "NDN.trainer", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.NDN.NDN.block_sample", "modulename": "NDNT.NDN", "qualname": "NDN.block_sample", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.NDN.NDN.networks", "modulename": "NDNT.NDN", "qualname": "NDN.networks", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.NDN.NDN.opt_params", "modulename": "NDNT.NDN", "qualname": "NDN.opt_params", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.NDN.NDN.speckled_flag", "modulename": "NDNT.NDN", "qualname": "NDN.speckled_flag", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.NDN.NDN.assemble_ffnetworks", "modulename": "NDNT.NDN", "qualname": "NDN.assemble_ffnetworks", "kind": "function", "doc": "<p>This function takes a list of ffnetworks and puts them together \nin order. This has to do two steps for each ffnetwork: </p>\n\n<ol>\n<li>Plug in the inputs to each ffnetwork as specified</li>\n<li>Builds the ff-network with the input</li>\n</ol>\n\n<p>This returns the 'network', which is (currently) a module with a \n'forward' and 'reg_loss' function specified.</p>\n\n<p>When multiple ffnet inputs are concatenated, it will always happen in the first\n(filter) dimension, so all other dimensions must match</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>ffnet_list (list):</strong>  A list of ffnetworks to be assembled.</li>\n<li><strong>external_nets (optional):</strong>  External networks to be passed into the 'external' type ffnetworks.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>networks (nn.ModuleList): A module list containing the assembled ff-networks.</p>\n</blockquote>\n\n<h6 id=\"raises\">Raises:</h6>\n\n<ul>\n<li><strong>AssertionError:</strong>  If ffnet_list is not a list.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">ffnet_list</span>, </span><span class=\"param\"><span class=\"n\">external_nets</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.compute_network_outputs", "modulename": "NDNT.NDN", "qualname": "NDN.compute_network_outputs", "kind": "function", "doc": "<p>Computes the network outputs for the given input data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>Xs (list):</strong>  The input data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>tuple: A tuple containing the network inputs and network outputs.</p>\n</blockquote>\n\n<h6 id=\"raises\">Raises:</h6>\n\n<ul>\n<li><strong>AssertionError:</strong>  If no networks are defined in this NDN.</li>\n</ul>\n\n<h6 id=\"note\">Note:</h6>\n\n<blockquote>\n  <p>This method currently saves only the network outputs and does not return the network inputs.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">Xs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.forward", "modulename": "NDNT.NDN", "qualname": "NDN.forward", "kind": "function", "doc": "<p>Applies the forward pass of each network in sequential order.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>Xs (list):</strong>  List of input data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>ndarray: Output of the forward pass.</p>\n</blockquote>\n\n<h6 id=\"notes\">Notes:</h6>\n\n<blockquote>\n  <p>The tricky thing is concatenating multiple-input dimensions together correctly.\n  Note that the external inputs are actually in principle a list of inputs.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">Xs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.training_step", "modulename": "NDNT.NDN", "qualname": "NDN.training_step", "kind": "function", "doc": "<p>Performs a single training step.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>batch (dict):</strong>  A dictionary containing the input data batch.</li>\n<li><strong>batch_idx (int, optional):</strong>  The index of the current batch. Defaults to None.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>dict: A dictionary containing the loss values for training, total loss, and regularization loss.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span>, </span><span class=\"param\"><span class=\"n\">batch_idx</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.validation_step", "modulename": "NDNT.NDN", "qualname": "NDN.validation_step", "kind": "function", "doc": "<p>Performs a validation step for the model: calculates the loss on the validation data: loss does not include\nregularization, although includes this to have the same categories as train_loss</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>batch (dict):</strong>  A dictionary containing the input batch data.</li>\n<li><strong>batch_idx (int, optional):</strong>  The index of the current batch. Defaults to None.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>dict: A dictionary containing the computed losses for the validation step, \n      with 'loss', 'val_loss' (same), and 'reg_loss' (actually reg_loss, but not used).</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span>, </span><span class=\"param\"><span class=\"n\">batch_idx</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.compute_reg_loss", "modulename": "NDNT.NDN", "qualname": "NDN.compute_reg_loss", "kind": "function", "doc": "<p>Computes the regularization loss for the NDNT model.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li>None</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The total regularization loss.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.get_trainer", "modulename": "NDNT.NDN", "qualname": "NDN.get_trainer", "kind": "function", "doc": "<p>Returns a trainer object.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>version (str):</strong>  The version of the trainer.</li>\n<li><strong>save_dir (str):</strong>  The directory to save the trainer checkpoints.</li>\n<li><strong>name (str):</strong>  The name of the trainer.</li>\n<li><strong>optimizer (torch.optim.Optimizer):</strong>  The optimizer to use for training.</li>\n<li><strong>scheduler (torch.optim.lr_scheduler._LRScheduler):</strong>  The scheduler to use for adjusting the learning rate during training.</li>\n<li><strong>device (str or torch.device):</strong>  The device to use for training. If not specified, it will default to 'cuda:0' if available, otherwise 'cpu'.</li>\n<li><strong>optimizer_type (str):</strong>  The type of optimizer to use. Default is 'AdamW'.</li>\n<li><strong>early_stopping (bool):</strong>  Whether to use early stopping during training. Default is False.</li>\n<li><strong>early_stopping_patience (int):</strong>  The number of epochs to wait for improvement before stopping early. Default is 5.</li>\n<li><strong>early_stopping_delta (float):</strong>  The minimum change in the monitored metric to be considered as improvement. Default is 0.0.</li>\n<li><strong>optimize_graph (bool):</strong>  Whether to optimize the computation graph during training. Default is False.</li>\n<li><strong>verbose (bool):</strong>  whether trainer should output messages, passed from fit options (default 0)</li>\n<li><strong>**kwargs:</strong>  Additional keyword arguments to be passed to the trainer.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>trainer (Trainer): The trainer object.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">version</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">save_dir</span><span class=\"o\">=</span><span class=\"s1\">&#39;./checkpoints&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">&#39;jnkname&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">optimizer</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">scheduler</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">optimizer_type</span><span class=\"o\">=</span><span class=\"s1\">&#39;AdamW&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">early_stopping</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">early_stopping_patience</span><span class=\"o\">=</span><span class=\"mi\">5</span>,</span><span class=\"param\">\t<span class=\"n\">early_stopping_delta</span><span class=\"o\">=</span><span class=\"mf\">0.0</span>,</span><span class=\"param\">\t<span class=\"n\">optimize_graph</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">save_epochs</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.get_dataloaders", "modulename": "NDNT.NDN", "qualname": "NDN.get_dataloaders", "kind": "function", "doc": "<p>Creates dataloaders for training and validation: essentially breaking up dataset into\nloaders for training and validation segment. While the default behavior if either train or\nval inds was not passed in was to randomly 5-fold, default behavior is now to give whole\ndatasets for both, unless fold_validation is set to an integer (fold). </p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>dataset (Dataset):</strong>  The dataset to use for training and validation</li>\n<li><strong>batch_size (int):</strong>  The batch size to use</li>\n<li><strong>train_inds (list):</strong>  The indices of the training data</li>\n<li><strong>val_inds (list):</strong>  The indices of the validation data</li>\n<li><strong>fold_validation (None or int):</strong>  if random n-fold validation</li>\n<li><strong>data_seed (int):</strong>  The seed to use for the data loader if fold_validation is used</li>\n<li><strong>num_workers (int):</strong>  The number of workers to use for data loading, default 0 lets system choose</li>\n<li><strong>is_multiexp (bool):</strong>  Whether sampling from the dataset should be balanced across experiments</li>\n<li><strong>pin_memory (bool):</strong>  Whether to pin memory for faster data loading</li>\n<li><strong>verbose (int):</strong>  whether to output information during fit (above 1) &lt;== not currently used</li>\n<li><strong>**kwargs:</strong>  Additional keyword arguments.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>train_dl (DataLoader): The training data loader.\n  valid_dl (DataLoader): The validation data loader.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">dataset</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">10</span>,</span><span class=\"param\">\t<span class=\"n\">train_inds</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">val_inds</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">fold_validation</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">is_multiexp</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">pin_memory</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">data_seed</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.get_optimizer", "modulename": "NDNT.NDN", "qualname": "NDN.get_optimizer", "kind": "function", "doc": "<p>Returns an optimizer object.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>optimizer_type (str):</strong>  The type of optimizer to use. Default is 'AdamW'.</li>\n<li><strong>learning_rate (float):</strong>  The learning rate to use. Default is 0.001.</li>\n<li><strong>weight_decay (float):</strong>  The weight decay to use. Default is 0.01.</li>\n<li><strong>amsgrad (bool):</strong>  Whether to use the AMSGrad variant of Adam. Default is False.</li>\n<li><strong>betas (tuple):</strong>  The beta values to use for the optimizer. Default is (0.9, 0.999).</li>\n<li><strong>max_iter (int):</strong>  The maximum number of iterations for the LBFGS optimizer. Default is 10.</li>\n<li><strong>history_size (int):</strong>  The history size to use for the LBFGS optimizer. Default is 4.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>optimizer (torch.optim.Optimizer): The optimizer object.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">optimizer_type</span><span class=\"o\">=</span><span class=\"s1\">&#39;AdamW&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"mf\">0.001</span>,</span><span class=\"param\">\t<span class=\"n\">weight_decay</span><span class=\"o\">=</span><span class=\"mf\">0.01</span>,</span><span class=\"param\">\t<span class=\"n\">amsgrad</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">betas</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"mf\">0.999</span><span class=\"p\">)</span>,</span><span class=\"param\">\t<span class=\"n\">momentum</span><span class=\"o\">=</span><span class=\"mf\">0.9</span>,</span><span class=\"param\">\t<span class=\"n\">max_iter</span><span class=\"o\">=</span><span class=\"mi\">10</span>,</span><span class=\"param\">\t<span class=\"n\">history_size</span><span class=\"o\">=</span><span class=\"mi\">4</span>,</span><span class=\"param\">\t<span class=\"n\">tolerance_change</span><span class=\"o\">=</span><span class=\"mf\">0.001</span>,</span><span class=\"param\">\t<span class=\"n\">tolerance_grad</span><span class=\"o\">=</span><span class=\"mf\">0.0001</span>,</span><span class=\"param\">\t<span class=\"n\">line_search_fn</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.prepare_regularization", "modulename": "NDNT.NDN", "qualname": "NDN.prepare_regularization", "kind": "function", "doc": "<p>Prepares the regularization for the model.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.fit", "modulename": "NDNT.NDN", "qualname": "NDN.fit", "kind": "function", "doc": "<p>Trains the model (self) using the specified dataset.\nNote the mechanics of train/validation indices are such that -- if they are not passed in --\nthe fit will look to the dataset itself, first to determine if block_sample is set, and then\neither use dataset.train_inds or dataset.train_blks accordingly.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>dataset (Dataset):</strong>  The dataset to use for training and validation.</li>\n<li><strong>train_inds (list):</strong>  The indices of the training data.</li>\n<li><strong>val_inds (list):</strong>  The indices of the validation data.</li>\n<li><strong>speckledXV (bool):</strong>  Whether to use speckled cross-validation. Default is False.</li>\n<li><strong>verbose (int):</strong>  level of text info while running (0=None, 1=epoch level, 2=batch and extra info): default 2</li>\n<li><strong>seed (int):</strong>  The seed to use for reproducibility.</li>\n<li><strong>save_dir (str):</strong>  The directory to save the model checkpoints.</li>\n<li><strong>version (int):</strong>  The version of the trainer.</li>\n<li><strong>optimizer (torch.optim.Optimizer):</strong>  The optimizer to use for training.</li>\n<li><strong>scheduler (torch.optim.lr_scheduler._LRScheduler):</strong>  The scheduler to use for adjusting the learning rate during training.</li>\n<li><strong>batch_size (int):</strong>  The batch size to use.</li>\n<li><strong>force_dict_training (bool):</strong>  Whether to force dictionary-based training. Default is False.</li>\n<li><strong>block_sample (bool):</strong>  Whether to use block sampling. Default is None.</li>\n<li><strong>reuse_trainer (bool):</strong>  Whether to reuse the trainer. Default is False.</li>\n<li><strong>device (str or torch.device):</strong>  The device to use for training. Default is None.</li>\n<li><strong>**kwargs:</strong>  Additional keyword arguments.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n\n<h6 id=\"raises\">Raises:</h6>\n\n<ul>\n<li><strong>AssertionError:</strong>  If batch_size is not specified.</li>\n</ul>\n\n<h6 id=\"notes\">Notes:</h6>\n\n<blockquote>\n  <p>The fit method is the main training loop for the model.\n  Steps:</p>\n  \n  <ol>\n  <li>Get a trainer and dataloaders</li>\n  <li>Prepare regularizers</li>\n  <li>Run the main fit loop from the trainer, checkpoint, and save model</li>\n  </ol>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">dataset</span>,</span><span class=\"param\">\t<span class=\"n\">train_inds</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">val_inds</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">speckledXV</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"mi\">2</span>,</span><span class=\"param\">\t<span class=\"n\">seed</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">save_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">version</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">optimizer</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">scheduler</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">force_dict_training</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">block_sample</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">reuse_trainer</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">save_epochs</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.fit_dl", "modulename": "NDNT.NDN", "qualname": "NDN.fit_dl", "kind": "function", "doc": "<p>Fits the model using deep learning training.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>train_ds (Dataset):</strong>  The training dataset.</li>\n<li><strong>val_ds (Dataset):</strong>  The validation dataset.</li>\n<li><strong>seed (int, optional):</strong>  The seed for data and optimization. Defaults to None.</li>\n<li><strong>save_dir (str, optional):</strong>  The directory to save the model. Defaults to None.</li>\n<li><strong>version (int, optional):</strong>  The version of the model. Defaults to None.</li>\n<li><strong>name (str, optional):</strong>  The name of the model. Defaults to None.</li>\n<li><strong>optimizer (Optimizer, optional):</strong>  The optimizer for training. Defaults to None.</li>\n<li><strong>scheduler (Scheduler, optional):</strong>  The scheduler for training. Defaults to None.</li>\n<li><strong>batch_size (int, optional):</strong>  The batch size for training. Defaults to None.</li>\n<li><strong>num_workers (int, optional):</strong>  The number of workers for data loading. Defaults to 0.</li>\n<li><strong>force_dict_training (bool, optional):</strong>  Whether to force dictionary-based training instead of using data loaders for LBFGS. Defaults to False.</li>\n<li><strong>device (str, optional):</strong>  The device to use for training. Defaults to None.</li>\n<li><strong>**kwargs:</strong>  Additional keyword arguments.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n\n<h6 id=\"raises\">Raises:</h6>\n\n<ul>\n<li><strong>AssertionError:</strong>  If batch_size is not provided.</li>\n</ul>\n\n<h6 id=\"notes\">Notes:</h6>\n\n<blockquote>\n  <p>This is the main training loop.\n  Steps:</p>\n  \n  <ol>\n  <li>Get a trainer and data loaders.</li>\n  <li>Prepare regularizers.</li>\n  <li>Run the main fit loop from the trainer, checkpoint, and save the model.</li>\n  </ol>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">train_ds</span>,</span><span class=\"param\">\t<span class=\"n\">val_ds</span>,</span><span class=\"param\">\t<span class=\"n\">seed</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">save_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">version</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">optimizer</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">scheduler</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">force_dict_training</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">save_epochs</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.initialize_loss", "modulename": "NDNT.NDN", "qualname": "NDN.initialize_loss", "kind": "function", "doc": "<p>Interacts with loss_module to set loss flags and/or pass in dataset information. In particular, it\ncan update batch_weighting (how the loss calculated during fitting is weighted by time), and especially\nthe unit_weighting, which -- if true -- this uses a unit-based normalization set to either what is passed\nin, OR (if nothing is passed in but unit_weighting=True), it will normalize by the RELATIVE\naverage firing rate.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>dataset (optional):</strong>  The dataset to be used for computing average batch size and unit weights.</li>\n<li><strong>batch_size (optional):</strong>  The batch size to be used for computing average batch size.</li>\n<li><strong>data_inds (optional):</strong>  The indices of the data in the dataset to be used.</li>\n<li><strong>batch_weighting (optional):</strong>  The batch weighting value to be set. Must be one of [-1, 0, 1, 2].</li>\n<li><strong>unit_weighting (optional):</strong>  The unit weighting value to be set.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n\n<h6 id=\"notes\">Notes:</h6>\n\n<blockquote>\n  <p>Without a dataset, this method will only set the boolean flags 'batch_weighting' and 'unit_weighting'.\n  With a dataset, this method will set 'av_batch_size' and 'unit_weights' based on the average rate.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">dataset</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">data_inds</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_weighting</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">unit_weighting</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.calc_spikingNL", "modulename": "NDNT.NDN", "qualname": "NDN.calc_spikingNL", "kind": "function", "doc": "<p>Computes measured and model spiking nonlineatities across data for all NDN outputs</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li>dataset</li>\n<li>data_inds</li>\n<li>gmin</li>\n<li>gmax</li>\n<li>gbins</li>\n</ul>\n\n<h6 id=\"output\">Output:</h6>\n\n<blockquote>\n  <p>spkNL: dict with fNLs (model NLs), spkNLs (measured), gbins, gdist</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">dataset</span>,</span><span class=\"param\">\t<span class=\"n\">data_inds</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">gmin</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">gmax</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">gbins</span><span class=\"o\">=</span><span class=\"mi\">30</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_lags</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.compute_average_responses", "modulename": "NDNT.NDN", "qualname": "NDN.compute_average_responses", "kind": "function", "doc": "<p>Computes the average responses for the specified dataset.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>dataset:</strong>  The dataset to use for computing the average responses.</li>\n<li><strong>data_inds:</strong>  The indices of the data in the dataset.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>ndarray: The average responses.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">dataset</span>, </span><span class=\"param\"><span class=\"n\">data_inds</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.initialize_biases", "modulename": "NDNT.NDN", "qualname": "NDN.initialize_biases", "kind": "function", "doc": "<p>Initializes the biases for the specified dataset.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>dataset:</strong>  The dataset to use for initializing the biases.</li>\n<li><strong>data_inds:</strong>  The indices of the data in the dataset.</li>\n<li><strong>ffnet_target:</strong>  The target feedforward network.</li>\n<li><strong>layer_target:</strong>  The target layer.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n\n<h6 id=\"notes\">Notes:</h6>\n\n<blockquote>\n  <p>This method initializes the biases for the specified feedforward network and layer using the average responses.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">dataset</span>, </span><span class=\"param\"><span class=\"n\">data_inds</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">ffnet_target</span><span class=\"o\">=-</span><span class=\"mi\">1</span>, </span><span class=\"param\"><span class=\"n\">layer_target</span><span class=\"o\">=-</span><span class=\"mi\">1</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.eval_models", "modulename": "NDNT.NDN", "qualname": "NDN.eval_models", "kind": "function", "doc": "<p>Evaluate the neural network models on the given data.</p>\n\n<p>get null-adjusted log likelihood (if null_adjusted = True)\nbits=True will return in units of bits/spike</p>\n\n<p>Note that data will be assumed to be a dataset, and data_inds will have to be specified batches\nfrom dataset.__get_item__()</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>data (dict or Dataset):</strong>  The input data to evaluate the models on. If a dictionary is provided, it is assumed to be a single sample. If a Dataset object is provided, it is assumed to contain multiple samples.</li>\n<li><strong>data_inds (list, optional):</strong>  The indices of the data samples to evaluate. Only applicable if <code>data</code> is a Dataset object. Defaults to None.</li>\n<li><strong>bits (bool, optional):</strong>  If True, the result will be returned in units of bits/spike. Defaults to False.</li>\n<li><strong>null_adjusted (bool, optional):</strong>  If True, the null-adjusted log likelihood will be calculated. Defaults to False.</li>\n<li><strong>speckledXV (bool, optional):</strong>  If True, speckled cross-validation will be used. Defaults to False.</li>\n<li><strong>train_val (int, optional):</strong>  The train/validation split ratio. Only applicable if <code>speckledXV</code> is True. Defaults to 1.</li>\n<li><strong>batch_size (int, optional):</strong>  The batch size for data loading. Defaults to 1000.</li>\n<li><strong>num_workers (int, optional):</strong>  The number of worker processes for data loading. Defaults to 0.</li>\n<li><strong>device (torch.device, optional):</strong>  The device to perform the evaluation on. If None, the device of the model will be used. Defaults to None.</li>\n<li><strong>**kwargs:</strong>  Additional keyword arguments.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>numpy.ndarray: The evaluated log likelihood values for each neuron.</p>\n</blockquote>\n\n<h6 id=\"note\">Note:</h6>\n\n<blockquote>\n  <p>If <code>data</code> is a dictionary, it is assumed to contain the following keys:\n      'robs': The observed responses.\n      'Mval': The validation mask.\n      'dfs': The data filters.\n  If <code>data</code> is a Dataset object, it is assumed to have the following attributes:\n      'robs': The observed responses.\n      'dfs': The data filters.</p>\n</blockquote>\n\n<h6 id=\"raises\">Raises:</h6>\n\n<ul>\n<li><strong>AssertionError:</strong>  If <code>data_inds</code> is not None when <code>data</code> is a dictionary.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">data</span>,</span><span class=\"param\">\t<span class=\"n\">data_inds</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">bits</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">null_adjusted</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">speckledXV</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">train_val</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">1000</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.predictions", "modulename": "NDNT.NDN", "qualname": "NDN.predictions", "kind": "function", "doc": "<p>Generate predictions for the model for a dataset. Note that will need to send to device if needed, and enter \nbatch_size information. </p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>data:</strong>  The dataset.</li>\n<li><strong>data_inds:</strong>  The indices to use from the dataset (if block_sample, will be interpreted as block_inds).</li>\n<li><strong>batch_size:</strong>  The batch size to use for processing. Reduce if running out of memory.</li>\n<li><strong>num_lags:</strong>  The number of lags to use for prediction.</li>\n<li><strong>ffnet_target:</strong>  The index of the feedforward network to use for prediction. Defaults to None.</li>\n<li><strong>device:</strong>  The device to use for prediction</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>torch.Tensor: The predictions array (detached, on cpu)</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">data</span>,</span><span class=\"param\">\t<span class=\"n\">data_inds</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_lags</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">ffnet_target</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.change_loss", "modulename": "NDNT.NDN", "qualname": "NDN.change_loss", "kind": "function", "doc": "<p>Change the loss function for the model.</p>\n\n<p>Swaps in new loss module for model.\nInclude dataset if wish to initialize loss (which happens during fit).</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>new_loss_type:</strong>  The new loss type.</li>\n<li><strong>dataset:</strong>  The dataset to use for initializing the loss.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n\n<h6 id=\"notes\">Notes:</h6>\n\n<blockquote>\n  <p>This method swaps in a new loss module for the model.\n  If a dataset is provided, the loss will be initialized during the fit.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">new_loss_type</span>, </span><span class=\"param\"><span class=\"n\">dataset</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.get_weights", "modulename": "NDNT.NDN", "qualname": "NDN.get_weights", "kind": "function", "doc": "<p>Get the weights for the specified feedforward network.</p>\n\n<p>Passed down to layer call, with optional arguments conveyed.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>ffnet_target:</strong>  The target feedforward network.</li>\n<li><strong>**kwargs:</strong>  Additional keyword arguments.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>list: The weights for the specified feedforward network.</p>\n</blockquote>\n\n<h6 id=\"notes\">Notes:</h6>\n\n<blockquote>\n  <p>This method is passed down to the layer call with optional arguments conveyed.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">ffnet_target</span><span class=\"o\">=</span><span class=\"mi\">0</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.get_biases", "modulename": "NDNT.NDN", "qualname": "NDN.get_biases", "kind": "function", "doc": "<p>Get the biases for the specified feedforward network.</p>\n\n<p>Passed down to layer call, with optional arguments conveyed.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>ffnet_target:</strong>  The target feedforward network.</li>\n<li><strong>**kwargs:</strong>  Additional keyword arguments.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>list: The weights for the specified feedforward network.</p>\n</blockquote>\n\n<h6 id=\"notes\">Notes:</h6>\n\n<blockquote>\n  <p>This method is passed down to the layer call with optional arguments conveyed.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">ffnet_target</span><span class=\"o\">=</span><span class=\"mi\">0</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.get_readout_locations", "modulename": "NDNT.NDN", "qualname": "NDN.get_readout_locations", "kind": "function", "doc": "<p>Get the readout locations and sigmas.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>list: The readout locations and sigmas.</p>\n</blockquote>\n\n<h6 id=\"notes\">Notes:</h6>\n\n<blockquote>\n  <p>This method currently returns a list of readout locations and sigmas set in the readout network.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.passive_readout", "modulename": "NDNT.NDN", "qualname": "NDN.passive_readout", "kind": "function", "doc": "<p>Applies the passive readout function to the readout layer.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n\n<h6 id=\"notes\">Notes:</h6>\n\n<blockquote>\n  <p>This method finds the readout layer and applies its passive_readout function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.fit_mus", "modulename": "NDNT.NDN", "qualname": "NDN.fit_mus", "kind": "function", "doc": "<p>Find the readout network and layer, and convert to fitting mus dependent on val.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>val (boolean):</strong>  whether or not to fit mus</li>\n<li><strong>sigma (float):</strong>  will change sigmas to beginning value specified (Default: None means leave alone)</li>\n<li><strong>sample_mode (str):</strong>  will deviate from default behavior if set</li>\n<li><strong>verbose (bool):</strong>  whether to print information about the fitting process</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None, although will modify readout layer</p>\n</blockquote>\n\n<h6 id=\"notes\">Notes:</h6>\n\n<blockquote>\n  <p>This method currently returns a list of readout locations and sigmas set in the readout network.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">val</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">sigma</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">sample_mode</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.fit_Qmus", "modulename": "NDNT.NDN", "qualname": "NDN.fit_Qmus", "kind": "function", "doc": "<p>Find the readout network and layer, and convert to fitting mus dependent on val.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>val (boolean):</strong>  whether or not to fit mus</li>\n<li><strong>sigma (float):</strong>  will change sigmas to beginning value specified (Default: None means leave alone)</li>\n<li><strong>sample_mode (str):</strong>  will deviate from default behavior if set</li>\n<li><strong>verbose (bool):</strong>  whether to print information about the fitting process</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None, although will modify readout layer</p>\n</blockquote>\n\n<h6 id=\"notes\">Notes:</h6>\n\n<blockquote>\n  <p>This method currently returns a list of readout locations and sigmas set in the readout network.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">val</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">Qsigma</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">sample_mode</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.list_parameters", "modulename": "NDNT.NDN", "qualname": "NDN.list_parameters", "kind": "function", "doc": "<p>List the parameters for the specified feedforward network and layer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>ffnet_target:</strong>  The target feedforward network.</li>\n<li><strong>layer_target:</strong>  The target layer.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">ffnet_target</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">layer_target</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.set_parameters", "modulename": "NDNT.NDN", "qualname": "NDN.set_parameters", "kind": "function", "doc": "<p>Set the parameters for the specified feedforward network and layer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>ffnet_target:</strong>  The target feedforward network.</li>\n<li><strong>layer_target:</strong>  The target layer.</li>\n<li><strong>name:</strong>  The name of the parameter.</li>\n<li><strong>val:</strong>  The value to set the parameter to.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n\n<h6 id=\"notes\">Notes:</h6>\n\n<blockquote>\n  <p>This method sets the parameters for the specified feedforward network and layer.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">ffnet_target</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">layer_target</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">val</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.set_reg_val", "modulename": "NDNT.NDN", "qualname": "NDN.set_reg_val", "kind": "function", "doc": "<p>Set reg_values for listed network and layer targets (default 0,0).</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>reg_type:</strong>  The regularization type.</li>\n<li><strong>reg_val:</strong>  The regularization value.</li>\n<li><strong>ffnet_target:</strong>  The target feedforward network.</li>\n<li><strong>layer_target:</strong>  The target layer.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n\n<h6 id=\"notes\">Notes:</h6>\n\n<blockquote>\n  <p>This method sets the regularization values for the specified feedforward network and layer.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">reg_type</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">reg_val</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">ffnet_target</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">layer_target</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.get_network_info", "modulename": "NDNT.NDN", "qualname": "NDN.get_network_info", "kind": "function", "doc": "<p>Prints out a decreiption of the model structure.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">abbrev</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.info", "modulename": "NDNT.NDN", "qualname": "NDN.info", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">expand</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.plot_filters", "modulename": "NDNT.NDN", "qualname": "NDN.plot_filters", "kind": "function", "doc": "<p>Plot the filters for the specified feedforward network.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>ffnet_target:</strong>  The target feedforward network.</li>\n<li><strong>**kwargs:</strong>  Additional keyword arguments.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">ffnet_target</span><span class=\"o\">=</span><span class=\"mi\">0</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.update_ffnet_list", "modulename": "NDNT.NDN", "qualname": "NDN.update_ffnet_list", "kind": "function", "doc": "<p>the ffnet_list builds the NDN, but ideally holds a record of the regularization (a dictionary)\nsince dictionaries are not saveable in a checkpoint. So, this function pulls the dictionaries\nfrom each reg_module back into the ffnet_list</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li>None</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None, but updates self.ffnet_list</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.save_model", "modulename": "NDNT.NDN", "qualname": "NDN.save_model", "kind": "function", "doc": "<p>Save the model as a zip file (with extension .ndn) containing a json file with the model parameters\nand a .ckpt file with the state_dict.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  The name of the zip file to save.</li>\n<li><strong>ffnet_list:</strong>  The list of feedforward networks. (if this is set, it uses the ffnet_list, ffnet_out, loss_type, model_name, and working_dir arguments)</li>\n<li><strong>ffnet_out:</strong>  The output layer of the feedforward network.</li>\n<li><strong>loss_type:</strong>  The loss type.</li>\n<li><strong>model_name:</strong>  The name of the model.</li>\n<li><strong>working_dir:</strong>  The working directory.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">path</span>,</span><span class=\"param\">\t<span class=\"n\">ffnet_list</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">ffnet_out</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">loss_type</span><span class=\"o\">=</span><span class=\"s1\">&#39;poisson&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">model_name</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">working_dir</span><span class=\"o\">=</span><span class=\"s1\">&#39;./checkpoints&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.load_model", "modulename": "NDNT.NDN", "qualname": "NDN.load_model", "kind": "function", "doc": "<p>Load the model from an a zip file (with extension .ndn) containing a json file with the model parameters\nand a .ckpt file with the state_dict.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  The name of the zip file to load.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>NDN: The loaded model.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">path</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.save_model_pkl", "modulename": "NDNT.NDN", "qualname": "NDN.save_model_pkl", "kind": "function", "doc": "<p>Models will be saved using dill/pickle in as the filename, which can contain\nthe directory information. Will be put in the CPU first</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>filename:</strong>  The name of the file to save.</li>\n<li><strong>pt:</strong>  Whether to use torch.save instead of dill.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">filename</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">pt</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.save_model_chk", "modulename": "NDNT.NDN", "qualname": "NDN.save_model_chk", "kind": "function", "doc": "<p>Models will be saved using dill/pickle in the directory above the version\ndirectories, which happen to be under the model-name itself. This assumes the\ncurrent save-directory (notebook specific) and the model name</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>filename:</strong>  The name of the file to save.</li>\n<li><strong>alt_dirname:</strong>  The alternate directory to save the file.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">filename</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">alt_dirname</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.get_null_adjusted_ll", "modulename": "NDNT.NDN", "qualname": "NDN.get_null_adjusted_ll", "kind": "function", "doc": "<p>Get the null-adjusted log likelihood.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>sample:</strong>  The sample data from a dataset.</li>\n<li><strong>bits:</strong>  Whether to return the result in units of bits/spike.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>float: The null-adjusted log likelihood.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">sample</span>, </span><span class=\"param\"><span class=\"n\">bits</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.get_activations", "modulename": "NDNT.NDN", "qualname": "NDN.get_activations", "kind": "function", "doc": "<p>Returns the inputs and outputs of a specified ffnet and layer</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>sample:</strong>  dictionary of sample data from a dataset</li>\n<li><strong>ffnet_target:</strong>  which network to target (default: 0)</li>\n<li><strong>layer_target:</strong>  which layer to target (default: 0)</li>\n<li><strong>NL:</strong>  get activations using the nonlinearity as the module target</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>activations dict\n  with keys:\n      'input' : input to layer\n      'output' : output of layer</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">sample</span>, </span><span class=\"param\"><span class=\"n\">ffnet_target</span><span class=\"o\">=</span><span class=\"mi\">0</span>, </span><span class=\"param\"><span class=\"n\">layer_target</span><span class=\"o\">=</span><span class=\"mi\">0</span>, </span><span class=\"param\"><span class=\"n\">NL</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.model_string", "modulename": "NDNT.NDN", "qualname": "NDN.model_string", "kind": "function", "doc": "<p>Automated way to name model, based on NDN network structure, num outputs, and number of layers.\nFormat is NDN101_S3_R1_N1_A1 would be an NDN with 101 outputs, 3-layer scaffold followed by readout, \nnormal (drift) layer, and add (comb) layer</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>str: The model name.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.device", "modulename": "NDNT.NDN", "qualname": "NDN.device", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.NDN.NDN.load_model_pkl", "modulename": "NDNT.NDN", "qualname": "NDN.load_model_pkl", "kind": "function", "doc": "<p>Load a pickled model from disk.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>filename:</strong>  The path and filename.</li>\n<li><strong>pt:</strong>  Whether to use torch.load instead of dill.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>model: The loaded model on CPU.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">filename</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">pt</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.NDN.NDN.load_model_ckpt", "modulename": "NDNT.NDN", "qualname": "NDN.load_model_ckpt", "kind": "function", "doc": "<p>Load a model from disk. Note that if model not named, the full path must be put into checkpoint path.\nIf filename </p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>checkpoint_path:</strong>  The path to the directory containing model checkpoints</li>\n<li><strong>model_name:</strong>  The name of the model (from model.model_name)</li>\n<li><strong>version:</strong>  The checkpoint version (default: best)</li>\n<li><strong>filename:</strong>  Enter if want to override 'best model' and load specific file in specified path</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>model: The loaded model</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">model_name</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">version</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">filename</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules", "modulename": "NDNT.modules", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.activations", "modulename": "NDNT.modules.activations", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.activations.adaptive_elu", "modulename": "NDNT.modules.activations", "qualname": "adaptive_elu", "kind": "function", "doc": "<p>Exponential Linear Unit shifted by user specified values.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x (torch.Tensor):</strong>  input tensor</li>\n<li><strong>xshift (float):</strong>  shift in x direction</li>\n<li><strong>yshift (float):</strong>  shift in y direction</li>\n<li><strong>inplace (bool):</strong>  whether to modify the input tensor in place</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>torch.Tensor: output tensor</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">xshift</span>, </span><span class=\"param\"><span class=\"n\">yshift</span>, </span><span class=\"param\"><span class=\"n\">inplace</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.activations.Square", "modulename": "NDNT.modules.activations", "qualname": "Square", "kind": "class", "doc": "<p>Square activation function.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "NDNT.modules.activations.Square.__init__", "modulename": "NDNT.modules.activations", "qualname": "Square.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">()</span>"}, {"fullname": "NDNT.modules.activations.Square.forward", "modulename": "NDNT.modules.activations", "qualname": "Square.forward", "kind": "function", "doc": "<p>Forward pass of the square activation function.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x (torch.Tensor):</strong>  input tensor</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>torch.Tensor: output tensor</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.activations.AdaptiveELU", "modulename": "NDNT.modules.activations", "qualname": "AdaptiveELU", "kind": "class", "doc": "<p>Exponential Linear Unit shifted by user specified values.\nThis helps to ensure the output to stay positive.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "NDNT.modules.activations.AdaptiveELU.__init__", "modulename": "NDNT.modules.activations", "qualname": "AdaptiveELU.__init__", "kind": "function", "doc": "<p>Initialize the AdaptiveELU activation function.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>xshift (float):</strong>  shift in x direction</li>\n<li><strong>yshift (float):</strong>  shift in y direction</li>\n<li><strong>inplace (bool):</strong>  whether to modify the input tensor in place</li>\n<li><strong>**kwargs:</strong>  additional keyword arguments</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">xshift</span><span class=\"o\">=</span><span class=\"mf\">0.0</span>, </span><span class=\"param\"><span class=\"n\">yshift</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>, </span><span class=\"param\"><span class=\"n\">inplace</span><span class=\"o\">=</span><span class=\"kc\">True</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.activations.AdaptiveELU.xshift", "modulename": "NDNT.modules.activations", "qualname": "AdaptiveELU.xshift", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.activations.AdaptiveELU.yshift", "modulename": "NDNT.modules.activations", "qualname": "AdaptiveELU.yshift", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.activations.AdaptiveELU.inplace", "modulename": "NDNT.modules.activations", "qualname": "AdaptiveELU.inplace", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.activations.AdaptiveELU.forward", "modulename": "NDNT.modules.activations", "qualname": "AdaptiveELU.forward", "kind": "function", "doc": "<p>Forward pass of the AdaptiveELU activation function.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x (torch.Tensor):</strong>  input tensor</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>torch.Tensor: output tensor</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.activations.NLtypes", "modulename": "NDNT.modules.activations", "qualname": "NLtypes", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;lin&#x27;: None, &#x27;relu&#x27;: ReLU(), &#x27;elu&#x27;: AdaptiveELU(), &#x27;square&#x27;: Square(), &#x27;softplus&#x27;: Softplus(beta=1, threshold=20), &#x27;tanh&#x27;: Tanh(), &#x27;sigmoid&#x27;: Sigmoid(), &#x27;gelu&#x27;: GELU(approximate=&#x27;none&#x27;)}"}, {"fullname": "NDNT.modules.activity_regularization", "modulename": "NDNT.modules.activity_regularization", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.activity_regularization.ActivityRegularization", "modulename": "NDNT.modules.activity_regularization", "qualname": "ActivityRegularization", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.activity_regularization.ActivityRegularization.__init__", "modulename": "NDNT.modules.activity_regularization", "qualname": "ActivityRegularization.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">reg_vals</span></span>)</span>"}, {"fullname": "NDNT.modules.activity_regularization.ActivityRegularization.reg_vals", "modulename": "NDNT.modules.activity_regularization", "qualname": "ActivityRegularization.reg_vals", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.activity_regularization.ActivityRegularization.activity", "modulename": "NDNT.modules.activity_regularization", "qualname": "ActivityRegularization.activity", "kind": "function", "doc": "<p>Activity regularization loss function.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>layer_output (torch.Tensor):</strong>  output of a layer</li>\n<li><strong>alpha (float):</strong>  (regularization strength)</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>torch.Tensor (regularization loss)</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">layer_output</span>, </span><span class=\"param\"><span class=\"n\">alpha</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.activity_regularization.ActivityRegularization.nonneg", "modulename": "NDNT.modules.activity_regularization", "qualname": "ActivityRegularization.nonneg", "kind": "function", "doc": "<p>Non-negative regularization loss function.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>layer_output (torch.Tensor):</strong>  output of a layer</li>\n<li><strong>alpha (float):</strong>  (regularization strength)</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>torch.Tensor (regularization loss)</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">layer_output</span>, </span><span class=\"param\"><span class=\"n\">alpha</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.activity_regularization.ActivityRegularization.regularize", "modulename": "NDNT.modules.activity_regularization", "qualname": "ActivityRegularization.regularize", "kind": "function", "doc": "<p>Regularization loss function.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>layer_output (torch.Tensor):</strong>  output of a layer</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>torch.Tensor (regularization loss)</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">layer_output</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.experiment_sampler", "modulename": "NDNT.modules.experiment_sampler", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.experiment_sampler.ExperimentBatchGenerator", "modulename": "NDNT.modules.experiment_sampler", "qualname": "ExperimentBatchGenerator", "kind": "class", "doc": "<p>Generator that returns batches of timepoints for a single experiment.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>timepoints (list):</strong>  list of timepoints to sample from</li>\n<li><strong>batch_size (int):</strong>  size of the batch to sample</li>\n<li><strong>shuffle (bool):</strong>  whether to shuffle the batches</li>\n<li><strong>random_seed (int):</strong>  random seed to use for shuffling</li>\n</ul>\n"}, {"fullname": "NDNT.modules.experiment_sampler.ExperimentBatchGenerator.__init__", "modulename": "NDNT.modules.experiment_sampler", "qualname": "ExperimentBatchGenerator.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">timepoints</span><span class=\"p\">:</span> <span class=\"nb\">list</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">shuffle</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">random_seed</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "NDNT.modules.experiment_sampler.ExperimentBatchGenerator.batches", "modulename": "NDNT.modules.experiment_sampler", "qualname": "ExperimentBatchGenerator.batches", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.experiment_sampler.ExperimentBatchGenerator.current_batch_index", "modulename": "NDNT.modules.experiment_sampler", "qualname": "ExperimentBatchGenerator.current_batch_index", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.experiment_sampler.ExperimentBatchGenerator.next", "modulename": "NDNT.modules.experiment_sampler", "qualname": "ExperimentBatchGenerator.next", "kind": "function", "doc": "<p>Next method of the generator.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>the next batch</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">list</span>:</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.experiment_sampler.ExperimentBatchIterator", "modulename": "NDNT.modules.experiment_sampler", "qualname": "ExperimentBatchIterator", "kind": "class", "doc": "<p>Iterator that returns batches of timepoints across experiments.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>exp_to_time (dict):</strong>  map from experiment indices to timepoints</li>\n<li><strong>exp_batch_sizes (list):</strong>  list of batch sizes for each experiment</li>\n<li><strong>shuffle (bool):</strong>  whether to shuffle the batches</li>\n<li><strong>random_seed (int):</strong>  random seed to use for shuffling</li>\n</ul>\n", "bases": "typing.Iterator"}, {"fullname": "NDNT.modules.experiment_sampler.ExperimentBatchIterator.__init__", "modulename": "NDNT.modules.experiment_sampler", "qualname": "ExperimentBatchIterator.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">exp_to_time</span>, </span><span class=\"param\"><span class=\"n\">exp_batch_sizes</span>, </span><span class=\"param\"><span class=\"n\">shuffle</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>, </span><span class=\"param\"><span class=\"n\">random_seed</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "NDNT.modules.experiment_sampler.ExperimentBatchIterator.exp_batch_generators", "modulename": "NDNT.modules.experiment_sampler", "qualname": "ExperimentBatchIterator.exp_batch_generators", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.experiment_sampler.construct_exp_to_time", "modulename": "NDNT.modules.experiment_sampler", "qualname": "construct_exp_to_time", "kind": "function", "doc": "<p>Construct a dictionary that maps experiment indices to timepoints.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>dataset (Dataset):</strong>  dataset to sample from</li>\n<li><strong>indices (list):</strong>  indices to filter by</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">dataset</span>, </span><span class=\"param\"><span class=\"n\">indices</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span>:</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.experiment_sampler.ExperimentSampler", "modulename": "NDNT.modules.experiment_sampler", "qualname": "ExperimentSampler", "kind": "class", "doc": "<p>Samples elements across experiments.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>dataset (Dataset):</strong>  dataset to sample from</li>\n<li><strong>batch_size (int):</strong>  size of the batch to sample</li>\n<li><strong>indices (list):</strong>  indices to sample from</li>\n<li><strong>shuffle (bool):</strong>  whether to shuffle the indices</li>\n<li><strong>random_seed (int):</strong>  random seed to use for shuffling</li>\n<li><strong>verbose (bool):</strong>  whether to print out information about the sampler</li>\n</ul>\n", "bases": "typing.Generic[+T_co]"}, {"fullname": "NDNT.modules.experiment_sampler.ExperimentSampler.__init__", "modulename": "NDNT.modules.experiment_sampler", "qualname": "ExperimentSampler.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">dataset</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">indices</span><span class=\"p\">:</span> <span class=\"nb\">list</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">shuffle</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">random_seed</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">False</span></span>)</span>"}, {"fullname": "NDNT.modules.experiment_sampler.ExperimentSampler.dataset", "modulename": "NDNT.modules.experiment_sampler", "qualname": "ExperimentSampler.dataset", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.experiment_sampler.ExperimentSampler.batch_size", "modulename": "NDNT.modules.experiment_sampler", "qualname": "ExperimentSampler.batch_size", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.experiment_sampler.ExperimentSampler.shuffle", "modulename": "NDNT.modules.experiment_sampler", "qualname": "ExperimentSampler.shuffle", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.experiment_sampler.ExperimentSampler.random_seed", "modulename": "NDNT.modules.experiment_sampler", "qualname": "ExperimentSampler.random_seed", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.experiment_sampler.ExperimentSampler.num_exps", "modulename": "NDNT.modules.experiment_sampler", "qualname": "ExperimentSampler.num_exps", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.experiment_sampler.ExperimentSampler.exp_to_time", "modulename": "NDNT.modules.experiment_sampler", "qualname": "ExperimentSampler.exp_to_time", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.experiment_sampler.ExperimentSampler.exp_lengths", "modulename": "NDNT.modules.experiment_sampler", "qualname": "ExperimentSampler.exp_lengths", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.experiment_sampler.ExperimentSampler.exp_batch_sizes", "modulename": "NDNT.modules.experiment_sampler", "qualname": "ExperimentSampler.exp_batch_sizes", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.experiment_sampler.ExperimentSampler.num_batches", "modulename": "NDNT.modules.experiment_sampler", "qualname": "ExperimentSampler.num_batches", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers", "modulename": "NDNT.modules.layers", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.bilayers", "modulename": "NDNT.modules.layers.bilayers", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.bilayers.BinocShiftLayer", "modulename": "NDNT.modules.layers.bilayers", "qualname": "BinocShiftLayer", "kind": "class", "doc": "<p>This processes monocular output that spans 2*NX and fits weight for each filter (decoding binocularity)\nand shift using mu and sigma\nweights are across shifts for each input filter (number output filters = num inputs)\nalso chooses shift for each filter</p>\n", "bases": "NDNT.modules.layers.ndnlayer.NDNLayer"}, {"fullname": "NDNT.modules.layers.bilayers.BinocShiftLayer.__init__", "modulename": "NDNT.modules.layers.bilayers", "qualname": "BinocShiftLayer.__init__", "kind": "function", "doc": "<p>Same arguments as ConvLayer, but will make binocular filter with range of shifts. This assumes\ninput from ConvLayer (not BiConv) with num_filters x 72 input dims (can adjust) </p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  tuple or list of ints, (num_channels, height, width, lags)</li>\n<li><strong>filter_width:</strong>  width of convolutional filter</li>\n<li><strong>num_filters:</strong>  number of output filters</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">init_sigma</span><span class=\"o\">=</span><span class=\"mi\">3</span>,</span><span class=\"param\">\t<span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">weights_initializer</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">norm_type</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.bilayers.BinocShiftLayer.NX", "modulename": "NDNT.modules.layers.bilayers", "qualname": "BinocShiftLayer.NX", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.bilayers.BinocShiftLayer.shifts", "modulename": "NDNT.modules.layers.bilayers", "qualname": "BinocShiftLayer.shifts", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.bilayers.BinocShiftLayer.sigmas", "modulename": "NDNT.modules.layers.bilayers", "qualname": "BinocShiftLayer.sigmas", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.bilayers.BinocShiftLayer.filter_pos", "modulename": "NDNT.modules.layers.bilayers", "qualname": "BinocShiftLayer.filter_pos", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.bilayers.BinocShiftLayer.gelu", "modulename": "NDNT.modules.layers.bilayers", "qualname": "BinocShiftLayer.gelu", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.bilayers.BinocShiftLayer.batch_sample", "modulename": "NDNT.modules.layers.bilayers", "qualname": "BinocShiftLayer.batch_sample", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.bilayers.BinocShiftLayer.sample", "modulename": "NDNT.modules.layers.bilayers", "qualname": "BinocShiftLayer.sample", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.bilayers.BinocShiftLayer.sample_mode", "modulename": "NDNT.modules.layers.bilayers", "qualname": "BinocShiftLayer.sample_mode", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.bilayers.BinocShiftLayer.mult", "modulename": "NDNT.modules.layers.bilayers", "qualname": "BinocShiftLayer.mult", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.bilayers.BinocShiftLayer.norm_sample", "modulename": "NDNT.modules.layers.bilayers", "qualname": "BinocShiftLayer.norm_sample", "kind": "function", "doc": "<p>Returns a gaussian (or zeroed) sample, given the batch_size and to_sample parameter</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>batch_size (int):</strong>  size of the batch</li>\n<li><strong>to_sample (bool/None):</strong>  determines whether we draw a sample from Gaussian distribution, \nN(shifts,sigmas), defined per filter or use the mean of the Gaussian distribution without \nsampling. If to_sample is None (default), samples from the Gaussian during training phase and\nfixes to the mean during evaluation phase. Note that if to_sample is True/False, it overrides \nthe model_state (i.e training or eval) and does as instructed</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>out_norm: batch_size x num_filters x shifts applied to all positions</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch_size</span>, </span><span class=\"param\"><span class=\"n\">to_sample</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.bilayers.BinocShiftLayer.preprocess_weights", "modulename": "NDNT.modules.layers.bilayers", "qualname": "BinocShiftLayer.preprocess_weights", "kind": "function", "doc": "<p>self.weight is a list of the binocular weighting of each filter</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">gelu_mult</span><span class=\"o\">=</span><span class=\"mf\">2.0</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.bilayers.BinocShiftLayer.forward", "modulename": "NDNT.modules.layers.bilayers", "qualname": "BinocShiftLayer.forward", "kind": "function", "doc": "<p>Propagates the input forwards through the readout</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  input data</li>\n<li><strong>shift (bool):</strong>  shifts the location of the grid (from eye-tracking data)</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>y: neuronal activity</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">shift</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.bilayers.BinocShiftLayer.layer_dict", "modulename": "NDNT.modules.layers.bilayers", "qualname": "BinocShiftLayer.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  list of 4 ints, dimensions of input</li>\n<li><strong>num_filters:</strong>  int, number of filters in layer</li>\n<li><strong>bias:</strong>  bool, whether to include bias term</li>\n<li><strong>NLtype:</strong>  str, type of nonlinearity to use (see activations.py)</li>\n<li><strong>norm_type:</strong>  int, type of filter normalization to use (0=None, 1=filters are unit vectors, 2=maxnorm?)</li>\n<li><strong>pos_constraint:</strong>  bool, whether to constrain weights to be positive</li>\n<li><strong>num_inh:</strong>  int, number of inhibitory units (creates ei_mask and makes \"inhibitory\" units have negative output)</li>\n<li><strong>initialize_center:</strong>  bool, whether to initialize the weights to have a Gaussian envelope</li>\n<li><strong>reg_vals:</strong>  dict, regularization values to use (see regularizers.py)</li>\n<li><strong>output_norm:</strong>  str, type of output normalization to use</li>\n<li><strong>weights_initializer:</strong>  str, type of weight initialization to use</li>\n<li><strong>bias_initializer:</strong>  str, type of bias initialization to use</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>dict: dictionary of layer parameters</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">init_sigma</span><span class=\"o\">=</span><span class=\"mi\">3</span>, </span><span class=\"param\"><span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"mi\">0</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.bilayers.BinocShiftLayerOld", "modulename": "NDNT.modules.layers.bilayers", "qualname": "BinocShiftLayerOld", "kind": "class", "doc": "<p>Alternative: this processes monocular output that spans 2*NX and fits weight for each filter (decoding binocularity)\nand shift using mu and sigma</p>\n\n<p>Makes binocular filters from filter bank of monocular filters and applies to binocular stimulus. The monocular\nfilters are convolutional (filter_width x num_lags) where filter_width is specified and num_lags matches input\ndims. Input dims should have 2 channels (and only one dim space): 2 x NX x 1 x num_lags</p>\n", "bases": "NDNT.modules.layers.convlayers.ConvLayer"}, {"fullname": "NDNT.modules.layers.bilayers.BinocShiftLayerOld.__init__", "modulename": "NDNT.modules.layers.bilayers", "qualname": "BinocShiftLayerOld.__init__", "kind": "function", "doc": "<p>Same arguments as ConvLayer, but will make binocular filter with range of shifts</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  tuple or list of ints, (num_channels, height, width, lags)</li>\n<li><strong>filter_width:</strong>  width of convolutional filter</li>\n<li><strong>num_filters:</strong>  number of output filters</li>\n<li><strong>padding:</strong>  'same' or 'valid' (default 'same')</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">filter_width</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_shifts</span><span class=\"o\">=</span><span class=\"mi\">11</span>,</span><span class=\"param\">\t<span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.bilayers.BinocShiftLayerOld.shiftsL", "modulename": "NDNT.modules.layers.bilayers", "qualname": "BinocShiftLayerOld.shiftsL", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.bilayers.BinocShiftLayerOld.shiftsR", "modulename": "NDNT.modules.layers.bilayers", "qualname": "BinocShiftLayerOld.shiftsR", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.bilayers.BinocShiftLayerOld.output_dims", "modulename": "NDNT.modules.layers.bilayers", "qualname": "BinocShiftLayerOld.output_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.bilayers.BinocShiftLayerOld.preprocess_weights", "modulename": "NDNT.modules.layers.bilayers", "qualname": "BinocShiftLayerOld.preprocess_weights", "kind": "function", "doc": "<p>Preprocess the weights before using them in the forward pass.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>w: torch.Tensor, preprocessed weights</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.bilayers.BinocLayer1D", "modulename": "NDNT.modules.layers.bilayers", "qualname": "BinocLayer1D", "kind": "class", "doc": "<p>Takes a monocular convolutional output over both eyes -- assumes first spatial dimension is doubled\n-- reinterprets input as separate filters from each eye, but keeps them grouped\n-- assumes each 2 input filters (for each eye -- 4 inputs total) are inputs to each output filter</p>\n", "bases": "NDNT.modules.layers.convlayers.ConvLayer"}, {"fullname": "NDNT.modules.layers.bilayers.BinocLayer1D.__init__", "modulename": "NDNT.modules.layers.bilayers", "qualname": "BinocLayer1D.__init__", "kind": "function", "doc": "<p>Same arguments as ConvLayer, but will reshape output to divide space in half</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  tuple or list of ints, (num_channels, height, width, lags)</li>\n<li><strong>num_filters:</strong>  number of output filters</li>\n<li><strong>padding:</strong>  'same' or 'valid' (default 'same')</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.bilayers.BinocLayer1D.forward", "modulename": "NDNT.modules.layers.bilayers", "qualname": "BinocLayer1D.forward", "kind": "function", "doc": "<p>Call conv forward, but then option to reshape</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  tensor of shape [batch, num_channels, height, width, lags]</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>y: tensor of shape [batch, num_outputs]</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.bilayers.BinocLayer1D.layer_dict", "modulename": "NDNT.modules.layers.bilayers", "qualname": "BinocLayer1D.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.bilayers.BiConvLayer1D", "modulename": "NDNT.modules.layers.bilayers", "qualname": "BiConvLayer1D", "kind": "class", "doc": "<p>Filters that act solely on filter-dimension (dim-0)</p>\n", "bases": "NDNT.modules.layers.convlayers.ConvLayer"}, {"fullname": "NDNT.modules.layers.bilayers.BiConvLayer1D.__init__", "modulename": "NDNT.modules.layers.bilayers", "qualname": "BiConvLayer1D.__init__", "kind": "function", "doc": "<p>Same arguments as ConvLayer, but will reshape output to divide space in half.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  tuple or list of ints, (num_channels, height, width, lags)</li>\n<li><strong>num_filters:</strong>  number of output filters</li>\n<li><strong>filter_dims:</strong>  width of convolutional kernel (int or list of ints)</li>\n<li><strong>padding:</strong>  'same' or 'valid' (default 'same')</li>\n<li><strong>weight_init:</strong>  str, 'uniform', 'normal', 'xavier', 'zeros', or None</li>\n<li><strong>bias_init:</strong>  str, 'uniform', 'normal', 'xavier', 'zeros', or None</li>\n<li><strong>bias:</strong>  bool, whether to include bias term</li>\n<li><strong>NLtype:</strong>  str, 'lin', 'relu', 'tanh', 'sigmoid', 'elu', 'none'</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.bilayers.BiConvLayer1D.layer_dict", "modulename": "NDNT.modules.layers.bilayers", "qualname": "BiConvLayer1D.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.bilayers.BiSTconv1D", "modulename": "NDNT.modules.layers.bilayers", "qualname": "BiSTconv1D", "kind": "class", "doc": "<p>To be BinocMixLayer (bimix):\nInputs of size B x C x 2xNX x 1: mixes 2 eyes based on ratio\n    filter number is NC -- one for each channel: so infered from input_dims\n    filter_dims is [1,1,1,1] -> one number per filter\nORIGINAL BiSTconv1D: Filters that act solely on filter-dimension (dim-0)</p>\n", "bases": "NDNT.modules.layers.ndnlayer.NDNLayer"}, {"fullname": "NDNT.modules.layers.bilayers.BiSTconv1D.__init__", "modulename": "NDNT.modules.layers.bilayers", "qualname": "BiSTconv1D.__init__", "kind": "function", "doc": "<p>Same arguments as ConvLayer, but will reshape output to divide space in half.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  tuple or list of ints, (num_channels, height, width, lags)</li>\n<li><strong>num_filters:</strong>  number of output filters</li>\n<li><strong>filter_dims:</strong>  width of convolutional kernel (int or list of ints)</li>\n<li><strong>padding:</strong>  'same' or 'valid' (default 'same')</li>\n<li><strong>weight_init:</strong>  str, 'uniform', 'normal', 'xavier', 'zeros', or None</li>\n<li><strong>bias_init:</strong>  str, 'uniform', 'normal', 'xavier', 'zeros', or None</li>\n<li><strong>bias:</strong>  bool, whether to include bias term</li>\n<li><strong>NLtype:</strong>  str, 'lin', 'relu', 'tanh', 'sigmoid', 'elu', 'none'</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">filter_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">norm_type</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">NLtype</span><span class=\"o\">=</span><span class=\"s1\">&#39;relu&#39;</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.bilayers.BiSTconv1D.output_dims", "modulename": "NDNT.modules.layers.bilayers", "qualname": "BiSTconv1D.output_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.bilayers.BiSTconv1D.num_outputs", "modulename": "NDNT.modules.layers.bilayers", "qualname": "BiSTconv1D.num_outputs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.bilayers.BiSTconv1D.forward", "modulename": "NDNT.modules.layers.bilayers", "qualname": "BiSTconv1D.forward", "kind": "function", "doc": "<p>Call conv forward, but then option to reshape.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  tensor of shape [batch, num_channels, height, width, lags]</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>y: tensor of shape [batch, num_outputs]</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.bilayers.BiSTconv1D.layer_dict", "modulename": "NDNT.modules.layers.bilayers", "qualname": "BiSTconv1D.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  list of 4 ints, dimensions of input</li>\n<li><strong>num_filters:</strong>  int, number of filters in layer</li>\n<li><strong>bias:</strong>  bool, whether to include bias term</li>\n<li><strong>NLtype:</strong>  str, type of nonlinearity to use (see activations.py)</li>\n<li><strong>norm_type:</strong>  int, type of filter normalization to use (0=None, 1=filters are unit vectors, 2=maxnorm?)</li>\n<li><strong>pos_constraint:</strong>  bool, whether to constrain weights to be positive</li>\n<li><strong>num_inh:</strong>  int, number of inhibitory units (creates ei_mask and makes \"inhibitory\" units have negative output)</li>\n<li><strong>initialize_center:</strong>  bool, whether to initialize the weights to have a Gaussian envelope</li>\n<li><strong>reg_vals:</strong>  dict, regularization values to use (see regularizers.py)</li>\n<li><strong>output_norm:</strong>  str, type of output normalization to use</li>\n<li><strong>weights_initializer:</strong>  str, type of weight initialization to use</li>\n<li><strong>bias_initializer:</strong>  str, type of bias initialization to use</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>dict: dictionary of layer parameters</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.bilayers.ChannelConvLayer", "modulename": "NDNT.modules.layers.bilayers", "qualname": "ChannelConvLayer", "kind": "class", "doc": "<p>Channel-Convolutional NDN Layer -- convolutional layer that has each output filter use different\ngroup of M input filters. So, if there are N output filters, the channel dimension of the input has to be M*N</p>\n\n<p>Args (required):\n    input_dims: tuple or list of ints, (num_channels, height, width, lags)\n    num_filters: number of output filters\n    filter_dims: width of convolutional kernel (int or list of ints)</p>\n\n<p>Args (optional):\n    padding: 'same' or 'valid' (default 'same')\n    weight_init: str, 'uniform', 'normal', 'xavier', 'zeros', or None\n    bias_init: str, 'uniform', 'normal', 'xavier', 'zeros', or None\n    bias: bool, whether to include bias term\n    NLtype: str, 'lin', 'relu', 'tanh', 'sigmoid', 'elu', 'none'</p>\n", "bases": "NDNT.modules.layers.convlayers.ConvLayer"}, {"fullname": "NDNT.modules.layers.bilayers.ChannelConvLayer.__init__", "modulename": "NDNT.modules.layers.bilayers", "qualname": "ChannelConvLayer.__init__", "kind": "function", "doc": "<p>Same arguments as ConvLayer, but will reshape output to divide space in half.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  tuple or list of ints, (num_channels, height, width, lags)</li>\n<li><strong>num_filters:</strong>  number of output filters</li>\n<li><strong>filter_width:</strong>  width of convolutional kernel (int or list of ints)</li>\n<li><strong>temporal_tent_spacing:</strong>  spacing of tent-basis functions in time</li>\n<li><strong>output_norm:</strong>  normalization to apply to output</li>\n<li><strong>window:</strong>  window function to apply to output</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">filter_width</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">temporal_tent_spacing</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">output_norm</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">window</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.bilayers.ChannelConvLayer.weights_folded_dims", "modulename": "NDNT.modules.layers.bilayers", "qualname": "ChannelConvLayer.weights_folded_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.bilayers.ChannelConvLayer.layer_dict", "modulename": "NDNT.modules.layers.bilayers", "qualname": "ChannelConvLayer.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">filter_width</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.bilayers.ChannelConvLayer.forward", "modulename": "NDNT.modules.layers.bilayers", "qualname": "ChannelConvLayer.forward", "kind": "function", "doc": "<p>Call conv forward, but then option to reshape.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  tensor of shape [batch, num_channels, height, width, lags]</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>y: tensor of shape [batch, num_outputs]</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.convlayers", "modulename": "NDNT.modules.layers.convlayers", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.convlayers.ConvLayer", "modulename": "NDNT.modules.layers.convlayers", "qualname": "ConvLayer", "kind": "class", "doc": "<p>Convolutional NDN Layer</p>\n\n<p>Args (required):\n    input_dims: tuple or list of ints, (num_channels, height, width, lags)\n    num_filters: number of output filters\n    filter_dims: width of convolutional kernel (int or list of ints)\nArgs (optional):\n    padding: 'same','valid', or 'circular' (default 'same')\n    weight_init: str, 'uniform', 'normal', 'xavier', 'zeros', or None\n    bias_init: str, 'uniform', 'normal', 'xavier', 'zeros', or None\n    bias: bool, whether to include bias term\n    NLtype: str, 'lin', 'relu', 'tanh', 'sigmoid', 'elu', 'none'</p>\n", "bases": "NDNT.modules.layers.ndnlayer.NDNLayer"}, {"fullname": "NDNT.modules.layers.convlayers.ConvLayer.__init__", "modulename": "NDNT.modules.layers.convlayers", "qualname": "ConvLayer.__init__", "kind": "function", "doc": "<p>Initialize the layer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  list of 4 ints, dimensions of input</li>\n<li><strong>num_filters:</strong>  int, number of filters in layer</li>\n<li><strong>NLtype:</strong>  str, type of nonlinearity to use (see activations.py)</li>\n<li><strong>norm:</strong>  int, type of filter normalization to use (0=None, 1=filters are unit vectors, 2=maxnorm?)</li>\n<li><strong>pos_constraint:</strong>  bool, whether to constrain weights to be positive</li>\n<li><strong>num_inh:</strong>  int, number of inhibitory units (creates ei_mask and makes \"inhibitory\" units have negative output)</li>\n<li><strong>bias:</strong>  bool, whether to include bias term</li>\n<li><strong>weight_init:</strong>  str, type of weight initialization to use (see reset_parameters, default 'xavier_uniform')</li>\n<li><strong>bias_init:</strong>  str, type of bias initialization to use (see reset_parameters, default 'zeros')</li>\n<li><strong>reg_vals:</strong>  dict, regularization values to use (see regularizers.py)</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">filter_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">temporal_tent_spacing</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">output_norm</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">stride</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">dilation</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"s1\">&#39;same&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">res_layer</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">window</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.convlayers.ConvLayer.window", "modulename": "NDNT.modules.layers.convlayers", "qualname": "ConvLayer.window", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.convlayers.ConvLayer.tent_basis", "modulename": "NDNT.modules.layers.convlayers", "qualname": "ConvLayer.tent_basis", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.convlayers.ConvLayer.is1D", "modulename": "NDNT.modules.layers.convlayers", "qualname": "ConvLayer.is1D", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.convlayers.ConvLayer.res_layer", "modulename": "NDNT.modules.layers.convlayers", "qualname": "ConvLayer.res_layer", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.convlayers.ConvLayer.filter_dims", "modulename": "NDNT.modules.layers.convlayers", "qualname": "ConvLayer.filter_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.convlayers.ConvLayer.padding", "modulename": "NDNT.modules.layers.convlayers", "qualname": "ConvLayer.padding", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.convlayers.ConvLayer.folded_dims", "modulename": "NDNT.modules.layers.convlayers", "qualname": "ConvLayer.folded_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.convlayers.ConvLayer.batchnorm_clone", "modulename": "NDNT.modules.layers.convlayers", "qualname": "ConvLayer.batchnorm_clone", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">bn_orig</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.convlayers.ConvLayer.batchnorm_convert", "modulename": "NDNT.modules.layers.convlayers", "qualname": "ConvLayer.batchnorm_convert", "kind": "function", "doc": "<p>Converts layer with batch_norm to have the same output without batch_norm. This involves\nadjusting the weights and adding offsets</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.convlayers.ConvLayer.info", "modulename": "NDNT.modules.layers.convlayers", "qualname": "ConvLayer.info", "kind": "function", "doc": "<p>This outputs the layer information in abbrev (default) or expanded format</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">expand</span><span class=\"o\">=</span><span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"n\">to_output</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.convlayers.ConvLayer.layer_dict", "modulename": "NDNT.modules.layers.convlayers", "qualname": "ConvLayer.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"s1\">&#39;same&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">filter_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">res_layer</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">window</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.convlayers.ConvLayer.preprocess_weights", "modulename": "NDNT.modules.layers.convlayers", "qualname": "ConvLayer.preprocess_weights", "kind": "function", "doc": "<p>Preprocess the weights before using them in the forward pass.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>w: torch.Tensor, preprocessed weights</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">mod_weight</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.convlayers.ConvLayer.forward", "modulename": "NDNT.modules.layers.convlayers", "qualname": "ConvLayer.forward", "kind": "function", "doc": "<p>Forward pass for the layer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  torch.Tensor, input tensor</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>x: torch.Tensor, output tensor</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.convlayers.TconvLayer", "modulename": "NDNT.modules.layers.convlayers", "qualname": "TconvLayer", "kind": "class", "doc": "<p>Temporal convolutional layer.\nTConv does not integrate out the time dimension and instead treats it as a true convolutional dimension</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims (list of ints):</strong>  input dimensions [C, W, H, T]</li>\n<li><strong>num_filters (int):</strong>  number of filters</li>\n<li><strong>filter_dims (list of ints):</strong>  filter dimensions [C, w, h, T]\nw &lt; W, h &lt; H</li>\n<li><strong>stride (int):</strong>  stride of convolution</li>\n</ul>\n", "bases": "ConvLayer"}, {"fullname": "NDNT.modules.layers.convlayers.TconvLayer.__init__", "modulename": "NDNT.modules.layers.convlayers", "qualname": "TconvLayer.__init__", "kind": "function", "doc": "<p>Initialize the layer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  list of 4 ints, dimensions of input</li>\n<li><strong>num_filters:</strong>  int, number of filters in layer</li>\n<li><strong>NLtype:</strong>  str, type of nonlinearity to use (see activations.py)</li>\n<li><strong>norm:</strong>  int, type of filter normalization to use (0=None, 1=filters are unit vectors, 2=maxnorm?)</li>\n<li><strong>pos_constraint:</strong>  bool, whether to constrain weights to be positive</li>\n<li><strong>num_inh:</strong>  int, number of inhibitory units (creates ei_mask and makes \"inhibitory\" units have negative output)</li>\n<li><strong>bias:</strong>  bool, whether to include bias term</li>\n<li><strong>weight_init:</strong>  str, type of weight initialization to use (see reset_parameters, default 'xavier_uniform')</li>\n<li><strong>bias_init:</strong>  str, type of bias initialization to use (see reset_parameters, default 'zeros')</li>\n<li><strong>reg_vals:</strong>  dict, regularization values to use (see regularizers.py)</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">conv_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">filter_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"s1\">&#39;spatial&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">output_norm</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.convlayers.TconvLayer.num_lags", "modulename": "NDNT.modules.layers.convlayers", "qualname": "TconvLayer.num_lags", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.convlayers.TconvLayer.is1D", "modulename": "NDNT.modules.layers.convlayers", "qualname": "TconvLayer.is1D", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.convlayers.TconvLayer.padding", "modulename": "NDNT.modules.layers.convlayers", "qualname": "TconvLayer.padding", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.convlayers.TconvLayer.forward", "modulename": "NDNT.modules.layers.convlayers", "qualname": "TconvLayer.forward", "kind": "function", "doc": "<p>Forward pass for the layer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  torch.Tensor, input tensor</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>x: torch.Tensor, output tensor</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.convlayers.TconvLayer.plot_filters", "modulename": "NDNT.modules.layers.convlayers", "qualname": "TconvLayer.plot_filters", "kind": "function", "doc": "<p>Plot the filters in the layer. It first determines whether layer is spatiotemporal (STRF plot)\nor \"internal\": the different being that spatiotemporal will have lags / dim-3 of filter</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>cmaps:</strong>  str or colormap, colormap to use for plotting (default 'gray')</li>\n<li><strong>num_cols:</strong>  int, number of columns to use in plot (default 8)</li>\n<li><strong>row_height:</strong>  int, number of rows to use in plot (default 2)</li>\n<li><strong>time_reverse:</strong>  bool, whether to reverse the time dimension (default depends on dimension)</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">cmaps</span><span class=\"o\">=</span><span class=\"s1\">&#39;viridis&#39;</span>, </span><span class=\"param\"><span class=\"n\">num_cols</span><span class=\"o\">=</span><span class=\"mi\">8</span>, </span><span class=\"param\"><span class=\"n\">row_height</span><span class=\"o\">=</span><span class=\"mi\">2</span>, </span><span class=\"param\"><span class=\"n\">time_reverse</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.convlayers.TconvLayer.layer_dict", "modulename": "NDNT.modules.layers.convlayers", "qualname": "TconvLayer.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"s1\">&#39;spatial&#39;</span>, </span><span class=\"param\"><span class=\"n\">conv_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.convlayers.STconvLayer", "modulename": "NDNT.modules.layers.convlayers", "qualname": "STconvLayer", "kind": "class", "doc": "<p>Spatio-temporal convolutional layer.\nSTConv Layers overload the batch dimension and assume they are contiguous in time.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims (list of ints):</strong>  input dimensions [C, W, H, T]\nThis is, of course, not the real input dimensions, because the batch dimension is assumed to be contiguous.\nThe real input dimensions are [B, C, W, H, 1], T specifies the number of lags</li>\n<li><strong>num_filters (int):</strong>  number of filters</li>\n<li><strong>filter_dims (list of ints):</strong>  filter dimensions [C, w, h, T]\nw &lt; W, h &lt; H</li>\n<li><strong>stride (int):</strong>  stride of convolution</li>\n</ul>\n", "bases": "TconvLayer"}, {"fullname": "NDNT.modules.layers.convlayers.STconvLayer.__init__", "modulename": "NDNT.modules.layers.convlayers", "qualname": "STconvLayer.__init__", "kind": "function", "doc": "<p>Initialize the layer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  list of 4 ints, dimensions of input</li>\n<li><strong>num_filters:</strong>  int, number of filters in layer</li>\n<li><strong>NLtype:</strong>  str, type of nonlinearity to use (see activations.py)</li>\n<li><strong>norm:</strong>  int, type of filter normalization to use (0=None, 1=filters are unit vectors, 2=maxnorm?)</li>\n<li><strong>pos_constraint:</strong>  bool, whether to constrain weights to be positive</li>\n<li><strong>num_inh:</strong>  int, number of inhibitory units (creates ei_mask and makes \"inhibitory\" units have negative output)</li>\n<li><strong>bias:</strong>  bool, whether to include bias term</li>\n<li><strong>weight_init:</strong>  str, type of weight initialization to use (see reset_parameters, default 'xavier_uniform')</li>\n<li><strong>bias_init:</strong>  str, type of bias initialization to use (see reset_parameters, default 'zeros')</li>\n<li><strong>reg_vals:</strong>  dict, regularization values to use (see regularizers.py)</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">output_norm</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.convlayers.STconvLayer.num_lags", "modulename": "NDNT.modules.layers.convlayers", "qualname": "STconvLayer.num_lags", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.convlayers.STconvLayer.output_dims", "modulename": "NDNT.modules.layers.convlayers", "qualname": "STconvLayer.output_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.convlayers.STconvLayer.forward", "modulename": "NDNT.modules.layers.convlayers", "qualname": "STconvLayer.forward", "kind": "function", "doc": "<p>Forward pass for the layer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  torch.Tensor, input tensor</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>x: torch.Tensor, output tensor</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.convlayers.STconvLayer.plot_filters", "modulename": "NDNT.modules.layers.convlayers", "qualname": "STconvLayer.plot_filters", "kind": "function", "doc": "<p>Plot the filters in the layer. It first determines whether layer is spatiotemporal (STRF plot)\nor \"internal\": the different being that spatiotemporal will have lags / dim-3 of filter</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>cmaps:</strong>  str or colormap, colormap to use for plotting (default 'gray')</li>\n<li><strong>num_cols:</strong>  int, number of columns to use in plot (default 8)</li>\n<li><strong>row_height:</strong>  int, number of rows to use in plot (default 2)</li>\n<li><strong>time_reverse:</strong>  bool, whether to reverse the time dimension (default depends on dimension)</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">cmaps</span><span class=\"o\">=</span><span class=\"s1\">&#39;gray&#39;</span>, </span><span class=\"param\"><span class=\"n\">num_cols</span><span class=\"o\">=</span><span class=\"mi\">8</span>, </span><span class=\"param\"><span class=\"n\">row_height</span><span class=\"o\">=</span><span class=\"mi\">2</span>, </span><span class=\"param\"><span class=\"n\">time_reverse</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.convlayers.STconvLayer.layer_dict", "modulename": "NDNT.modules.layers.convlayers", "qualname": "STconvLayer.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.dimlayers", "modulename": "NDNT.modules.layers.dimlayers", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.dimlayers.Dim0Layer", "modulename": "NDNT.modules.layers.dimlayers", "qualname": "Dim0Layer", "kind": "class", "doc": "<p>Filters that act solely on filter-dimension (dim-0).</p>\n", "bases": "NDNT.modules.layers.ndnlayer.NDNLayer"}, {"fullname": "NDNT.modules.layers.dimlayers.Dim0Layer.__init__", "modulename": "NDNT.modules.layers.dimlayers", "qualname": "Dim0Layer.__init__", "kind": "function", "doc": "<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  tuple or list of ints, (num_channels, height, width, lags)</li>\n<li><strong>num_filters:</strong>  number of output filters</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.dimlayers.Dim0Layer.output_dims", "modulename": "NDNT.modules.layers.dimlayers", "qualname": "Dim0Layer.output_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.dimlayers.Dim0Layer.num_outputs", "modulename": "NDNT.modules.layers.dimlayers", "qualname": "Dim0Layer.num_outputs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.dimlayers.Dim0Layer.num_other_dims", "modulename": "NDNT.modules.layers.dimlayers", "qualname": "Dim0Layer.num_other_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.dimlayers.Dim0Layer.forward", "modulename": "NDNT.modules.layers.dimlayers", "qualname": "Dim0Layer.forward", "kind": "function", "doc": "<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  torch.Tensor, input tensor of shape [B, C, H, W, L]</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>y: torch.Tensor, output tensor of shape [B, N, H, W, L]</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.dimlayers.Dim0Layer.dim_info", "modulename": "NDNT.modules.layers.dimlayers", "qualname": "Dim0Layer.dim_info", "kind": "function", "doc": "<p>This uses the methods in the init to determine the input_dims, output_dims, filter_dims, and actual size of\nthe weight tensor (weight_shape), given inputs, and package in a dictionary. This should be overloaded with each\nchild of NDNLayer if want to use -- but this is external to function of actual layer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  tuple or list of ints, (num_channels, height, width, lags)</li>\n<li><strong>num_filters:</strong>  number of output filters</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>dinfo: dictionary with keys 'input_dims', 'filter_dims', 'output_dims', 'num_outputs', 'weight_shape'</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.dimlayers.Dim0Layer.layer_dict", "modulename": "NDNT.modules.layers.dimlayers", "qualname": "Dim0Layer.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.dimlayers.ChannelLayer", "modulename": "NDNT.modules.layers.dimlayers", "qualname": "ChannelLayer", "kind": "class", "doc": "<p>Applies individual filter for each filter (dim0) dimension, preserving separate channels but filtering over other dimensions\n=&gt; num_filters equals the channel dimension by definition, otherwise like NDNLayer</p>\n", "bases": "NDNT.modules.layers.ndnlayer.NDNLayer"}, {"fullname": "NDNT.modules.layers.dimlayers.ChannelLayer.__init__", "modulename": "NDNT.modules.layers.dimlayers", "qualname": "ChannelLayer.__init__", "kind": "function", "doc": "<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  tuple or list of ints, (num_channels, height, width, lags)</li>\n<li><strong>num_filters:</strong>  number of output filters</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.dimlayers.ChannelLayer.layer_dict", "modulename": "NDNT.modules.layers.dimlayers", "qualname": "ChannelLayer.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.dimlayers.ChannelLayer.forward", "modulename": "NDNT.modules.layers.dimlayers", "qualname": "ChannelLayer.forward", "kind": "function", "doc": "<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  torch.Tensor, input tensor of shape [B, C, H, W, L]</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>y: torch.Tensor, output tensor of shape [B, N, H, W, L]</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.dimlayers.DimSPLayer", "modulename": "NDNT.modules.layers.dimlayers", "qualname": "DimSPLayer", "kind": "class", "doc": "<p>Filters that act solely on spatial-dimensions (dims-1,3)\ntransparent=True means that one spatial filter for each channel</p>\n", "bases": "NDNT.modules.layers.ndnlayer.NDNLayer"}, {"fullname": "NDNT.modules.layers.dimlayers.DimSPLayer.__init__", "modulename": "NDNT.modules.layers.dimlayers", "qualname": "DimSPLayer.__init__", "kind": "function", "doc": "<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  tuple or list of ints, (num_channels, height, width, lags)</li>\n<li><strong>num_filters:</strong>  number of output filters</li>\n<li><strong>transparent:</strong>  if True, one spatial filter for each channel</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">transparent</span><span class=\"o\">=</span><span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.dimlayers.DimSPLayer.transparent", "modulename": "NDNT.modules.layers.dimlayers", "qualname": "DimSPLayer.transparent", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.dimlayers.DimSPLayer.num_outputs", "modulename": "NDNT.modules.layers.dimlayers", "qualname": "DimSPLayer.num_outputs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.dimlayers.DimSPLayer.num_sp_dims", "modulename": "NDNT.modules.layers.dimlayers", "qualname": "DimSPLayer.num_sp_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.dimlayers.DimSPLayer.forward", "modulename": "NDNT.modules.layers.dimlayers", "qualname": "DimSPLayer.forward", "kind": "function", "doc": "<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  torch.Tensor, input tensor of shape [B, C, H, W, L]</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>y: torch.Tensor, output tensor of shape [B, N, H, W, L]</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.dimlayers.DimSPLayer.layer_dict", "modulename": "NDNT.modules.layers.dimlayers", "qualname": "DimSPLayer.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">transparent</span><span class=\"o\">=</span><span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.dimlayers.DimSPTLayer", "modulename": "NDNT.modules.layers.dimlayers", "qualname": "DimSPTLayer", "kind": "class", "doc": "<p>Filters that act solely on spatial-dimensions (dims-1,3).</p>\n", "bases": "NDNT.modules.layers.ndnlayer.NDNLayer"}, {"fullname": "NDNT.modules.layers.dimlayers.DimSPTLayer.__init__", "modulename": "NDNT.modules.layers.dimlayers", "qualname": "DimSPTLayer.__init__", "kind": "function", "doc": "<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  tuple or list of ints, (num_channels, height, width, lags)</li>\n<li><strong>num_filters:</strong>  number of output filters</li>\n<li><strong>transparent:</strong>  if True, one spatial filter for each channel</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">transparent</span><span class=\"o\">=</span><span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.dimlayers.DimSPTLayer.output_dims", "modulename": "NDNT.modules.layers.dimlayers", "qualname": "DimSPTLayer.output_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.dimlayers.DimSPTLayer.num_outputs", "modulename": "NDNT.modules.layers.dimlayers", "qualname": "DimSPTLayer.num_outputs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.dimlayers.DimSPTLayer.transparent", "modulename": "NDNT.modules.layers.dimlayers", "qualname": "DimSPTLayer.transparent", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.dimlayers.DimSPTLayer.num_spt_dims", "modulename": "NDNT.modules.layers.dimlayers", "qualname": "DimSPTLayer.num_spt_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.dimlayers.DimSPTLayer.forward", "modulename": "NDNT.modules.layers.dimlayers", "qualname": "DimSPTLayer.forward", "kind": "function", "doc": "<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  torch.Tensor, input tensor of shape [B, C, H, W, L]</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>y: torch.Tensor, output tensor of shape [B, N, H, W, L]</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.dimlayers.DimSPTLayer.layer_dict", "modulename": "NDNT.modules.layers.dimlayers", "qualname": "DimSPTLayer.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">transparent</span><span class=\"o\">=</span><span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.externallayer", "modulename": "NDNT.modules.layers.externallayer", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.externallayer.ExternalLayer", "modulename": "NDNT.modules.layers.externallayer", "qualname": "ExternalLayer", "kind": "class", "doc": "<p>This is a dummy 'layer' for the Extenal network that gets filled in by the passed-in network.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "NDNT.modules.layers.externallayer.ExternalLayer.__init__", "modulename": "NDNT.modules.layers.externallayer", "qualname": "ExternalLayer.__init__", "kind": "function", "doc": "<p>ExternalLayer: Dummy layer for the Extenal network that gets filled in by the passed-in network.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  tuple or list of ints, (num_channels, height, width, lags)</li>\n<li><strong>num_filters:</strong>  number of output filters</li>\n<li><strong>output_dims:</strong>  tuple or list of ints, (num_channels, height, width, lags)</li>\n<li><strong>**kwargs:</strong>  additional arguments to pass to NDNLayer</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">output_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.externallayer.ExternalLayer.input_dims", "modulename": "NDNT.modules.layers.externallayer", "qualname": "ExternalLayer.input_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.externallayer.ExternalLayer.num_filters", "modulename": "NDNT.modules.layers.externallayer", "qualname": "ExternalLayer.num_filters", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.externallayer.ExternalLayer.filter_dims", "modulename": "NDNT.modules.layers.externallayer", "qualname": "ExternalLayer.filter_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.externallayer.ExternalLayer.output_dims", "modulename": "NDNT.modules.layers.externallayer", "qualname": "ExternalLayer.output_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.externallayer.ExternalLayer.reg", "modulename": "NDNT.modules.layers.externallayer", "qualname": "ExternalLayer.reg", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.externallayer.ExternalLayer.forward", "modulename": "NDNT.modules.layers.externallayer", "qualname": "ExternalLayer.forward", "kind": "function", "doc": "<p>Forward pass for ExternalLayer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  input tensor</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>y: output tensor</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.laglayers", "modulename": "NDNT.modules.layers.laglayers", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.laglayers.LagLayer", "modulename": "NDNT.modules.layers.laglayers", "qualname": "LagLayer", "kind": "class", "doc": "<p>Operates spatiotemporal filter om time-embeeded stimulus\nSpatiotemporal filter should have less lags than stimulus -- then ends up with some lags left\nFilter is full spatial width and the number of lags is explicity specified in initializer\n(so, inherits spatial and chanel dimensions of stimulus input)</p>\n\n<p>Args (required):\n    input_dims: tuple or list of ints, (num_channels, height, width, lags)\n    num_filters: number of output filters\n    num_lags: number of lags in spatiotemporal filter</p>\n\n<p>Args (optional):\n    weight_init: str, 'uniform', 'normal', 'xavier', 'zeros', or None\n    bias_init: str, 'uniform', 'normal', 'xavier', 'zeros', or None\n    bias: bool, whether to include bias term\n    NLtype: str, 'lin', 'relu', 'tanh', 'sigmoid', 'elu', 'none'</p>\n", "bases": "NDNT.modules.layers.ndnlayer.NDNLayer"}, {"fullname": "NDNT.modules.layers.laglayers.LagLayer.__init__", "modulename": "NDNT.modules.layers.laglayers", "qualname": "LagLayer.__init__", "kind": "function", "doc": "<p>Initialize the layer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  tuple or list of ints, (num_channels, height, width, lags)</li>\n<li><strong>num_filters:</strong>  int, number of output filters</li>\n<li><strong>num_lags:</strong>  int, number of lags in spatiotemporal filter</li>\n<li><strong>**kwargs:</strong>  keyword arguments to pass to the parent class</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">num_lags</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.laglayers.LagLayer.num_lags", "modulename": "NDNT.modules.layers.laglayers", "qualname": "LagLayer.num_lags", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.laglayers.LagLayer.num_folded_dims", "modulename": "NDNT.modules.layers.laglayers", "qualname": "LagLayer.num_folded_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.laglayers.LagLayer.forward", "modulename": "NDNT.modules.layers.laglayers", "qualname": "LagLayer.forward", "kind": "function", "doc": "<p>Forward pass through the layer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  torch.Tensor, input tensor of shape (batch_size, *input_dims)</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>y: torch.Tensor, output tensor of shape (batch_size, *output_dims)</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.laglayers.LagLayer.plot_filters", "modulename": "NDNT.modules.layers.laglayers", "qualname": "LagLayer.plot_filters", "kind": "function", "doc": "<p>Overload plot_filters to automatically time_reverse.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>cmaps:</strong>  str or list of str, colormap(s) to use for plotting</li>\n<li><strong>num_cols:</strong>  int, number of columns in the plot</li>\n<li><strong>row_height:</strong>  int, height of each row in the plot</li>\n<li><strong>time_reverse:</strong>  bool, whether to reverse the time axis</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>fig: matplotlib.figure.Figure, the figure object\n  axs: list of matplotlib.axes.Axes, the axes objects</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">cmaps</span><span class=\"o\">=</span><span class=\"s1\">&#39;gray&#39;</span>, </span><span class=\"param\"><span class=\"n\">num_cols</span><span class=\"o\">=</span><span class=\"mi\">8</span>, </span><span class=\"param\"><span class=\"n\">row_height</span><span class=\"o\">=</span><span class=\"mi\">2</span>, </span><span class=\"param\"><span class=\"n\">time_reverse</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.laglayers.LagLayer.layer_dict", "modulename": "NDNT.modules.layers.laglayers", "qualname": "LagLayer.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>num_lags:</strong>  int, number of lags in spatiotemporal filter</li>\n<li><strong>**kwargs:</strong>  keyword arguments to pass to the parent class</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Ldict: dict, dictionary of layer parameters</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">num_lags</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.lvlayers", "modulename": "NDNT.modules.layers.lvlayers", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.lvlayers.LVLayer", "modulename": "NDNT.modules.layers.lvlayers", "qualname": "LVLayer", "kind": "class", "doc": "<p>Generates output at based on a number of LVs, sampled by time point.\nEach LV has T weights, and can be many LVs (so weight matrix is T x NLVs.\nRequires specifically formatted input that passes in indices of LVs and relative\nweight, so will linearly interpolate. Also, can have trial-structure so will have \ntemporal reg that does not need to go across trials</p>\n\n<p>Input will specifically be of form of (for each batch point):\n[index1, index2, w1]  and output will be w1<em>LV(i1) + (1-w1)</em>LV(i2)</p>\n", "bases": "NDNT.modules.layers.ndnlayer.NDNLayer"}, {"fullname": "NDNT.modules.layers.lvlayers.LVLayer.__init__", "modulename": "NDNT.modules.layers.lvlayers", "qualname": "LVLayer.__init__", "kind": "function", "doc": "<p>If num_trials is None, then assumes one continuous sequence, otherwise will assume num_time_pnts\nis per-trial, and dimensionality of filter is [num_trials, 1, 1, num_time_pnts].</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>num_time_pnts:</strong>  int, number of time points</li>\n<li><strong>num_lvs:</strong>  int, number of latent variables</li>\n<li><strong>num_trials:</strong>  int, number of trials</li>\n<li><strong>norm_type:</strong>  int, normalization type</li>\n<li><strong>weights_initializer:</strong>  str, 'uniform', 'normal', 'xavier', 'zeros', or None</li>\n<li><strong>**kwargs:</strong>  keyword arguments to pass to the parent class</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">num_time_pnts</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_lvs</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_trials</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">norm_type</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">weights_initializer</span><span class=\"o\">=</span><span class=\"s1\">&#39;normal&#39;</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.lvlayers.LVLayer.preprocess_weights", "modulename": "NDNT.modules.layers.lvlayers", "qualname": "LVLayer.preprocess_weights", "kind": "function", "doc": "<p>Preprocesses weights by applying positivity constraint and normalization.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>w: torch.Tensor, preprocessed weights</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.lvlayers.LVLayer.forward", "modulename": "NDNT.modules.layers.lvlayers", "qualname": "LVLayer.forward", "kind": "function", "doc": "<p>Assumes input of B x 3 (where three numbers are indexes of 2 surrounding LV and relative weight of LV1).</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  torch.Tensor, input tensor.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>y: torch.Tensor, output tensor.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.lvlayers.LVLayer.layer_dict", "modulename": "NDNT.modules.layers.lvlayers", "qualname": "LVLayer.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>num_time_pnts:</strong>  int, number of time points</li>\n<li><strong>num_lvs:</strong>  int, number of latent variables</li>\n<li><strong>num_trials:</strong>  int, number of trials</li>\n<li><strong>weights_initializer:</strong>  str, 'uniform', 'normal', 'xavier', 'zeros', or None</li>\n<li><strong>**kwargs:</strong>  keyword arguments to pass to the parent class</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">num_time_pnts</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_lvs</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_trials</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">weights_initializer</span><span class=\"o\">=</span><span class=\"s1\">&#39;normal&#39;</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.lvlayers.LVLayerOLD", "modulename": "NDNT.modules.layers.lvlayers", "qualname": "LVLayerOLD", "kind": "class", "doc": "<p>No input. Produces LVs sampled from mu/sigma at each time step\nCould make tent functions for smoothness over time as well</p>\n", "bases": "NDNT.modules.layers.ndnlayer.NDNLayer"}, {"fullname": "NDNT.modules.layers.lvlayers.LVLayerOLD.__init__", "modulename": "NDNT.modules.layers.lvlayers", "qualname": "LVLayerOLD.__init__", "kind": "function", "doc": "<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li>num_time_points</li>\n<li><strong>num_lvs:</strong>  default 1</li>\n<li><strong>init_mu_range:</strong>  default 0.1</li>\n<li><strong>init_sigma:</strong> </li>\n<li><strong>sm_reg:</strong>  smoothness regularization penalty</li>\n<li><strong>gauss_type:</strong>  isotropic</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">num_time_pnts</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_lvs</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">init_mu_range</span><span class=\"o\">=</span><span class=\"mf\">0.5</span>,</span><span class=\"param\">\t<span class=\"n\">init_sigma</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">sigma_shape</span><span class=\"o\">=</span><span class=\"s1\">&#39;lv&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.lvlayers.LVLayerOLD.numLVs", "modulename": "NDNT.modules.layers.lvlayers", "qualname": "LVLayerOLD.numLVs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.lvlayers.LVLayerOLD.nt", "modulename": "NDNT.modules.layers.lvlayers", "qualname": "LVLayerOLD.nt", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.lvlayers.LVLayerOLD.init_mu_range", "modulename": "NDNT.modules.layers.lvlayers", "qualname": "LVLayerOLD.init_mu_range", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.lvlayers.LVLayerOLD.init_sigma", "modulename": "NDNT.modules.layers.lvlayers", "qualname": "LVLayerOLD.init_sigma", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.lvlayers.LVLayerOLD.sigma", "modulename": "NDNT.modules.layers.lvlayers", "qualname": "LVLayerOLD.sigma", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.lvlayers.LVLayerOLD.sm_skips", "modulename": "NDNT.modules.layers.lvlayers", "qualname": "LVLayerOLD.sm_skips", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.lvlayers.LVLayerOLD.sample", "modulename": "NDNT.modules.layers.lvlayers", "qualname": "LVLayerOLD.sample", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.lvlayers.LVLayerOLD.weight_scale", "modulename": "NDNT.modules.layers.lvlayers", "qualname": "LVLayerOLD.weight_scale", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.lvlayers.LVLayerOLD.forward", "modulename": "NDNT.modules.layers.lvlayers", "qualname": "LVLayerOLD.forward", "kind": "function", "doc": "<p>Input has to be time-indices of what data is being indexed</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  time-indices of LVs</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>z: output LVs</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.lvlayers.LVLayerOLD.layer_dict", "modulename": "NDNT.modules.layers.lvlayers", "qualname": "LVLayerOLD.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>num_time_pnts:</strong>  int, number of time points</li>\n<li><strong>num_lvs:</strong>  int, number of latent variables</li>\n<li><strong>init_mu:</strong>  float, initial mean value</li>\n<li><strong>init_sigma:</strong>  float, initial sigma value</li>\n<li><strong>sigma_shape:</strong>  str, 'full', 'lv', or 'time'</li>\n<li><strong>**kwargs:</strong>  keyword arguments to pass to the parent class</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Ldict: dict, dictionary of layer parameters</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">num_time_pnts</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">init_mu</span><span class=\"o\">=</span><span class=\"mf\">0.5</span>,</span><span class=\"param\">\t<span class=\"n\">init_sigma</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">num_lvs</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">sigma_shape</span><span class=\"o\">=</span><span class=\"s1\">&#39;lv&#39;</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.masklayers", "modulename": "NDNT.modules.layers.masklayers", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.masklayers.MaskLayer", "modulename": "NDNT.modules.layers.masklayers", "qualname": "MaskLayer", "kind": "class", "doc": "<p>MaskLayer: Layer with a mask applied to the weights.</p>\n", "bases": "NDNT.modules.layers.ndnlayer.NDNLayer"}, {"fullname": "NDNT.modules.layers.masklayers.MaskLayer.__init__", "modulename": "NDNT.modules.layers.masklayers", "qualname": "MaskLayer.__init__", "kind": "function", "doc": "<p>MaskLayer: Layer with a mask applied to the weights.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  tuple or list of ints, (num_channels, height, width, lags)</li>\n<li><strong>num_filters:</strong>  number of output filters</li>\n<li><strong>## NOT CANT BE PART OF DICTIONARY mask:</strong>  np.ndarray, mask to apply to the weights</li>\n<li><strong>NLtype:</strong>  str, 'lin', 'relu', 'tanh', 'sigmoid', 'elu', 'none'</li>\n<li><strong>norm_type:</strong>  int, normalization type</li>\n<li><strong>pos_constraint:</strong>  int, whether to enforce non-negative weights</li>\n<li><strong>num_inh:</strong>  int, number of inhibitory filters</li>\n<li><strong>bias:</strong>  bool, whether to include bias term</li>\n<li><strong>weights_initializer:</strong>  str, 'uniform', 'normal', 'xavier', 'zeros', or None</li>\n<li><strong>output_norm:</strong>  str, 'batch', 'batchX', or None</li>\n<li><strong>initialize_center:</strong>  bool, whether to initialize the center</li>\n<li><strong>bias_initializer:</strong>  str, 'uniform', 'normal', 'xavier', 'zeros', or None</li>\n<li><strong>reg_vals:</strong>  dict, regularization values</li>\n<li><strong>**kwargs:</strong>  additional arguments to pass to NDNLayer</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">NLtype</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;lin&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">norm_type</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">pos_constraint</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">num_inh</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">weights_initializer</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;xavier_uniform&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">output_norm</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">initialize_center</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">bias_initializer</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;zeros&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">reg_vals</span><span class=\"p\">:</span> <span class=\"nb\">dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.masklayers.MaskLayer.set_mask", "modulename": "NDNT.modules.layers.masklayers", "qualname": "MaskLayer.set_mask", "kind": "function", "doc": "<p>Sets mask -- instead of plugging in by hand. Registers mask as being set, and checks dimensions.\nCan also use to rest to have trivial mask (all 1s)\nMask will be numpy, leave mask blank if want to set to default mask (all ones)</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>mask:</strong>  numpy array of size of filters (filter_dims x num_filters)</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.masklayers.MaskLayer.preprocess_weights", "modulename": "NDNT.modules.layers.masklayers", "qualname": "MaskLayer.preprocess_weights", "kind": "function", "doc": "<p>Preprocess weights for MaskLayer.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>w: torch.Tensor, preprocessed weights</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.masklayers.MaskLayer.layer_dict", "modulename": "NDNT.modules.layers.masklayers", "qualname": "MaskLayer.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>mask:</strong>  np.ndarray, mask to apply to the weights</li>\n<li><strong>**kwargs:</strong>  additional arguments to pass to NDNLayer</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Ldict: dict, dictionary of layer parameters</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.masklayers.MaskSTconvLayer", "modulename": "NDNT.modules.layers.masklayers", "qualname": "MaskSTconvLayer", "kind": "class", "doc": "<p>MaskSTConvLayer: STconvLayer with a mask applied to the weights.</p>\n", "bases": "NDNT.modules.layers.convlayers.STconvLayer"}, {"fullname": "NDNT.modules.layers.masklayers.MaskSTconvLayer.__init__", "modulename": "NDNT.modules.layers.masklayers", "qualname": "MaskSTconvLayer.__init__", "kind": "function", "doc": "<p>MaskSTconvLayer: STconvLayer with a mask applied to the weights.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  tuple or list of ints, (num_channels, height, width, lags)</li>\n<li><strong>num_filters:</strong>  number of output filters</li>\n<li><strong>mask:</strong>  np.ndarray, mask to apply to the weights</li>\n<li><strong>output_norm:</strong>  str, 'batch', 'batchX', or None</li>\n<li><strong>**kwargs:</strong>  additional arguments to pass to STConvLayer</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">initialize_center</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">output_norm</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.masklayers.MaskSTconvLayer.set_mask", "modulename": "NDNT.modules.layers.masklayers", "qualname": "MaskSTconvLayer.set_mask", "kind": "function", "doc": "<p>Sets mask -- instead of plugging in by hand. Registers mask as being set, and checks dimensions.\nCan also use to rest to have trivial mask (all 1s)\nMask will be numpy, leave mask blank if want to set to default mask (all ones)</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>mask:</strong>  numpy array of size of filters (filter_dims x num_filters)</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.masklayers.MaskSTconvLayer.preprocess_weights", "modulename": "NDNT.modules.layers.masklayers", "qualname": "MaskSTconvLayer.preprocess_weights", "kind": "function", "doc": "<p>Preprocess weights for MaskLayer.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>w: torch.Tensor, preprocessed weights</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.masklayers.MaskSTconvLayer.layer_dict", "modulename": "NDNT.modules.layers.masklayers", "qualname": "MaskSTconvLayer.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>mask:</strong>  np.ndarray, mask to apply to the weights</li>\n<li><strong>**kwargs:</strong>  additional arguments to pass to NDNLayer</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Ldict: dict, dictionary of layer parameters</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.ndnlayer", "modulename": "NDNT.modules.layers.ndnlayer", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.ndnlayer.NDNLayer", "modulename": "NDNT.modules.layers.ndnlayer", "qualname": "NDNLayer", "kind": "class", "doc": "<p>Base class for NDN layers.\nHandles the weight initialization and regularization</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  list of 4 ints, dimensions of input\n[Channels, Height, Width, Lags]\nuse 1 if the dimension is not used</li>\n<li><strong>num_filters:</strong>  int, number of filters in layer</li>\n<li><strong>NLtype:</strong>  str, type of nonlinearity to use (see activations.py)</li>\n<li><strong>norm:</strong>  int, type of filter normalization to use (0=None, 1=filters are unit vectors)</li>\n<li><strong>pos_constraint:</strong>  bool, whether to constrain weights to be positive</li>\n<li><strong>num_inh:</strong>  int, number of inhibitory units (creates ei_mask and makes \"inhibitory\" units have negative output)</li>\n<li><strong>bias:</strong>  bool, whether to include bias term</li>\n<li><strong>weight_init:</strong>  str, type of weight initialization to use (see reset_parameters, default 'xavier_uniform')</li>\n<li><strong>bias_init:</strong>  str, type of bias initialization to use (see reset_parameters, default 'zeros')</li>\n<li><strong>reg_vals:</strong>  dict, regularization values to use (see regularizers.py)</li>\n</ul>\n\n<p>NDNLayers have parameters 'weight' and 'bias' (if bias=True)</p>\n\n<h6 id=\"the-forward-has-steps\">The forward has steps:</h6>\n\n<blockquote>\n  <ol>\n  <li>preprocess_weights</li>\n  <li>main computation (this is just a linear layer for the base class)</li>\n  <li>nonlinearity</li>\n  <li>ei_mask</li>\n  </ol>\n</blockquote>\n\n<p>weight is always flattened to a vector, and then reshaped to the appropriate size\nuse get_weights() to get the weights in the correct shape</p>\n\n<p>preprocess_weights() applies positive constraint and normalization if requested\ncompute_reg_loss() computes the regularization loss for the regularization specified in reg_vals</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "NDNT.modules.layers.ndnlayer.NDNLayer.__init__", "modulename": "NDNT.modules.layers.ndnlayer", "qualname": "NDNLayer.__init__", "kind": "function", "doc": "<p>Initialize the layer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  list of 4 ints, dimensions of input</li>\n<li><strong>num_filters:</strong>  int, number of filters in layer</li>\n<li><strong>NLtype:</strong>  str, type of nonlinearity to use (see activations.py)</li>\n<li><strong>norm:</strong>  int, type of filter normalization to use (0=None, 1=filters are unit vectors, 2=maxnorm?)</li>\n<li><strong>pos_constraint:</strong>  bool, whether to constrain weights to be positive</li>\n<li><strong>num_inh:</strong>  int, number of inhibitory units (creates ei_mask and makes \"inhibitory\" units have negative output)</li>\n<li><strong>bias:</strong>  bool, whether to include bias term</li>\n<li><strong>weight_init:</strong>  str, type of weight initialization to use (see reset_parameters, default 'xavier_uniform')</li>\n<li><strong>bias_init:</strong>  str, type of bias initialization to use (see reset_parameters, default 'zeros')</li>\n<li><strong>reg_vals:</strong>  dict, regularization values to use (see regularizers.py)</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">filter_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">NLtype</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;lin&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">norm_type</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">pos_constraint</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">num_inh</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">weights_initializer</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;xavier_uniform&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">initialize_center</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">bias_initializer</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;zeros&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">reg_vals</span><span class=\"p\">:</span> <span class=\"nb\">dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.ndnlayer.NDNLayer.input_dims", "modulename": "NDNT.modules.layers.ndnlayer", "qualname": "NDNLayer.input_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.ndnlayer.NDNLayer.num_filters", "modulename": "NDNT.modules.layers.ndnlayer", "qualname": "NDNLayer.num_filters", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.ndnlayer.NDNLayer.output_dims", "modulename": "NDNT.modules.layers.ndnlayer", "qualname": "NDNLayer.output_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.ndnlayer.NDNLayer.norm_type", "modulename": "NDNT.modules.layers.ndnlayer", "qualname": "NDNLayer.norm_type", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.ndnlayer.NDNLayer.pos_constraint", "modulename": "NDNT.modules.layers.ndnlayer", "qualname": "NDNLayer.pos_constraint", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.ndnlayer.NDNLayer.conv", "modulename": "NDNT.modules.layers.ndnlayer", "qualname": "NDNLayer.conv", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.ndnlayer.NDNLayer.NL", "modulename": "NDNT.modules.layers.ndnlayer", "qualname": "NDNLayer.NL", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.ndnlayer.NDNLayer.shape", "modulename": "NDNT.modules.layers.ndnlayer", "qualname": "NDNLayer.shape", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.ndnlayer.NDNLayer.weight_scale", "modulename": "NDNT.modules.layers.ndnlayer", "qualname": "NDNLayer.weight_scale", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.ndnlayer.NDNLayer.weight", "modulename": "NDNT.modules.layers.ndnlayer", "qualname": "NDNLayer.weight", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.ndnlayer.NDNLayer.reg", "modulename": "NDNT.modules.layers.ndnlayer", "qualname": "NDNLayer.reg", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.ndnlayer.NDNLayer.num_inh", "modulename": "NDNT.modules.layers.ndnlayer", "qualname": "NDNLayer.num_inh", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.ndnlayer.NDNLayer.reset_parameters", "modulename": "NDNT.modules.layers.ndnlayer", "qualname": "NDNLayer.reset_parameters", "kind": "function", "doc": "<p>Initialize the weights and bias parameters of the layer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>weights_initializer:</strong>  str, type of weight initialization to use\noptions: 'xavier_uniform', 'xavier_normal', 'kaiming_uniform', 'kaiming_normal', 'orthogonal', 'zeros', 'ones'</li>\n<li><strong>bias_initializer:</strong>  str, type of bias initialization to use\noptions: 'uniform', 'normal', 'zeros', 'ones'</li>\n<li><strong>param:</strong>  float or list of floats, parameter for weight initialization</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">weights_initializer</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">bias_initializer</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">param</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.ndnlayer.NDNLayer.initialize_gaussian_envelope", "modulename": "NDNT.modules.layers.ndnlayer", "qualname": "NDNLayer.initialize_gaussian_envelope", "kind": "function", "doc": "<p>Initialize the weights to have a Gaussian envelope.\nThis is useful for initializing filters to have a spatial-temporal Gaussian shape.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.ndnlayer.NDNLayer.preprocess_weights", "modulename": "NDNT.modules.layers.ndnlayer", "qualname": "NDNLayer.preprocess_weights", "kind": "function", "doc": "<p>Preprocess the weights before using them in the forward pass.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>w: torch.Tensor, preprocessed weights</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.ndnlayer.NDNLayer.forward", "modulename": "NDNT.modules.layers.ndnlayer", "qualname": "NDNLayer.forward", "kind": "function", "doc": "<p>Forward pass for the layer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  torch.Tensor, input tensor</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>x: torch.Tensor, output tensor</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.ndnlayer.NDNLayer.compute_reg_loss", "modulename": "NDNT.modules.layers.ndnlayer", "qualname": "NDNLayer.compute_reg_loss", "kind": "function", "doc": "<p>Compute the regularization loss for the layer by calling reg_module function on\nthe preprocessed weights.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li>None</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>reg_loss: torch.Tensor, regularization loss</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.ndnlayer.NDNLayer.get_weights", "modulename": "NDNT.modules.layers.ndnlayer", "qualname": "NDNLayer.get_weights", "kind": "function", "doc": "<p>num-inh can take into account previous layer inhibition weights.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>to_reshape:</strong>  bool, whether to reshape the weights to the original filter shape</li>\n<li><strong>time_reverse:</strong>  bool, whether to reverse the time dimension</li>\n<li><strong>num_inh:</strong>  int, number of inhibitory units</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>ws: np.ndarray, weights of the layer, on the CPU</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">to_reshape</span><span class=\"o\">=</span><span class=\"kc\">True</span>, </span><span class=\"param\"><span class=\"n\">time_reverse</span><span class=\"o\">=</span><span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"n\">num_inh</span><span class=\"o\">=</span><span class=\"mi\">0</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.ndnlayer.NDNLayer.get_biases", "modulename": "NDNT.modules.layers.ndnlayer", "qualname": "NDNLayer.get_biases", "kind": "function", "doc": "<p>Return biases of layer in numpy array</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li>None</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>bs: np.ndarray, biases of the layer, on the CPU</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.ndnlayer.NDNLayer.list_parameters", "modulename": "NDNT.modules.layers.ndnlayer", "qualname": "NDNLayer.list_parameters", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.ndnlayer.NDNLayer.set_parameters", "modulename": "NDNT.modules.layers.ndnlayer", "qualname": "NDNLayer.set_parameters", "kind": "function", "doc": "<p>Turn fitting for named params on or off.\nIf name is none, do for whole layer.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">val</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.ndnlayer.NDNLayer.set_reg_val", "modulename": "NDNT.modules.layers.ndnlayer", "qualname": "NDNLayer.set_reg_val", "kind": "function", "doc": "<p>Set the regularization value for a given regularization type.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>reg_type:</strong>  str, type of regularization to set</li>\n<li><strong>reg_val:</strong>  float, value to set the regularization to</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">reg_type</span>, </span><span class=\"param\"><span class=\"n\">reg_val</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.ndnlayer.NDNLayer.plot_filters", "modulename": "NDNT.modules.layers.ndnlayer", "qualname": "NDNLayer.plot_filters", "kind": "function", "doc": "<p>Plot the filters in the layer. It first determines whether layer is spatiotemporal (STRF plot)\nor \"internal\": the different being that spatiotemporal will have lags / dim-3 of filter</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>cmaps:</strong>  str or colormap, colormap to use for plotting (default 'gray')</li>\n<li><strong>num_cols:</strong>  int, number of columns to use in plot (default 8)</li>\n<li><strong>row_height:</strong>  int, number of rows to use in plot (default 2)</li>\n<li><strong>time_reverse:</strong>  bool, whether to reverse the time dimension (default depends on dimension)</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">time_reverse</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.ndnlayer.NDNLayer.info", "modulename": "NDNT.modules.layers.ndnlayer", "qualname": "NDNLayer.info", "kind": "function", "doc": "<p>This outputs the layer information in abbrev (default) or expanded format</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">expand</span><span class=\"o\">=</span><span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"n\">to_output</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.ndnlayer.NDNLayer.dim_info", "modulename": "NDNT.modules.layers.ndnlayer", "qualname": "NDNLayer.dim_info", "kind": "function", "doc": "<p>This uses the methods in the init to determine the input_dims, output_dims, filter_dims, and actual size of\nthe weight tensor (weight_shape), given inputs, and package in a dictionary. This should be overloaded with each\nchild of NDNLayer if want to use -- but this is external to function of actual layer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  list of 4 ints, dimensions of input</li>\n<li><strong>num_filters:</strong>  int, number of filters in layer</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>dinfo: dict, dictionary of dimension information</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.ndnlayer.NDNLayer.layer_dict", "modulename": "NDNT.modules.layers.ndnlayer", "qualname": "NDNLayer.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  list of 4 ints, dimensions of input</li>\n<li><strong>num_filters:</strong>  int, number of filters in layer</li>\n<li><strong>bias:</strong>  bool, whether to include bias term</li>\n<li><strong>NLtype:</strong>  str, type of nonlinearity to use (see activations.py)</li>\n<li><strong>norm_type:</strong>  int, type of filter normalization to use (0=None, 1=filters are unit vectors, 2=maxnorm?)</li>\n<li><strong>pos_constraint:</strong>  bool, whether to constrain weights to be positive</li>\n<li><strong>num_inh:</strong>  int, number of inhibitory units (creates ei_mask and makes \"inhibitory\" units have negative output)</li>\n<li><strong>initialize_center:</strong>  bool, whether to initialize the weights to have a Gaussian envelope</li>\n<li><strong>reg_vals:</strong>  dict, regularization values to use (see regularizers.py)</li>\n<li><strong>output_norm:</strong>  str, type of output normalization to use</li>\n<li><strong>weights_initializer:</strong>  str, type of weight initialization to use</li>\n<li><strong>bias_initializer:</strong>  str, type of bias initialization to use</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>dict: dictionary of layer parameters</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">NLtype</span><span class=\"o\">=</span><span class=\"s1\">&#39;lin&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">norm_type</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">initialize_center</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">num_inh</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">pos_constraint</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">reg_vals</span><span class=\"o\">=</span><span class=\"p\">{}</span>,</span><span class=\"param\">\t<span class=\"n\">output_norm</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">weights_initializer</span><span class=\"o\">=</span><span class=\"s1\">&#39;xavier_uniform&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">bias_initializer</span><span class=\"o\">=</span><span class=\"s1\">&#39;zeros&#39;</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.normlayers", "modulename": "NDNT.modules.layers.normlayers", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.normlayers.DivNormLayer", "modulename": "NDNT.modules.layers.normlayers", "qualname": "DivNormLayer", "kind": "class", "doc": "<p>Divisive normalization implementation: not explicitly convolutional.</p>\n", "bases": "NDNT.modules.layers.ndnlayer.NDNLayer"}, {"fullname": "NDNT.modules.layers.normlayers.DivNormLayer.__init__", "modulename": "NDNT.modules.layers.normlayers", "qualname": "DivNormLayer.__init__", "kind": "function", "doc": "<p>Divisive normalization layer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  list or tuple of ints, dimensions of input tensor.</li>\n<li><strong>num_filters:</strong>  int, number of filters to use.</li>\n<li><strong>filter_dims:</strong>  list or tuple of ints, dimensions of filter tensor.</li>\n<li><strong>pos_constraint:</strong>  bool, if True, apply a positivity constraint to the weights.</li>\n<li><strong>bias:</strong>  bool, if True, include a bias term.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">filter_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">pos_constraint</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.normlayers.DivNormLayer.output_dims", "modulename": "NDNT.modules.layers.normlayers", "qualname": "DivNormLayer.output_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.normlayers.DivNormLayer.num_outputs", "modulename": "NDNT.modules.layers.normlayers", "qualname": "DivNormLayer.num_outputs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.normlayers.DivNormLayer.preprocess_weights", "modulename": "NDNT.modules.layers.normlayers", "qualname": "DivNormLayer.preprocess_weights", "kind": "function", "doc": "<p>Preprocesses weights by applying positivity constraint and normalization.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>w: torch.Tensor, preprocessed weights.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.normlayers.DivNormLayer.forward", "modulename": "NDNT.modules.layers.normlayers", "qualname": "DivNormLayer.forward", "kind": "function", "doc": "<p>Forward pass for divisive normalization layer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  torch.Tensor, input tensor.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>x: torch.Tensor, output tensor.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.orilayers", "modulename": "NDNT.modules.layers.orilayers", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.orilayers.OriLayer", "modulename": "NDNT.modules.layers.orilayers", "qualname": "OriLayer", "kind": "class", "doc": "<p>Orientation layer.</p>\n", "bases": "NDNT.modules.layers.ndnlayer.NDNLayer"}, {"fullname": "NDNT.modules.layers.orilayers.OriLayer.__init__", "modulename": "NDNT.modules.layers.orilayers", "qualname": "OriLayer.__init__", "kind": "function", "doc": "<p>Initialize orientation layer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  input dimensions</li>\n<li><strong>num_filters:</strong>  number of filters</li>\n<li><strong>filter_dims:</strong>  filter dimensions</li>\n<li><strong>angles:</strong>  angles for rotation (in degrees)</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">filter_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">angles</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.orilayers.OriLayer.angles", "modulename": "NDNT.modules.layers.orilayers", "qualname": "OriLayer.angles", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.orilayers.OriLayer.output_dims", "modulename": "NDNT.modules.layers.orilayers", "qualname": "OriLayer.output_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.orilayers.OriLayer.rotation_matrix_tensor", "modulename": "NDNT.modules.layers.orilayers", "qualname": "OriLayer.rotation_matrix_tensor", "kind": "function", "doc": "<p>Create a rotation matrix tensor for each angle in theta_list.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>filter_dims:</strong>  filter dimensions</li>\n<li><strong>theta_list:</strong>  list of angles in degrees</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>rotation_matrix_tensor: rotation matrix tensor</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">filter_dims</span>, </span><span class=\"param\"><span class=\"n\">theta_list</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.orilayers.OriLayer.forward", "modulename": "NDNT.modules.layers.orilayers", "qualname": "OriLayer.forward", "kind": "function", "doc": "<p>Forward pass through the layer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  torch.Tensor, input tensor of shape (batch_size, *input_dims)</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>y: torch.Tensor, output tensor of shape (batch_size, *output_dims)</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.orilayers.OriLayer.layer_dict", "modulename": "NDNT.modules.layers.orilayers", "qualname": "OriLayer.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>angles:</strong>  list of angles for rotation (in degrees)</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Ldict: dict, dictionary of layer parameters</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">angles</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.orilayers.OriConvLayer", "modulename": "NDNT.modules.layers.orilayers", "qualname": "OriConvLayer", "kind": "class", "doc": "<p>Orientation-Convolutional layer.</p>\n\n<p>Will detect if needs to expand to multiple orientations (first layer, original OriConv) or use group-convolutions</p>\n\n<p>2-d conv layer that creates and maintains a third convolutional dimension through grouping and weight sharing. In\nother words, a third convolutional dimension is passed in, but the filters here act on each element of that third\ndimension, and weight-share between different groups.</p>\n", "bases": "NDNT.modules.layers.convlayers.ConvLayer"}, {"fullname": "NDNT.modules.layers.orilayers.OriConvLayer.__init__", "modulename": "NDNT.modules.layers.orilayers", "qualname": "OriConvLayer.__init__", "kind": "function", "doc": "<p>Initialize orientation layer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  input dimensions</li>\n<li><strong>num_filters:</strong>  number of filters</li>\n<li><strong>filter_width:</strong>  filter spatial width -- the rest of filter dims is determined</li>\n<li><strong>angles:</strong>  angles for rotation (in degrees)</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">res_layer</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">filter_width</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"s1\">&#39;valid&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">output_norm</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">angles</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.orilayers.OriConvLayer.angles", "modulename": "NDNT.modules.layers.orilayers", "qualname": "OriConvLayer.angles", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.orilayers.OriConvLayer.oriented_input", "modulename": "NDNT.modules.layers.orilayers", "qualname": "OriConvLayer.oriented_input", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.orilayers.OriConvLayer.folded_dims", "modulename": "NDNT.modules.layers.orilayers", "qualname": "OriConvLayer.folded_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.orilayers.OriConvLayer.output_dims", "modulename": "NDNT.modules.layers.orilayers", "qualname": "OriConvLayer.output_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.orilayers.OriConvLayer.forward", "modulename": "NDNT.modules.layers.orilayers", "qualname": "OriConvLayer.forward", "kind": "function", "doc": "<p>Forward pass through the layer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  torch.Tensor, input tensor of shape (batch_size, *input_dims)</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>y: torch.Tensor, output tensor of shape (batch_size, *output_dims)</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.orilayers.OriConvLayer.layer_dict", "modulename": "NDNT.modules.layers.orilayers", "qualname": "OriConvLayer.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>angles:</strong>  list of angles for rotation (in degrees)</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Ldict: dict, dictionary of layer parameters</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">filter_width</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">angles</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.orilayers.ConvLayer3D", "modulename": "NDNT.modules.layers.orilayers", "qualname": "ConvLayer3D", "kind": "class", "doc": "<p>3D convolutional layer.</p>\n", "bases": "NDNT.modules.layers.convlayers.ConvLayer"}, {"fullname": "NDNT.modules.layers.orilayers.ConvLayer3D.__init__", "modulename": "NDNT.modules.layers.orilayers", "qualname": "ConvLayer3D.__init__", "kind": "function", "doc": "<p>Initialize 3D convolutional layer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  input dimensions</li>\n<li><strong>filter_width:</strong>  filter width</li>\n<li><strong>ori_filter_width:</strong>  orientation filter width</li>\n<li><strong>num_filters:</strong>  number of filters</li>\n<li><strong>output_norm:</strong>  output normalization</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"p\">:</span> <span class=\"nb\">list</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">filter_width</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">ori_filter_width</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_filters</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">output_norm</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.orilayers.ConvLayer3D.ori_padding", "modulename": "NDNT.modules.layers.orilayers", "qualname": "ConvLayer3D.ori_padding", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.orilayers.ConvLayer3D.input_dims", "modulename": "NDNT.modules.layers.orilayers", "qualname": "ConvLayer3D.input_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.orilayers.ConvLayer3D.output_dims", "modulename": "NDNT.modules.layers.orilayers", "qualname": "ConvLayer3D.output_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.orilayers.ConvLayer3D.forward", "modulename": "NDNT.modules.layers.orilayers", "qualname": "ConvLayer3D.forward", "kind": "function", "doc": "<p>Forward pass through the layer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  torch.Tensor, input tensor of shape (batch_size, *input_dims)</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>y: torch.Tensor, output tensor of shape (batch_size, *output_dims)</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.orilayers.ConvLayer3D.plot_filters", "modulename": "NDNT.modules.layers.orilayers", "qualname": "ConvLayer3D.plot_filters", "kind": "function", "doc": "<p>Plot the filters.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>cmaps:</strong>  color map</li>\n<li><strong>num_cols:</strong>  number of columns</li>\n<li><strong>row_height:</strong>  row height</li>\n<li><strong>time_reverse:</strong>  time reverse</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>fig: figure\n  axs: axes</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">cmaps</span><span class=\"o\">=</span><span class=\"s1\">&#39;gray&#39;</span>, </span><span class=\"param\"><span class=\"n\">num_cols</span><span class=\"o\">=</span><span class=\"mi\">8</span>, </span><span class=\"param\"><span class=\"n\">row_height</span><span class=\"o\">=</span><span class=\"mi\">2</span>, </span><span class=\"param\"><span class=\"n\">time_reverse</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.orilayers.ConvLayer3D.layer_dict", "modulename": "NDNT.modules.layers.orilayers", "qualname": "ConvLayer3D.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>filter_width:</strong>  filter width</li>\n<li><strong>ori_filter_width:</strong>  orientation filter width</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">filter_width</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">ori_filter_width</span><span class=\"o\">=</span><span class=\"mi\">1</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.orilayers.HermiteOriConvLayer", "modulename": "NDNT.modules.layers.orilayers", "qualname": "HermiteOriConvLayer", "kind": "class", "doc": "<p>HermiteOriConv Layer: An OriConvLayer whose filters are expressed in Hermite basis functions. From Ecker et al (2019)</p>\n", "bases": "NDNT.modules.layers.convlayers.ConvLayer"}, {"fullname": "NDNT.modules.layers.orilayers.HermiteOriConvLayer.__init__", "modulename": "NDNT.modules.layers.orilayers", "qualname": "HermiteOriConvLayer.__init__", "kind": "function", "doc": "<p>Initialize the layer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  list of 4 ints, dimensions of input</li>\n<li><strong>num_filters:</strong>  int, number of filters in layer</li>\n<li><strong>NLtype:</strong>  str, type of nonlinearity to use (see activations.py)</li>\n<li><strong>norm:</strong>  int, type of filter normalization to use (0=None, 1=filters are unit vectors, 2=maxnorm?)</li>\n<li><strong>pos_constraint:</strong>  bool, whether to constrain weights to be positive</li>\n<li><strong>num_inh:</strong>  int, number of inhibitory units (creates ei_mask and makes \"inhibitory\" units have negative output)</li>\n<li><strong>bias:</strong>  bool, whether to include bias term</li>\n<li><strong>weight_init:</strong>  str, type of weight initialization to use (see reset_parameters, default 'xavier_uniform')</li>\n<li><strong>bias_init:</strong>  str, type of bias initialization to use (see reset_parameters, default 'zeros')</li>\n<li><strong>reg_vals:</strong>  dict, regularization values to use (see regularizers.py)</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">hermite_rank</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">filter_width</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">output_norm</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">filter_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">angles</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.orilayers.HermiteOriConvLayer.hermite_rank", "modulename": "NDNT.modules.layers.orilayers", "qualname": "HermiteOriConvLayer.hermite_rank", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.orilayers.HermiteOriConvLayer.filter_width", "modulename": "NDNT.modules.layers.orilayers", "qualname": "HermiteOriConvLayer.filter_width", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.orilayers.HermiteOriConvLayer.angles", "modulename": "NDNT.modules.layers.orilayers", "qualname": "HermiteOriConvLayer.angles", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.orilayers.HermiteOriConvLayer.output_dims", "modulename": "NDNT.modules.layers.orilayers", "qualname": "HermiteOriConvLayer.output_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.orilayers.HermiteOriConvLayer.hermcgen", "modulename": "NDNT.modules.layers.orilayers", "qualname": "HermiteOriConvLayer.hermcgen", "kind": "function", "doc": "<p>Generate coefficients of 2D Hermite functions</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">mu</span>, </span><span class=\"param\"><span class=\"n\">nu</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.orilayers.HermiteOriConvLayer.hermite_2d", "modulename": "NDNT.modules.layers.orilayers", "qualname": "HermiteOriConvLayer.hermite_2d", "kind": "function", "doc": "<p>Generate 2D Hermite function basis</p>\n\n<p>Arguments:\nN           -- the maximum rank.\nnpts        -- the number of points in x and y</p>\n\n<p>Keyword arguments:\nxvalmax     -- the maximum x and y value (default: 2.5 * sqrt(N))</p>\n\n<p>Returns:\nH           -- Basis set of size N*(N+1)/2 x npts x npts\ndesc        -- List of descriptors specifying for each\n               basis function whether it is:\n                    'z': rotationally symmetric\n                    'r': real part of quadrature pair\n                    'i': imaginary part of quadrature pair</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">N</span>, </span><span class=\"param\"><span class=\"n\">npts</span>, </span><span class=\"param\"><span class=\"n\">xvalmax</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.orilayers.HermiteOriConvLayer.padding", "modulename": "NDNT.modules.layers.orilayers", "qualname": "HermiteOriConvLayer.padding", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.orilayers.HermiteOriConvLayer.forward", "modulename": "NDNT.modules.layers.orilayers", "qualname": "HermiteOriConvLayer.forward", "kind": "function", "doc": "<p>Forward pass through the layer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  torch.Tensor, input tensor of shape (batch_size, *input_dims)</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>y: torch.Tensor, output tensor of shape (batch_size, *output_dims)</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.orilayers.HermiteOriConvLayer.get_filters", "modulename": "NDNT.modules.layers.orilayers", "qualname": "HermiteOriConvLayer.get_filters", "kind": "function", "doc": "<p>num-inh can take into account previous layer inhibition weights.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>to_reshape:</strong>  bool, whether to reshape the weights to the original filter shape</li>\n<li><strong>time_reverse:</strong>  bool, whether to reverse the time dimension</li>\n<li><strong>num_inh:</strong>  int, number of inhibitory units</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>ws: np.ndarray, weights of the layer, on the CPU</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">num_inh</span><span class=\"o\">=</span><span class=\"mi\">0</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.orilayers.HermiteOriConvLayer.layer_dict", "modulename": "NDNT.modules.layers.orilayers", "qualname": "HermiteOriConvLayer.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>num_angles:</strong>  number of rotations </li>\n<li><strong>**kwargs:</strong>  additional arguments to pass to NDNLayer</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Ldict: dict, dictionary of layer parameters</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">hermite_rank</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">filter_width</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">basis</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">angles</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.partiallayers", "modulename": "NDNT.modules.layers.partiallayers", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.partiallayers.NDNLayerPartial", "modulename": "NDNT.modules.layers.partiallayers", "qualname": "NDNLayerPartial", "kind": "class", "doc": "<p>NDNLayerPartial: NDNLayer where only some of the weights are fit</p>\n", "bases": "NDNT.modules.layers.ndnlayer.NDNLayer"}, {"fullname": "NDNT.modules.layers.partiallayers.NDNLayerPartial.__init__", "modulename": "NDNT.modules.layers.partiallayers", "qualname": "NDNLayerPartial.__init__", "kind": "function", "doc": "<p>NDNLayerPartial: NDNLayer with only some of the weights in the layer fit, determined by\n    fixed_dims OR num_fixed_filters</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  tuple or list of ints, (num_channels, height, width, lags)</li>\n<li><strong>num_filters:</strong>  number of output filters</li>\n<li><strong>num_fixed_filters:</strong>  number of filters to hold constant (default=0)</li>\n<li><strong>fixed_dims:</strong>  4-d size of fixed dims, use -1 for not-touching dims. Should have 3 -1s</li>\n<li><strong>NLtype:</strong>  str, 'lin', 'relu', 'tanh', 'sigmoid', 'elu', 'none'</li>\n<li><strong>norm_type:</strong>  int, normalization type</li>\n<li><strong>pos_constraint:</strong>  int, whether to enforce non-negative weights</li>\n<li><strong>num_inh:</strong>  int, number of inhibitory filters</li>\n<li><strong>bias:</strong>  bool, whether to include bias term</li>\n<li><strong>weights_initializer:</strong>  str, 'uniform', 'normal', 'xavier', 'zeros', or None</li>\n<li><strong>output_norm:</strong>  str, 'batch', 'batchX', or None</li>\n<li><strong>initialize_center:</strong>  bool, whether to initialize the center</li>\n<li><strong>bias_initializer:</strong>  str, 'uniform', 'normal', 'xavier', 'zeros', or None</li>\n<li><strong>reg_vals:</strong>  dict, regularization values</li>\n<li><strong>**kwargs:</strong>  additional arguments to pass to NDNLayer</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_fixed_filters</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">fixed_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">fixed_num_inh</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">NLtype</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;lin&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">norm_type</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">pos_constraint</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">num_inh</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">weights_initializer</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;xavier_uniform&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">output_norm</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">initialize_center</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">bias_initializer</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;zeros&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">reg_vals</span><span class=\"p\">:</span> <span class=\"nb\">dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.partiallayers.NDNLayerPartial.dims_mod", "modulename": "NDNT.modules.layers.partiallayers", "qualname": "NDNLayerPartial.dims_mod", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.partiallayers.NDNLayerPartial.num_fixed_filters", "modulename": "NDNT.modules.layers.partiallayers", "qualname": "NDNLayerPartial.num_fixed_filters", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.partiallayers.NDNLayerPartial.num_filters", "modulename": "NDNT.modules.layers.partiallayers", "qualname": "NDNLayerPartial.num_filters", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.partiallayers.NDNLayerPartial.num_inh", "modulename": "NDNT.modules.layers.partiallayers", "qualname": "NDNLayerPartial.num_inh", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.partiallayers.NDNLayerPartial.preprocess_weights", "modulename": "NDNT.modules.layers.partiallayers", "qualname": "NDNLayerPartial.preprocess_weights", "kind": "function", "doc": "<p>Preprocess weights: assemble from real weights + fixed and then NDNLayer Process weights clipped in.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>w: torch.Tensor, preprocessed weights</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.partiallayers.NDNLayerPartial.layer_dict", "modulename": "NDNT.modules.layers.partiallayers", "qualname": "NDNLayerPartial.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>mask:</strong>  np.ndarray, mask to apply to the weights</li>\n<li><strong>**kwargs:</strong>  additional arguments to pass to NDNLayer</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Ldict: dict, dictionary of layer parameters</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">fixed_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">num_fixed_filters</span><span class=\"o\">=</span><span class=\"mi\">0</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.partiallayers.ConvLayerPartial", "modulename": "NDNT.modules.layers.partiallayers", "qualname": "ConvLayerPartial", "kind": "class", "doc": "<p>ConvLayerPartial: NDNLayer with only some of the weights in the layer fit, determined by\n    fixed_dims OR num_fixed_filters</p>\n", "bases": "NDNT.modules.layers.convlayers.ConvLayer"}, {"fullname": "NDNT.modules.layers.partiallayers.ConvLayerPartial.__init__", "modulename": "NDNT.modules.layers.partiallayers", "qualname": "ConvLayerPartial.__init__", "kind": "function", "doc": "<p>ConvLayerPartial: NDNLayer with only some of the weights in the layer fit, determined by\n    fixed_dims OR num_fixed_filters</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  tuple or list of ints, (num_channels, height, width, lags)</li>\n<li><strong>num_filters:</strong>  number of output filters</li>\n<li><strong>filter_dims:</strong>  width of convolutional kernel (int or list of ints)</li>\n<li><strong>num_fixed_filters:</strong>  number of filters to hold constant (default=0)</li>\n<li><strong>fixed_dims:</strong>  4-d size of fixed dims, use -1 for not-touching dims. Should have 3 -1s</li>\n<li><strong>NLtype:</strong>  str, 'lin', 'relu', 'tanh', 'sigmoid', 'elu', 'none'</li>\n<li><strong>norm_type:</strong>  int, normalization type</li>\n<li><strong>pos_constraint:</strong>  int, whether to enforce non-negative weights</li>\n<li><strong>num_inh:</strong>  int, number of inhibitory filters</li>\n<li><strong>bias:</strong>  bool, whether to include bias term</li>\n<li><strong>weights_initializer:</strong>  str, 'uniform', 'normal', 'xavier', 'zeros', or None</li>\n<li><strong>output_norm:</strong>  str, 'batch', 'batchX', or None</li>\n<li><strong>padding:</strong>  'same','valid', or 'circular' (default 'same')</li>\n<li><strong>initialize_center:</strong>  bool, whether to initialize the center</li>\n<li><strong>bias_initializer:</strong>  str, 'uniform', 'normal', 'xavier', 'zeros', or None</li>\n<li><strong>reg_vals:</strong>  dict, regularization values</li>\n<li><strong>**kwargs:</strong>  additional arguments to pass to NDNLayer</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">filter_width</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_fixed_filters</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">fixed_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">fixed_num_inh</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">num_inh</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">output_norm</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.partiallayers.ConvLayerPartial.num_inh", "modulename": "NDNT.modules.layers.partiallayers", "qualname": "ConvLayerPartial.num_inh", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.partiallayers.ConvLayerPartial.dims_mod", "modulename": "NDNT.modules.layers.partiallayers", "qualname": "ConvLayerPartial.dims_mod", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.partiallayers.ConvLayerPartial.num_fixed_filters", "modulename": "NDNT.modules.layers.partiallayers", "qualname": "ConvLayerPartial.num_fixed_filters", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.partiallayers.ConvLayerPartial.num_filters", "modulename": "NDNT.modules.layers.partiallayers", "qualname": "ConvLayerPartial.num_filters", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.partiallayers.ConvLayerPartial.num_outputs", "modulename": "NDNT.modules.layers.partiallayers", "qualname": "ConvLayerPartial.num_outputs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.partiallayers.ConvLayerPartial.preprocess_weights", "modulename": "NDNT.modules.layers.partiallayers", "qualname": "ConvLayerPartial.preprocess_weights", "kind": "function", "doc": "<p>Preprocess weights: assemble from real weights + fixed and then NDNLayer Process weights clipped in.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>w: torch.Tensor, preprocessed weights</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.partiallayers.ConvLayerPartial.layer_dict", "modulename": "NDNT.modules.layers.partiallayers", "qualname": "ConvLayerPartial.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>mask:</strong>  np.ndarray, mask to apply to the weights</li>\n<li><strong>**kwargs:</strong>  additional arguments to pass to NDNLayer</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Ldict: dict, dictionary of layer parameters</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">fixed_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_fixed_filters</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">fixed_num_inh</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">filter_width</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.partiallayers.OriConvLayerPartial", "modulename": "NDNT.modules.layers.partiallayers", "qualname": "OriConvLayerPartial", "kind": "class", "doc": "<p>ConvLayerPartial: NDNLayer with only some of the weights in the layer fit, determined by\n    fixed_dims OR num_fixed_filters</p>\n", "bases": "NDNT.modules.layers.orilayers.OriConvLayer"}, {"fullname": "NDNT.modules.layers.partiallayers.OriConvLayerPartial.__init__", "modulename": "NDNT.modules.layers.partiallayers", "qualname": "OriConvLayerPartial.__init__", "kind": "function", "doc": "<p>OriConvLayerPartial: OriConvLayer with only some of the weights in the layer fit, determined by\n    fixed_dims OR num_fixed_filters</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  tuple or list of ints, (num_channels, height, width, lags)</li>\n<li><strong>num_filters:</strong>  number of output filters</li>\n<li><strong>filter_dims:</strong>  width of convolutional kernel (int or list of ints)</li>\n<li><strong>num_fixed_filters:</strong>  number of filters to hold constant (default=0)</li>\n<li><strong>fixed_dims:</strong>  4-d size of fixed dims, use -1 for not-touching dims. Should have 3 -1s</li>\n<li><strong>NLtype:</strong>  str, 'lin', 'relu', 'tanh', 'sigmoid', 'elu', 'none'</li>\n<li><strong>norm_type:</strong>  int, normalization type</li>\n<li><strong>pos_constraint:</strong>  int, whether to enforce non-negative weights</li>\n<li><strong>num_inh:</strong>  int, number of inhibitory filters</li>\n<li><strong>bias:</strong>  bool, whether to include bias term</li>\n<li><strong>weights_initializer:</strong>  str, 'uniform', 'normal', 'xavier', 'zeros', or None</li>\n<li><strong>output_norm:</strong>  str, 'batch', 'batchX', or None</li>\n<li><strong>padding:</strong>  'same','valid', or 'circular' (default 'same')</li>\n<li><strong>initialize_center:</strong>  bool, whether to initialize the center</li>\n<li><strong>bias_initializer:</strong>  str, 'uniform', 'normal', 'xavier', 'zeros', or None</li>\n<li><strong>reg_vals:</strong>  dict, regularization values</li>\n<li><strong>**kwargs:</strong>  additional arguments to pass to NDNLayer</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">filter_width</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_fixed_filters</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">fixed_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">fixed_num_inh</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">num_inh</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">angles</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">output_norm</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.partiallayers.OriConvLayerPartial.dims_mod", "modulename": "NDNT.modules.layers.partiallayers", "qualname": "OriConvLayerPartial.dims_mod", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.partiallayers.OriConvLayerPartial.num_fixed_filters", "modulename": "NDNT.modules.layers.partiallayers", "qualname": "OriConvLayerPartial.num_fixed_filters", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.partiallayers.OriConvLayerPartial.num_filters", "modulename": "NDNT.modules.layers.partiallayers", "qualname": "OriConvLayerPartial.num_filters", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.partiallayers.OriConvLayerPartial.num_outputs", "modulename": "NDNT.modules.layers.partiallayers", "qualname": "OriConvLayerPartial.num_outputs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.partiallayers.OriConvLayerPartial.preprocess_weights", "modulename": "NDNT.modules.layers.partiallayers", "qualname": "OriConvLayerPartial.preprocess_weights", "kind": "function", "doc": "<p>Preprocess weights: assemble from real weights + fixed and then NDNLayer Process weights clipped in.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>w: torch.Tensor, preprocessed weights</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.partiallayers.OriConvLayerPartial.layer_dict", "modulename": "NDNT.modules.layers.partiallayers", "qualname": "OriConvLayerPartial.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li>FIX</li>\n<li><strong>**kwargs:</strong>  additional arguments to pass to NDNLayer</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Ldict: dict, dictionary of layer parameters</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">fixed_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">num_fixed_filters</span><span class=\"o\">=</span><span class=\"mi\">0</span>, </span><span class=\"param\"><span class=\"n\">fixed_num_inh</span><span class=\"o\">=</span><span class=\"mi\">0</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.pyrlayers", "modulename": "NDNT.modules.layers.pyrlayers", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.pyrlayers.MaskLayer", "modulename": "NDNT.modules.layers.pyrlayers", "qualname": "MaskLayer", "kind": "class", "doc": "<p>MaskLayer: Layer with a mask applied to the weights.</p>\n", "bases": "NDNT.modules.layers.ndnlayer.NDNLayer"}, {"fullname": "NDNT.modules.layers.pyrlayers.MaskLayer.__init__", "modulename": "NDNT.modules.layers.pyrlayers", "qualname": "MaskLayer.__init__", "kind": "function", "doc": "<p>MaskLayer: Layer with a mask applied to the weights.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  tuple or list of ints, (num_channels, height, width, lags)</li>\n<li><strong>num_filters:</strong>  number of output filters</li>\n<li><strong>## NOT CANT BE PART OF DICTIONARY mask:</strong>  np.ndarray, mask to apply to the weights</li>\n<li><strong>NLtype:</strong>  str, 'lin', 'relu', 'tanh', 'sigmoid', 'elu', 'none'</li>\n<li><strong>norm_type:</strong>  int, normalization type</li>\n<li><strong>pos_constraint:</strong>  int, whether to enforce non-negative weights</li>\n<li><strong>num_inh:</strong>  int, number of inhibitory filters</li>\n<li><strong>bias:</strong>  bool, whether to include bias term</li>\n<li><strong>weights_initializer:</strong>  str, 'uniform', 'normal', 'xavier', 'zeros', or None</li>\n<li><strong>output_norm:</strong>  str, 'batch', 'batchX', or None</li>\n<li><strong>initialize_center:</strong>  bool, whether to initialize the center</li>\n<li><strong>bias_initializer:</strong>  str, 'uniform', 'normal', 'xavier', 'zeros', or None</li>\n<li><strong>reg_vals:</strong>  dict, regularization values</li>\n<li><strong>**kwargs:</strong>  additional arguments to pass to NDNLayer</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">NLtype</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;lin&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">norm_type</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">pos_constraint</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">num_inh</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">weights_initializer</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;xavier_uniform&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">output_norm</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">initialize_center</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">bias_initializer</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;zeros&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">reg_vals</span><span class=\"p\">:</span> <span class=\"nb\">dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.pyrlayers.MaskLayer.set_mask", "modulename": "NDNT.modules.layers.pyrlayers", "qualname": "MaskLayer.set_mask", "kind": "function", "doc": "<p>Sets mask -- instead of plugging in by hand. Registers mask as being set, and checks dimensions.\nCan also use to rest to have trivial mask (all 1s)\nMask will be numpy, leave mask blank if want to set to default mask (all ones)</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>mask:</strong>  numpy array of size of filters (filter_dims x num_filters)</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.pyrlayers.MaskLayer.preprocess_weights", "modulename": "NDNT.modules.layers.pyrlayers", "qualname": "MaskLayer.preprocess_weights", "kind": "function", "doc": "<p>Preprocess weights for MaskLayer.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>w: torch.Tensor, preprocessed weights</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.pyrlayers.MaskLayer.layer_dict", "modulename": "NDNT.modules.layers.pyrlayers", "qualname": "MaskLayer.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>mask:</strong>  np.ndarray, mask to apply to the weights</li>\n<li><strong>**kwargs:</strong>  additional arguments to pass to NDNLayer</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Ldict: dict, dictionary of layer parameters</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.pyrlayers.MaskSTconvLayer", "modulename": "NDNT.modules.layers.pyrlayers", "qualname": "MaskSTconvLayer", "kind": "class", "doc": "<p>MaskSTConvLayer: STconvLayer with a mask applied to the weights.</p>\n", "bases": "NDNT.modules.layers.convlayers.STconvLayer"}, {"fullname": "NDNT.modules.layers.pyrlayers.MaskSTconvLayer.__init__", "modulename": "NDNT.modules.layers.pyrlayers", "qualname": "MaskSTconvLayer.__init__", "kind": "function", "doc": "<p>MaskSTconvLayer: STconvLayer with a mask applied to the weights.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  tuple or list of ints, (num_channels, height, width, lags)</li>\n<li><strong>num_filters:</strong>  number of output filters</li>\n<li><strong>mask:</strong>  np.ndarray, mask to apply to the weights</li>\n<li><strong>output_norm:</strong>  str, 'batch', 'batchX', or None</li>\n<li><strong>**kwargs:</strong>  additional arguments to pass to STConvLayer</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">initialize_center</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">output_norm</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.pyrlayers.MaskSTconvLayer.set_mask", "modulename": "NDNT.modules.layers.pyrlayers", "qualname": "MaskSTconvLayer.set_mask", "kind": "function", "doc": "<p>Sets mask -- instead of plugging in by hand. Registers mask as being set, and checks dimensions.\nCan also use to rest to have trivial mask (all 1s)\nMask will be numpy, leave mask blank if want to set to default mask (all ones)</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>mask:</strong>  numpy array of size of filters (filter_dims x num_filters)</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.pyrlayers.MaskSTconvLayer.preprocess_weights", "modulename": "NDNT.modules.layers.pyrlayers", "qualname": "MaskSTconvLayer.preprocess_weights", "kind": "function", "doc": "<p>Preprocess weights for MaskLayer.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>w: torch.Tensor, preprocessed weights</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.pyrlayers.MaskSTconvLayer.layer_dict", "modulename": "NDNT.modules.layers.pyrlayers", "qualname": "MaskSTconvLayer.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>mask:</strong>  np.ndarray, mask to apply to the weights</li>\n<li><strong>**kwargs:</strong>  additional arguments to pass to NDNLayer</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Ldict: dict, dictionary of layer parameters</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.readouts", "modulename": "NDNT.modules.layers.readouts", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayer", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayer", "kind": "class", "doc": "<p>ReadoutLayer for spatial readout.</p>\n", "bases": "NDNT.modules.layers.ndnlayer.NDNLayer"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayer.__init__", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayer.__init__", "kind": "function", "doc": "<p>ReadoutLayer: Spatial readout layer for NDNs.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  tuple or list of ints, (num_channels, height, width, lags)</li>\n<li><strong>num_filters:</strong>  number of output filters</li>\n<li><strong>filter_dims:</strong>  tuple or list of ints, (num_channels, height, width, lags)</li>\n<li><strong>batch_sample:</strong>  bool, whether to sample grid locations separately per sample per batch</li>\n<li><strong>init_mu_range:</strong>  float, range for uniform initialization of means</li>\n<li><strong>init_sigma:</strong>  float, standard deviation for uniform initialization of sigmas</li>\n<li><strong>gauss_type:</strong>  str, 'isotropic', 'uncorrelated', or 'full'</li>\n<li><strong>align_corners:</strong>  bool, whether to align corners in grid_sample</li>\n<li><strong>mode:</strong>  str, 'bilinear' or 'nearest'</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">filter_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_sample</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">init_mu_range</span><span class=\"o\">=</span><span class=\"mf\">0.1</span>,</span><span class=\"param\">\t<span class=\"n\">init_sigma</span><span class=\"o\">=</span><span class=\"mf\">0.2</span>,</span><span class=\"param\">\t<span class=\"n\">gauss_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;isotropic&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">align_corners</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">mode</span><span class=\"o\">=</span><span class=\"s1\">&#39;nearest&#39;</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayer.batch_sample", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayer.batch_sample", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayer.sample_mode", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayer.sample_mode", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayer.init_mu_range", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayer.init_mu_range", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayer.init_sigma", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayer.init_sigma", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayer.align_corners", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayer.align_corners", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayer.grid_shape", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayer.grid_shape", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayer.mu", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayer.mu", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayer.sigma", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayer.sigma", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayer.level_reg", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayer.level_reg", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayer.sample", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayer.sample", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayer.features", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayer.features", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayer.grid", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayer.grid", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayer.initialize_spatial_mapping", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayer.initialize_spatial_mapping", "kind": "function", "doc": "<p>Initializes the mean, and sigma of the Gaussian readout.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li>None</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayer.sample_grid", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayer.sample_grid", "kind": "function", "doc": "<p>Returns the grid locations from the core by sampling from a Gaussian distribution\nMore specifically, it returns sampled positions for each batch over all elements, given mus and sigmas\nIf 'sample' is false, it just gives mu back (so testing has no randomness)\nThis code is inherited and then modified, so different style than most other</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>batch_size (int):</strong>  size of the batch</li>\n<li><strong>sample (bool/None):</strong>  sample determines whether we draw a sample from Gaussian distribution, N(mu,sigma), defined per neuron\n or use the mean, mu, of the Gaussian distribution without sampling.\nif sample is None (default), samples from the N(mu,sigma) during training phase and\n  fixes to the mean, mu, during evaluation phase.\nif sample is True/False, overrides the model_state (i.e training or eval) and does as instructed</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch_size</span>, </span><span class=\"param\"><span class=\"n\">sample</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayer.get_weights", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayer.get_weights", "kind": "function", "doc": "<p>Overloaded to read not use preprocess weights but instead use layer property features.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>to_reshape (bool):</strong>  whether to reshape the weights to the filter_dims</li>\n<li><strong>time_reverse (bool):</strong>  whether to reverse the time dimension</li>\n<li><strong>num_inh (int):</strong>  number of inhibitory units</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>ws: weights of the readout layer</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">to_reshape</span><span class=\"o\">=</span><span class=\"kc\">True</span>, </span><span class=\"param\"><span class=\"n\">time_reverse</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">num_inh</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayer.compute_reg_loss", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayer.compute_reg_loss", "kind": "function", "doc": "<p>Compute the regularization loss for the layer: superceding super, by calling reg_module to do\nthis and then adding scaffold weight regularization if needed.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li>None</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>reg_loss: torch.Tensor, regularization loss</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayer.forward", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayer.forward", "kind": "function", "doc": "<p>Propagates the input forwards through the readout.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  input data</li>\n<li><strong>shift (bool):</strong>  shifts the location of the grid (from eye-tracking data)</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>y: neuronal activity</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">shift</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayer.set_readout_locations", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayer.set_readout_locations", "kind": "function", "doc": "<p>This hasn't been tested yet, but should be self-explanatory.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>locs:</strong>  locations of the readout units</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">locs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayer.get_readout_locations", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayer.get_readout_locations", "kind": "function", "doc": "<p>Currently returns center location and sigmas, as list.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>mu: center locations of the readout units\n  sigma: sigmas of the readout units</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayer.passive_readout", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayer.passive_readout", "kind": "function", "doc": "<p>This will not fit mu and std, but set them to zero. It will pass identities in,\nso number of input filters must equal number of readout units.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li>None</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayer.fit_mus", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayer.fit_mus", "kind": "function", "doc": "<p>Quick function that turns on or off fitting mus/sigmas and toggles sample</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>val (bool):</strong>  True or False -- must specify</li>\n<li><strong>sigma (float):</strong>  choose starting sigma value (default no choice)</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">val</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">sigma</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">sample_mode</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayer.enforce_grid", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayer.enforce_grid", "kind": "function", "doc": "<p>Function that adjusts mus to correspond to precise points on pixel grid</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li>None</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayer.layer_dict", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayer.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>NLtype:</strong>  str, type of nonlinearity</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Ldict: dictionary of layer parameters</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">NLtype</span><span class=\"o\">=</span><span class=\"s1\">&#39;softplus&#39;</span>, </span><span class=\"param\"><span class=\"n\">mode</span><span class=\"o\">=</span><span class=\"s1\">&#39;nearest&#39;</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayer3d", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayer3d", "kind": "class", "doc": "<p>ReadoutLayer3d for 3d readout.\nThis is a subclass of ReadoutLayer, but with the added dimension of time.</p>\n", "bases": "ReadoutLayer"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayer3d.__init__", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayer3d.__init__", "kind": "function", "doc": "<p>ReadoutLayer3d: 3d readout layer for NDNs.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  tuple or list of ints, (num_channels, height, width, depth, lags)</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayer3d.input_dims", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayer3d.input_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayer3d.filter_dims", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayer3d.filter_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayer3d.reg", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayer3d.reg", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayer3d.set_mask", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayer3d.set_mask", "kind": "function", "doc": "<p>Sets mask -- instead of plugging in by hand. Registers mask as being set, and checks dimensions.\nCan also use to rest to have trivial mask (all 1s)\nMask will be numpy, leave mask blank if want to set to default mask (all ones)</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>mask:</strong>  numpy array of size of filters (filter_dims x num_filters)</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayer3d.forward", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayer3d.forward", "kind": "function", "doc": "<p>Propagates the input forwards through the readout</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  input data</li>\n<li><strong>shift (bool):</strong>  shifts the location of the grid (from eye-tracking data)</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>y: neuronal activity</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">shift</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayer3d.passive_readout", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayer3d.passive_readout", "kind": "function", "doc": "<p>This might have to be redone for readout3d to take into account extra filter dim.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayer3d.layer_dict", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayer3d.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li>None</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Ldict: dictionary of layer parameters</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.readouts.FixationLayer", "modulename": "NDNT.modules.layers.readouts", "qualname": "FixationLayer", "kind": "class", "doc": "<p>FixationLayer to work in tandem with readout-layer to shift the stimulus (well, the\nposition of the readout) for each fixation. These shifts are in units of mus, with the\nwhole field of view corresponding to -1 to +1. They have a 'sigma' (width) that is either \nthe same across all fixations or one for each fixation determined by 'single_sigma' arg.</p>\n", "bases": "NDNT.modules.layers.ndnlayer.NDNLayer"}, {"fullname": "NDNT.modules.layers.readouts.FixationLayer.__init__", "modulename": "NDNT.modules.layers.readouts", "qualname": "FixationLayer.__init__", "kind": "function", "doc": "<p>Layer weights become the shift for each fixation, sigma is constant over each dimension.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>num_fixations:</strong>  int, number of fixations</li>\n<li><strong>num_spatial_dims:</strong>  int, number of spatial dimensions</li>\n<li><strong>batch_sample:</strong>  bool, whether to sample grid locations separately per sample per batch</li>\n<li><strong>fix_n_index:</strong>  bool, whether to index fixations starting at 1</li>\n<li><strong>init_sigma:</strong>  float, standard deviation for uniform initialization of sigmas</li>\n<li><strong>single_sigma:</strong>  bool, whether to use a single sigma for all fixations</li>\n<li><strong>bias:</strong>  bool, whether to include bias term</li>\n<li><strong>NLtype:</strong>  str, type of nonlinearity</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">num_fixations</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_spatial_dims</span><span class=\"o\">=</span><span class=\"mi\">2</span>,</span><span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_sample</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">fix_n_index</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">init_sigma</span><span class=\"o\">=</span><span class=\"mf\">0.2</span>,</span><span class=\"param\">\t<span class=\"n\">single_sigma</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.readouts.FixationLayer.num_spatial_dims", "modulename": "NDNT.modules.layers.readouts", "qualname": "FixationLayer.num_spatial_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.readouts.FixationLayer.num_fixations", "modulename": "NDNT.modules.layers.readouts", "qualname": "FixationLayer.num_fixations", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.readouts.FixationLayer.batch_sample", "modulename": "NDNT.modules.layers.readouts", "qualname": "FixationLayer.batch_sample", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.readouts.FixationLayer.init_sigma", "modulename": "NDNT.modules.layers.readouts", "qualname": "FixationLayer.init_sigma", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.readouts.FixationLayer.single_sigma", "modulename": "NDNT.modules.layers.readouts", "qualname": "FixationLayer.single_sigma", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.readouts.FixationLayer.fix_n_index", "modulename": "NDNT.modules.layers.readouts", "qualname": "FixationLayer.fix_n_index", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.readouts.FixationLayer.shifts", "modulename": "NDNT.modules.layers.readouts", "qualname": "FixationLayer.shifts", "kind": "function", "doc": "<p>Converts weights into shifts in units of pixels, knowing the stimulus size in pixels (L).\nNote it uses mu2pixel function, but modifies so L/2 is zero-shift.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>L (int):</strong>  number of pixels in the stimulus, assuming square stim (Default: 60)</li>\n<li><strong>force_int (bool):</strong>  whether to return in integer (default) or fractional values</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>shifts: predicted shifts in units of pixels (num_fixations x num_dims)</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">L</span><span class=\"o\">=</span><span class=\"mi\">60</span>, </span><span class=\"param\"><span class=\"n\">force_int</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.readouts.FixationLayer.forward", "modulename": "NDNT.modules.layers.readouts", "qualname": "FixationLayer.forward", "kind": "function", "doc": "<p>The input is the fixation number across all relevant time points. This will return\nthe corresponding weight, processed by a tanh (so max value is +/- 1)</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  input data</li>\n<li><strong>shift (bool):</strong>  shifts the location of the grid (from eye-tracking data)</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>y: neuronal activity</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">shift</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.readouts.FixationLayer.layer_dict", "modulename": "NDNT.modules.layers.readouts", "qualname": "FixationLayer.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>num_fixations:</strong>  int, number of fixations</li>\n<li><strong>num_spatial_dims:</strong>  int, number of spatial dimensions</li>\n<li><strong>init_sigma:</strong>  float, standard deviation for uniform initialization of sigmas</li>\n<li><strong>input_dims:</strong>  tuple or list of ints, (num_channels, height, width, lags)</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Ldict: dictionary of layer parameters</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">num_fixations</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_spatial_dims</span><span class=\"o\">=</span><span class=\"mi\">2</span>,</span><span class=\"param\">\t<span class=\"n\">init_sigma</span><span class=\"o\">=</span><span class=\"mf\">0.2</span>,</span><span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayerQsample", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayerQsample", "kind": "class", "doc": "<p>ReadoutLayerQsample for 3d readout with sampling over angle dimension Q.\nThis is a subclass of ReadoutLayer3d.</p>\n", "bases": "ReadoutLayer3d"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayerQsample.__init__", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayerQsample.__init__", "kind": "function", "doc": "<p>ReadoutLayerQsample: 3d readout layer for NDNs.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  tuple or list of ints, (num_channels, height, width, lags)</li>\n<li><strong>filter_dims:</strong>  tuple or list of ints, (num_channels, height, width, depth, lags)</li>\n<li><strong>batch_Qample:</strong>  bool, whether to sample Qgrid locations separately per sample per batch</li>\n<li><strong>init_sigma:</strong>  float, standard deviation for uniform initialization of sigmas</li>\n<li><strong>Qsample_mode:</strong>  str, 'bilinear' or 'nearest'</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">filter_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_Qsample</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">init_Qsigma</span><span class=\"o\">=</span><span class=\"mf\">0.5</span>,</span><span class=\"param\">\t<span class=\"n\">Qsample_mode</span><span class=\"o\">=</span><span class=\"s1\">&#39;nearest&#39;</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayerQsample.batch_Qsample", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayerQsample.batch_Qsample", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayerQsample.Qsample_mode", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayerQsample.Qsample_mode", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayerQsample.init_Qsigma", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayerQsample.init_Qsigma", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayerQsample.Qgrid_shape", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayerQsample.Qgrid_shape", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayerQsample.Qmu", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayerQsample.Qmu", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayerQsample.Qsigma", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayerQsample.Qsigma", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayerQsample.Qsample", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayerQsample.Qsample", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayerQsample.sample_Qgrid", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayerQsample.sample_Qgrid", "kind": "function", "doc": "<p>Returns the chosen angle (Q) from the core by sampling from a Gaussian distribution\nMore specifically, it returns sampled angle for each batch over all elements, given Qmus and Qsigmas\nAlso implements wrap around for Qmus so edge mus get mapped back to first angle\nIf 'Qsample' is false, it just gives mu back (so testing has no randomness)\nThis code is inherited and then modified, so different style than most other</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>batch_size (int):</strong>  size of the batch</li>\n<li><strong>Qsample (bool/None):</strong>  sample determines whether we draw a sample from Gaussian distribution, N(Qmu,Qsigma), defined per neuron\n or use the mean, Qmu, of the Gaussian distribution without sampling.\nif Qsample is None (default), samples from the N(Qmu,Qsigma) during training phase and\n  fixes to the mean, Qmu, during evaluation phase.\nif Qsample is True/False, overrides the model_state (i.e training or eval) and does as instructed</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch_size</span>, </span><span class=\"param\"><span class=\"n\">Qsample</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayerQsample.fit_Qmus", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayerQsample.fit_Qmus", "kind": "function", "doc": "<p>Quick function that turns on or off fitting Qmus/Qsigmas and toggles sample</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>val (bool):</strong>  True or False -- must specify</li>\n<li><strong>Qsigma (float):</strong>  choose starting Qsigma value (default no choice)</li>\n<li><strong>sample_mode (str):</strong>  'bilinear' or 'nearest', default is None, which work with existing sample_mode</li>\n<li><strong>verbose (bool):</strong>  whether to print out information about the fitting process</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">val</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">Qsigma</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">sample_mode</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayerQsample.forward", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayerQsample.forward", "kind": "function", "doc": "<p>Propagates the input forwards through the readout</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  input data</li>\n<li><strong>shift (bool):</strong>  shifts the location of the grid (from eye-tracking data)</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>z: neuronal activity</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">shift</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayerQsample.degrees2mu", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayerQsample.degrees2mu", "kind": "function", "doc": "<p>Converts degrees into mu-values. If to_output=True, outputs to an array, and otherwise\nstores in the Qmu variable. It detects whether half-circle of full circle using stored angle values</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>theta_deg (np array):</strong>  array of angles in degrees into mu values, based on 180 or 360 deg wrap-around</li>\n<li><strong>to_output (Boolean):</strong>  whether to output to variable or store internally as Qmu (default: False, is the latter)</li>\n<li><strong>continuous (Boolean):</strong>  whether to convert to continuous angle or closest \"integer\" mu value (def True, continuous)</li>\n<li><strong>max_angle:</strong>  maximum angle represented in OriConv layers (default 180, but could be 360)</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Qmus: as numpy-array, if to_output is set to True, otherwise, nothing</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">theta_deg</span>, </span><span class=\"param\"><span class=\"n\">to_output</span><span class=\"o\">=</span><span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"n\">continuous</span><span class=\"o\">=</span><span class=\"kc\">True</span>, </span><span class=\"param\"><span class=\"n\">max_angle</span><span class=\"o\">=</span><span class=\"mi\">180</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayerQsample.mu2degrees", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayerQsample.mu2degrees", "kind": "function", "doc": "<p>Converts Qmu-values into degrees. Automatically reads from Qmu variable, and outputs a numpy array\nIt detects whether half-circle of full circle using stored angle values</p>\n\n<p>Args:<br />\n    Qmus: if dont want to read from layer, pass in values directly (default None, using self.Qmu)<br />\n    max_angle: maximum angle represented in OriConv layers (default 180, but could be 360)</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>thetas: angles in degrees (between 0 to max_angle)</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">mu_in</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">max_angle</span><span class=\"o\">=</span><span class=\"mi\">180</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.readouts.ReadoutLayerQsample.layer_dict", "modulename": "NDNT.modules.layers.readouts", "qualname": "ReadoutLayerQsample.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li>None</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Ldict: dictionary of layer parameters</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">Qsigma</span><span class=\"o\">=</span><span class=\"mi\">1</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.readouts.GridSampleLayer", "modulename": "NDNT.modules.layers.readouts", "qualname": "GridSampleLayer", "kind": "class", "doc": "<p>Takes convolutional input and passes out layer with one or more dimensions collapsed to the\nsample-values passed in for that dimension. For now, this is made to sample the third conv\ndimension, based on selected values</p>\n", "bases": "NDNT.modules.layers.ndnlayer.NDNLayer"}, {"fullname": "NDNT.modules.layers.readouts.GridSampleLayer.__init__", "modulename": "NDNT.modules.layers.readouts", "qualname": "GridSampleLayer.__init__", "kind": "function", "doc": "<p>TimeLayer: Layer to track experiment time and a weighted output.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  tuple or list of ints, (num_channels, height, width, lags/angles)</li>\n<li><strong>num_lags:</strong>  number of lags to shift back by</li>\n<li><strong>**kwargs:</strong>  additional arguments to pass to NDNLayer</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">num_lags</span><span class=\"o\">=</span><span class=\"mi\">1</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.readouts.GridSampleLayer.output_dims", "modulename": "NDNT.modules.layers.readouts", "qualname": "GridSampleLayer.output_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.readouts.GridSampleLayer.num_outputs", "modulename": "NDNT.modules.layers.readouts", "qualname": "GridSampleLayer.num_outputs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.readouts.GridSampleLayer.num_lags", "modulename": "NDNT.modules.layers.readouts", "qualname": "GridSampleLayer.num_lags", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.readouts.GridSampleLayer.forward", "modulename": "NDNT.modules.layers.readouts", "qualname": "GridSampleLayer.forward", "kind": "function", "doc": "<p>Shift in batch dimesnion by num_lags and pad by 0</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  torch.Tensor, input tensor</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>y: torch.Tensor, output tensor</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.readouts.GridSampleLayer.layer_dict", "modulename": "NDNT.modules.layers.readouts", "qualname": "GridSampleLayer.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">num_lags</span><span class=\"o\">=</span><span class=\"mi\">1</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.reslayers", "modulename": "NDNT.modules.layers.reslayers", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.reslayers.IterLayer", "modulename": "NDNT.modules.layers.reslayers", "qualname": "IterLayer", "kind": "class", "doc": "<p>Residual network composed of many layers (num_iter) with weight-sharing across layers.\nIt is based on conv-net setup, and can output from all layers or just last (output_config)\nAlso, num_inh is setup for only output layers by default</p>\n\n<p>Args (required):\n    input_dims: tuple or list of ints, (num_channels, height, width, lags)\n    num_filters: number of output filters\n    filter_width: width of convolutional kernel (int or list of ints)</p>\n\n<p>Args (optional):\n    padding: 'same' or 'valid' (default 'same')\n    weight_init: str, 'uniform', 'normal', 'xavier', 'zeros', or None\n    bias_init: str, 'uniform', 'normal', 'xavier', 'zeros', or None\n    bias: bool, whether to include bias term\n    NLtype: str, 'lin', 'relu', 'tanh', 'sigmoid', 'elu', 'none'</p>\n", "bases": "NDNT.modules.layers.convlayers.ConvLayer"}, {"fullname": "NDNT.modules.layers.reslayers.IterLayer.__init__", "modulename": "NDNT.modules.layers.reslayers", "qualname": "IterLayer.__init__", "kind": "function", "doc": "<p>Initialize IterLayer with specified parameters.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  tuple or list of ints, (num_channels, height, width, lags)</li>\n<li><strong>num_filters:</strong>  number of output filters</li>\n<li><strong>filter_width:</strong>  width of convolutional kernel (int or list of ints)</li>\n<li><strong>num_iter:</strong>  number of iterations to apply the layer</li>\n<li><strong>output_config:</strong>  'last' or 'full' (default 'last')</li>\n<li><strong>temporal_tent_spacing:</strong>  spacing of temporal tent-basis functions (default None)</li>\n<li><strong>output_norm:</strong>  'batch', 'batchX', or None (default None)</li>\n<li><strong>window:</strong>  'hamming' or None (default None)</li>\n<li><strong>res_layer:</strong>  bool, whether to include a residual connection (default True)</li>\n<li><strong>LN_reverse:</strong>  bool, whether to apply layer normalization after nonlinearity (default False)</li>\n<li><strong>**kwargs:</strong>  additional arguments to pass to ConvLayer</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">filter_width</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_iter</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">output_config</span><span class=\"o\">=</span><span class=\"s1\">&#39;last&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">temporal_tent_spacing</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">output_norm</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">window</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">res_layer</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">LN_reverse</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.reslayers.IterLayer.num_iter", "modulename": "NDNT.modules.layers.reslayers", "qualname": "IterLayer.num_iter", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.reslayers.IterLayer.LN_reverse", "modulename": "NDNT.modules.layers.reslayers", "qualname": "IterLayer.LN_reverse", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.reslayers.IterLayer.output_config", "modulename": "NDNT.modules.layers.reslayers", "qualname": "IterLayer.output_config", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.reslayers.IterLayer.res_layer", "modulename": "NDNT.modules.layers.reslayers", "qualname": "IterLayer.res_layer", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.reslayers.IterLayer.layer_dict", "modulename": "NDNT.modules.layers.reslayers", "qualname": "IterLayer.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>filter_width:</strong>  width of convolutional kernel (int or list of ints)</li>\n<li><strong>num_iter:</strong>  number of iterations to apply the layer</li>\n<li><strong>output_config:</strong>  'last' or 'full' (default 'last')</li>\n<li><strong>res_layer:</strong>  bool, whether to include a residual connection (default True)</li>\n<li><strong>LN_reverse:</strong>  bool, whether to apply layer normalization after nonlinearity (default False)</li>\n<li><strong>**kwargs:</strong>  additional arguments to pass to ConvLayer</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Ldict: dictionary of layer parameters</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">filter_width</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_iter</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">output_config</span><span class=\"o\">=</span><span class=\"s1\">&#39;last&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">res_layer</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">LN_reverse</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.reslayers.IterLayer.forward", "modulename": "NDNT.modules.layers.reslayers", "qualname": "IterLayer.forward", "kind": "function", "doc": "<p>Forward pass through the IterLayer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  torch.Tensor, input tensor</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>y: torch.Tensor, output tensor</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.reslayers.IterTlayer", "modulename": "NDNT.modules.layers.reslayers", "qualname": "IterTlayer", "kind": "class", "doc": "<p>Residual network layer based on conv-net setup but with different forward.\nNamely, the forward includes a skip-connection, and has to be 'same'</p>\n\n<p>Args (required):\n    input_dims: tuple or list of ints, (num_channels, height, width, lags)\n    num_filters: number of output filters\n    filter_dims: width of convolutional kernel (int or list of ints)</p>\n\n<p>Args (optional):\n    padding: 'same' or 'valid' (default 'same')\n    weight_init: str, 'uniform', 'normal', 'xavier', 'zeros', or None\n    bias_init: str, 'uniform', 'normal', 'xavier', 'zeros', or None\n    bias: bool, whether to include bias term\n    NLtype: str, 'lin', 'relu', 'tanh', 'sigmoid', 'elu', 'none'</p>\n", "bases": "NDNT.modules.layers.convlayers.TconvLayer"}, {"fullname": "NDNT.modules.layers.reslayers.IterTlayer.__init__", "modulename": "NDNT.modules.layers.reslayers", "qualname": "IterTlayer.__init__", "kind": "function", "doc": "<p>Initialize IterTLayer with specified parameters.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  tuple or list of ints, (num_channels, height, width, lags)</li>\n<li><strong>num_filters:</strong>  number of output filters</li>\n<li><strong>filter_width:</strong>  width of convolutional kernel (int or list of ints)</li>\n<li><strong>num_iter:</strong>  number of iterations to apply the layer</li>\n<li><strong>num_lags:</strong>  number of lags in spatiotemporal filter</li>\n<li><strong>res_layer:</strong>  bool, whether to include a residual connection (default True)</li>\n<li><strong>output_config:</strong>  'last' or 'full' (default 'last')</li>\n<li><strong>output_norm:</strong>  'batch', 'batchX', or None (default None)</li>\n<li><strong>**kwargs:</strong>  additional arguments to pass to ConvLayer</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">filter_width</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_iter</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_lags</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">res_layer</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">output_config</span><span class=\"o\">=</span><span class=\"s1\">&#39;last&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">output_norm</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.reslayers.IterTlayer.num_iter", "modulename": "NDNT.modules.layers.reslayers", "qualname": "IterTlayer.num_iter", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.reslayers.IterTlayer.output_config", "modulename": "NDNT.modules.layers.reslayers", "qualname": "IterTlayer.output_config", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.reslayers.IterTlayer.res_layer", "modulename": "NDNT.modules.layers.reslayers", "qualname": "IterTlayer.res_layer", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.reslayers.IterTlayer.num_outputs", "modulename": "NDNT.modules.layers.reslayers", "qualname": "IterTlayer.num_outputs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.reslayers.IterTlayer.layer_dict", "modulename": "NDNT.modules.layers.reslayers", "qualname": "IterTlayer.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>filter_width:</strong>  width of convolutional kernel (int or list of ints)</li>\n<li><strong>num_iter:</strong>  number of iterations to apply the layer</li>\n<li><strong>num_lags:</strong>  number of lags in spatiotemporal filter</li>\n<li><strong>res_layer:</strong>  bool, whether to include a residual connection (default True)</li>\n<li><strong>output_config:</strong>  'last' or 'full' (default 'last')</li>\n<li><strong>**kwargs:</strong>  additional arguments to pass to ConvLayer</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Ldict: dictionary of layer parameters</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">filter_width</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_iter</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_lags</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">res_layer</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">output_config</span><span class=\"o\">=</span><span class=\"s1\">&#39;last&#39;</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.reslayers.IterTlayer.forward", "modulename": "NDNT.modules.layers.reslayers", "qualname": "IterTlayer.forward", "kind": "function", "doc": "<p>Forward pass through the IterLayer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  torch.Tensor, input tensor</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>y: torch.Tensor, output tensor</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.reslayers.IterSTlayer", "modulename": "NDNT.modules.layers.reslayers", "qualname": "IterSTlayer", "kind": "class", "doc": "<p>Residual network layer based on conv-net setup but with different forward.\nNamely, the forward includes a skip-connection, and has to be 'same'</p>\n\n<p>Args (required):\n    input_dims: tuple or list of ints, (num_channels, height, width, lags)\n    num_filters: number of output filters\n    filter_dims: width of convolutional kernel (int or list of ints)</p>\n\n<p>Args (optional):\n    padding: 'same' or 'valid' (default 'same')\n    weight_init: str, 'uniform', 'normal', 'xavier', 'zeros', or None\n    bias_init: str, 'uniform', 'normal', 'xavier', 'zeros', or None\n    bias: bool, whether to include bias term\n    NLtype: str, 'lin', 'relu', 'tanh', 'sigmoid', 'elu', 'none'</p>\n", "bases": "NDNT.modules.layers.convlayers.STconvLayer"}, {"fullname": "NDNT.modules.layers.reslayers.IterSTlayer.__init__", "modulename": "NDNT.modules.layers.reslayers", "qualname": "IterSTlayer.__init__", "kind": "function", "doc": "<p>Initialize IterLayer with specified parameters.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  tuple or list of ints, (num_channels, height, width, lags)</li>\n<li><strong>num_filters:</strong>  number of output filters</li>\n<li><strong>filter_width:</strong>  width of convolutional kernel (int or list of ints)</li>\n<li><strong>num_iter:</strong>  number of iterations to apply the layer</li>\n<li><strong>num_lags:</strong>  number of lags in spatiotemporal filter</li>\n<li><strong>res_layer:</strong>  bool, whether to include a residual connection (default True)</li>\n<li><strong>output_config:</strong>  'last' or 'full' (default 'last')</li>\n<li><strong>output_norm:</strong>  'batch', 'batchX', or None (default None)</li>\n<li><strong>**kwargs:</strong>  additional arguments to pass to ConvLayer</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">filter_width</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_iter</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_lags</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">res_layer</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">output_config</span><span class=\"o\">=</span><span class=\"s1\">&#39;last&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">output_norm</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.reslayers.IterSTlayer.num_iter", "modulename": "NDNT.modules.layers.reslayers", "qualname": "IterSTlayer.num_iter", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.reslayers.IterSTlayer.output_config", "modulename": "NDNT.modules.layers.reslayers", "qualname": "IterSTlayer.output_config", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.reslayers.IterSTlayer.res_layer", "modulename": "NDNT.modules.layers.reslayers", "qualname": "IterSTlayer.res_layer", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.reslayers.IterSTlayer.num_outputs", "modulename": "NDNT.modules.layers.reslayers", "qualname": "IterSTlayer.num_outputs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.reslayers.IterSTlayer.layer_dict", "modulename": "NDNT.modules.layers.reslayers", "qualname": "IterSTlayer.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>filter_width:</strong>  width of convolutional kernel (int or list of ints)</li>\n<li><strong>num_iter:</strong>  number of iterations to apply the layer</li>\n<li><strong>num_lags:</strong>  number of lags in spatiotemporal filter</li>\n<li><strong>res_layer:</strong>  bool, whether to include a residual connection (default True)</li>\n<li><strong>output_config:</strong>  'last' or 'full' (default 'last')</li>\n<li><strong>**kwargs:</strong>  additional arguments to pass to ConvLayer</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Ldict: dictionary of layer parameters</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">filter_width</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_iter</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_lags</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">res_layer</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">output_config</span><span class=\"o\">=</span><span class=\"s1\">&#39;last&#39;</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.reslayers.IterSTlayer.forward", "modulename": "NDNT.modules.layers.reslayers", "qualname": "IterSTlayer.forward", "kind": "function", "doc": "<p>Forward pass through the IterLayer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  torch.Tensor, input tensor</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>y: torch.Tensor, output tensor</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.reslayers.ResnetBlock", "modulename": "NDNT.modules.layers.reslayers", "qualname": "ResnetBlock", "kind": "class", "doc": "<p>Residual network layer based on conv-net setup but with different forward.\nNamely, the forward includes a skip-connection, and has to be 'same'</p>\n\n<p>Args (required):\n    input_dims: tuple or list of ints, (num_channels, height, width, lags)\n    num_filters: number of output filters\n    filter_width: width of convolutional kernel (int or list of ints)</p>\n\n<p>Args (optional):\n    padding: 'same' or 'valid' (default 'same')\n    weight_init: str, 'uniform', 'normal', 'xavier', 'zeros', or None\n    bias_init: str, 'uniform', 'normal', 'xavier', 'zeros', or None\n    bias: bool, whether to include bias term\n    NLtype: str, 'lin', 'relu', 'tanh', 'sigmoid', 'elu', 'none'</p>\n", "bases": "IterLayer"}, {"fullname": "NDNT.modules.layers.reslayers.ResnetBlock.__init__", "modulename": "NDNT.modules.layers.reslayers", "qualname": "ResnetBlock.__init__", "kind": "function", "doc": "<p>Initialize IterLayer with specified parameters.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  tuple or list of ints, (num_channels, height, width, lags)</li>\n<li><strong>num_filters:</strong>  number of output filters</li>\n<li><strong>filter_width:</strong>  width of convolutional kernel (int or list of ints)</li>\n<li><strong>num_iter:</strong>  number of iterations to apply the layer</li>\n<li><strong>output_config:</strong>  'last' or 'full' (default 'last')</li>\n<li><strong>temporal_tent_spacing:</strong>  spacing of temporal tent-basis functions (default None)</li>\n<li><strong>output_norm:</strong>  'batch', 'batchX', or None (default None)</li>\n<li><strong>window:</strong>  'hamming' or None (default None)</li>\n<li><strong>res_layer:</strong>  bool, whether to include a residual connection (default True)</li>\n<li><strong>LN_reverse:</strong>  bool, whether to apply layer normalization after nonlinearity (default False)</li>\n<li><strong>**kwargs:</strong>  additional arguments to pass to ConvLayer</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">filter_width</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_iter</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">res_layer</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.reslayers.ResnetBlock.layer_dict", "modulename": "NDNT.modules.layers.reslayers", "qualname": "ResnetBlock.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>filter_width:</strong>  width of convolutional kernel (int or list of ints)</li>\n<li><strong>num_iter:</strong>  number of iterations to apply the layer</li>\n<li><strong>output_config:</strong>  'last' or 'full' (default 'last')</li>\n<li><strong>res_layer:</strong>  bool, whether to include a residual connection (default True)</li>\n<li><strong>LN_reverse:</strong>  bool, whether to apply layer normalization after nonlinearity (default False)</li>\n<li><strong>**kwargs:</strong>  additional arguments to pass to ConvLayer</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Ldict: dictionary of layer parameters</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">filter_width</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_iter</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">res_layer</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">output_norm</span><span class=\"o\">=</span><span class=\"s1\">&#39;batch&#39;</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.reslayers.ResnetBlock.forward", "modulename": "NDNT.modules.layers.reslayers", "qualname": "ResnetBlock.forward", "kind": "function", "doc": "<p>Forward pass through the ResnetBlock: input filtering (one layer wit filter width) followed\nby some number of filter_width-1 conv layers</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  torch.Tensor, input tensor</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>y: torch.Tensor, output tensor</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.specialtylayers", "modulename": "NDNT.modules.layers.specialtylayers", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.specialtylayers.Tlayer", "modulename": "NDNT.modules.layers.specialtylayers", "qualname": "Tlayer", "kind": "class", "doc": "<p>NDN Layer where num_lags is handled convolutionally (but all else is normal)</p>\n\n<p>Args (required):\n    input_dims: tuple or list of ints, (num_channels, height, width, lags)\n    num_filters: number of output filters\n    filter_dims: width of convolutional kernel (int or list of ints)\nArgs (optional):\n    padding: 'same' or 'valid' (default 'same')\n    weight_init: str, 'uniform', 'normal', 'xavier', 'zeros', or None\n    bias_init: str, 'uniform', 'normal', 'xavier', 'zeros', or None\n    bias: bool, whether to include bias term\n    NLtype: str, 'lin', 'relu', 'tanh', 'sigmoid', 'elu', 'none'</p>\n", "bases": "NDNT.modules.layers.ndnlayer.NDNLayer"}, {"fullname": "NDNT.modules.layers.specialtylayers.Tlayer.__init__", "modulename": "NDNT.modules.layers.specialtylayers", "qualname": "Tlayer.__init__", "kind": "function", "doc": "<p>Tlayer: NDN Layer where num_lags is handled convolutionally (but all else is normal).</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  tuple or list of ints, (num_channels, height, width, lags)</li>\n<li><strong>num_filters:</strong>  number of output filters</li>\n<li><strong>num_lags:</strong>  number of lags in spatiotemporal filter</li>\n<li><strong>temporal_tent_spacing:</strong>  int, spacing of tent basis functions</li>\n<li><strong>output_norm:</strong>  str, 'batch', 'batchX', or None</li>\n<li><strong>res_layer:</strong>  bool, whether to make a residual layer</li>\n<li><strong>**kwargs:</strong>  additional arguments to pass to NDNLayer</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_lags</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">temporal_tent_spacing</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">output_norm</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">res_layer</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.specialtylayers.Tlayer.tent_basis", "modulename": "NDNT.modules.layers.specialtylayers", "qualname": "Tlayer.tent_basis", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.specialtylayers.Tlayer.res_layer", "modulename": "NDNT.modules.layers.specialtylayers", "qualname": "Tlayer.res_layer", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.specialtylayers.Tlayer.folded_dims", "modulename": "NDNT.modules.layers.specialtylayers", "qualname": "Tlayer.folded_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.specialtylayers.Tlayer.forward", "modulename": "NDNT.modules.layers.specialtylayers", "qualname": "Tlayer.forward", "kind": "function", "doc": "<p>Forward pass through the Tlayer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  torch.Tensor, input tensor</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>y: torch.Tensor, output tensor</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.specialtylayers.Tlayer.plot_filters", "modulename": "NDNT.modules.layers.specialtylayers", "qualname": "Tlayer.plot_filters", "kind": "function", "doc": "<p>Overload plot_filters to automatically time_reverse.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>cmaps:</strong>  str or list of str, colormap(s) to use</li>\n<li><strong>num_cols:</strong>  int, number of columns to use in plot</li>\n<li><strong>row_height:</strong>  int, height of each row in plot</li>\n<li><strong>time_reverse:</strong>  bool, whether to reverse the time dimension</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">cmaps</span><span class=\"o\">=</span><span class=\"s1\">&#39;gray&#39;</span>, </span><span class=\"param\"><span class=\"n\">num_cols</span><span class=\"o\">=</span><span class=\"mi\">8</span>, </span><span class=\"param\"><span class=\"n\">row_height</span><span class=\"o\">=</span><span class=\"mi\">2</span>, </span><span class=\"param\"><span class=\"n\">time_reverse</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.specialtylayers.Tlayer.layer_dict", "modulename": "NDNT.modules.layers.specialtylayers", "qualname": "Tlayer.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">num_lags</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">res_layer</span><span class=\"o\">=</span><span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.specialtylayers.L1convLayer", "modulename": "NDNT.modules.layers.specialtylayers", "qualname": "L1convLayer", "kind": "class", "doc": "<p>First start with non-convolutional version.</p>\n\n<p>L1convLayer: Convolutional layer with L1 regularization.</p>\n", "bases": "NDNT.modules.layers.ndnlayer.NDNLayer"}, {"fullname": "NDNT.modules.layers.specialtylayers.L1convLayer.__init__", "modulename": "NDNT.modules.layers.specialtylayers", "qualname": "L1convLayer.__init__", "kind": "function", "doc": "<p>Set up ConvLayer with L1 regularization.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>**kwargs:</strong>  additional arguments to pass to ConvLayer</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.specialtylayers.L1convLayer.weight_minus", "modulename": "NDNT.modules.layers.specialtylayers", "qualname": "L1convLayer.weight_minus", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.specialtylayers.L1convLayer.window", "modulename": "NDNT.modules.layers.specialtylayers", "qualname": "L1convLayer.window", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.specialtylayers.L1convLayer.tent_basis", "modulename": "NDNT.modules.layers.specialtylayers", "qualname": "L1convLayer.tent_basis", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.specialtylayers.L1convLayer.preprocess_weights", "modulename": "NDNT.modules.layers.specialtylayers", "qualname": "L1convLayer.preprocess_weights", "kind": "function", "doc": "<p>Preprocess weights for L1convLayer.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>w: torch.Tensor, preprocessed weights</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.specialtylayers.L1convLayer.reset_parameters2", "modulename": "NDNT.modules.layers.specialtylayers", "qualname": "L1convLayer.reset_parameters2", "kind": "function", "doc": "<p>Reset parameters for L1convLayer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>weights_initializer:</strong>  str, 'uniform', 'normal', 'xavier', 'zeros', or None</li>\n<li><strong>bias_initializer:</strong>  str, 'uniform', 'normal', 'xavier', 'zeros', or None</li>\n<li><strong>param:</strong>  dict, additional parameters to pass to the initializer</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">weights_initializer</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">bias_initializer</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">param</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.specialtylayers.L1convLayer.layer_dict", "modulename": "NDNT.modules.layers.specialtylayers", "qualname": "L1convLayer.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.specialtylayers.ParametricTuneLayer", "modulename": "NDNT.modules.layers.specialtylayers", "qualname": "ParametricTuneLayer", "kind": "class", "doc": "<p>Function specific to Declan/Huk datasets: generates parametric orientation tuning curves that \nare comobined with weights over spatial frequency</p>\n", "bases": "NDNT.modules.layers.ndnlayer.NDNLayer"}, {"fullname": "NDNT.modules.layers.specialtylayers.ParametricTuneLayer.__init__", "modulename": "NDNT.modules.layers.specialtylayers", "qualname": "ParametricTuneLayer.__init__", "kind": "function", "doc": "<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  tuple or list of ints, (num_channels, height, width, lags)</li>\n<li><strong>num_filters:</strong>  number of output filters</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">pos_constraint</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">static_Ftuning</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.specialtylayers.ParametricTuneLayer.output_dims", "modulename": "NDNT.modules.layers.specialtylayers", "qualname": "ParametricTuneLayer.output_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.specialtylayers.ParametricTuneLayer.num_outputs", "modulename": "NDNT.modules.layers.specialtylayers", "qualname": "ParametricTuneLayer.num_outputs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.specialtylayers.ParametricTuneLayer.filters", "modulename": "NDNT.modules.layers.specialtylayers", "qualname": "ParametricTuneLayer.filters", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.specialtylayers.ParametricTuneLayer.make_orientation_filters", "modulename": "NDNT.modules.layers.specialtylayers", "qualname": "ParametricTuneLayer.make_orientation_filters", "kind": "function", "doc": "<p>Construct self.tuning_cuves based on thetas, widths, and ds_index lists. If a single\nvalue is passed, it is used for all filters. If a list is passed, it must be the same\nlength as num_filters. </p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>theta_list:</strong>  list or float, preferred angles (0-6) corresponding to 0-180 degrees</li>\n<li><strong>width_list:</strong>  list or float, tuning widths (std of gaussian)</li>\n<li><strong>ds_list:</strong>  list or float, ratio of preferred orientation to 180 off</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None, but creates self.tuning_curves</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">theta_list</span>, </span><span class=\"param\"><span class=\"n\">width_list</span>, </span><span class=\"param\"><span class=\"n\">ds_list</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.specialtylayers.ParametricTuneLayer.stim_tuning", "modulename": "NDNT.modules.layers.specialtylayers", "qualname": "ParametricTuneLayer.stim_tuning", "kind": "function", "doc": "<p>Generate parametric tuning curve over 12 angles (6 are preferred direction)</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>theta:</strong>  preferred angle (0-6) corresponding to 0-180 degrees</li>\n<li><strong>width:</strong>  tuning width (std of gaussian)</li>\n<li><strong>ds_index:</strong>  ratio of preferred orientation to 180 off</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">theta</span>, </span><span class=\"param\"><span class=\"n\">width</span>, </span><span class=\"param\"><span class=\"n\">ds_index</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.specialtylayers.ParametricTuneLayer.preprocess_weights", "modulename": "NDNT.modules.layers.specialtylayers", "qualname": "ParametricTuneLayer.preprocess_weights", "kind": "function", "doc": "<p>Preprocess the weights before using them in the forward pass.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>w: torch.Tensor, preprocessed weights</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.specialtylayers.ParametricTuneLayer.get_weights", "modulename": "NDNT.modules.layers.specialtylayers", "qualname": "ParametricTuneLayer.get_weights", "kind": "function", "doc": "<p>Because preprocess_weights is overloaded, need to overload get_weights too</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">basic</span><span class=\"o\">=</span><span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"n\">to_reshape</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.specialtylayers.ParametricTuneLayer.layer_dict", "modulename": "NDNT.modules.layers.specialtylayers", "qualname": "ParametricTuneLayer.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  list of 4 ints, dimensions of input</li>\n<li><strong>num_filters:</strong>  int, number of filters in layer</li>\n<li><strong>bias:</strong>  bool, whether to include bias term</li>\n<li><strong>NLtype:</strong>  str, type of nonlinearity to use (see activations.py)</li>\n<li><strong>norm_type:</strong>  int, type of filter normalization to use (0=None, 1=filters are unit vectors, 2=maxnorm?)</li>\n<li><strong>pos_constraint:</strong>  bool, whether to constrain weights to be positive</li>\n<li><strong>num_inh:</strong>  int, number of inhibitory units (creates ei_mask and makes \"inhibitory\" units have negative output)</li>\n<li><strong>initialize_center:</strong>  bool, whether to initialize the weights to have a Gaussian envelope</li>\n<li><strong>reg_vals:</strong>  dict, regularization values to use (see regularizers.py)</li>\n<li><strong>output_norm:</strong>  str, type of output normalization to use</li>\n<li><strong>weights_initializer:</strong>  str, type of weight initialization to use</li>\n<li><strong>bias_initializer:</strong>  str, type of bias initialization to use</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>dict: dictionary of layer parameters</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">static_Ftuning</span><span class=\"o\">=</span><span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.specialtylayers.OnOffLayer", "modulename": "NDNT.modules.layers.specialtylayers", "qualname": "OnOffLayer", "kind": "class", "doc": "<p>OnOffLayer: Layer with separate on and off filters.</p>\n", "bases": "Tlayer"}, {"fullname": "NDNT.modules.layers.specialtylayers.OnOffLayer.__init__", "modulename": "NDNT.modules.layers.specialtylayers", "qualname": "OnOffLayer.__init__", "kind": "function", "doc": "<p>Args (required):\n    input_dims: tuple or list of ints, (num_channels, height, width, lags)\n    num_filters: number of output filters\n    num_lags: number of lags in spatiotemporal filter</p>\n\n<p>Args (optional):\n    temporal_tent_spacing: int, spacing of tent basis functions\n    output_norm: str, 'batch', 'batchX', or None\n    res_layer: bool, whether to make a residual layer\n    **kwargs: additional arguments to pass to NDNLayer</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_lags</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">temporal_tent_spacing</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">output_norm</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">res_layer</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.specialtylayers.OnOffLayer.plot_filters", "modulename": "NDNT.modules.layers.specialtylayers", "qualname": "OnOffLayer.plot_filters", "kind": "function", "doc": "<p>Plot the filters for the OnOffLayer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>time_reverse:</strong>  bool, whether to reverse the time dimension</li>\n<li><strong>**kwargs:</strong>  additional arguments to pass to the plotting function</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">time_reverse</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.specialtylayers.OnOffLayer.forward", "modulename": "NDNT.modules.layers.specialtylayers", "qualname": "OnOffLayer.forward", "kind": "function", "doc": "<p>Forward pass through the OnOffLayer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  torch.Tensor, input tensor</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>y: torch.Tensor, output tensor</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.specialtylayers.OnOffLayer.layer_dict", "modulename": "NDNT.modules.layers.specialtylayers", "qualname": "OnOffLayer.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>num_lags:</strong>  int, number of lags in spatiotemporal filter</li>\n<li><strong>**kwargs:</strong>  additional arguments to pass to NDNLayer</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Ldict: dict, dictionary of layer parameters</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">num_lags</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.specialtylayers.MaskLayer", "modulename": "NDNT.modules.layers.specialtylayers", "qualname": "MaskLayer", "kind": "class", "doc": "<p>MaskLayer: Layer with a mask applied to the weights.</p>\n", "bases": "NDNT.modules.layers.ndnlayer.NDNLayer"}, {"fullname": "NDNT.modules.layers.specialtylayers.MaskLayer.__init__", "modulename": "NDNT.modules.layers.specialtylayers", "qualname": "MaskLayer.__init__", "kind": "function", "doc": "<p>MaskLayer: Layer with a mask applied to the weights.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  tuple or list of ints, (num_channels, height, width, lags)</li>\n<li><strong>num_filters:</strong>  number of output filters</li>\n<li><strong>mask:</strong>  np.ndarray, mask to apply to the weights</li>\n<li><strong>NLtype:</strong>  str, 'lin', 'relu', 'tanh', 'sigmoid', 'elu', 'none'</li>\n<li><strong>norm_type:</strong>  int, normalization type</li>\n<li><strong>pos_constraint:</strong>  int, whether to enforce non-negative weights</li>\n<li><strong>num_inh:</strong>  int, number of inhibitory filters</li>\n<li><strong>bias:</strong>  bool, whether to include bias term</li>\n<li><strong>weights_initializer:</strong>  str, 'uniform', 'normal', 'xavier', 'zeros', or None</li>\n<li><strong>output_norm:</strong>  str, 'batch', 'batchX', or None</li>\n<li><strong>initialize_center:</strong>  bool, whether to initialize the center</li>\n<li><strong>bias_initializer:</strong>  str, 'uniform', 'normal', 'xavier', 'zeros', or None</li>\n<li><strong>reg_vals:</strong>  dict, regularization values</li>\n<li><strong>**kwargs:</strong>  additional arguments to pass to NDNLayer</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">NLtype</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;lin&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">norm_type</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">pos_constraint</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">num_inh</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">weights_initializer</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;xavier_uniform&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">output_norm</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">initialize_center</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">bias_initializer</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;zeros&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">reg_vals</span><span class=\"p\">:</span> <span class=\"nb\">dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.specialtylayers.MaskLayer.preprocess_weights", "modulename": "NDNT.modules.layers.specialtylayers", "qualname": "MaskLayer.preprocess_weights", "kind": "function", "doc": "<p>Preprocess weights for MaskLayer.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>w: torch.Tensor, preprocessed weights</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.specialtylayers.MaskLayer.layer_dict", "modulename": "NDNT.modules.layers.specialtylayers", "qualname": "MaskLayer.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>mask:</strong>  np.ndarray, mask to apply to the weights</li>\n<li><strong>**kwargs:</strong>  additional arguments to pass to NDNLayer</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Ldict: dict, dictionary of layer parameters</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.timelayers", "modulename": "NDNT.modules.layers.timelayers", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.timelayers.TimeShiftLayer", "modulename": "NDNT.modules.layers.timelayers", "qualname": "TimeShiftLayer", "kind": "class", "doc": "<p>Layer to shift in time dimension by num_lags</p>\n", "bases": "NDNT.modules.layers.ndnlayer.NDNLayer"}, {"fullname": "NDNT.modules.layers.timelayers.TimeShiftLayer.__init__", "modulename": "NDNT.modules.layers.timelayers", "qualname": "TimeShiftLayer.__init__", "kind": "function", "doc": "<p>TimeLayer: Layer to track experiment time and a weighted output.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims:</strong>  tuple or list of ints, (num_channels, height, width, lags/angles)</li>\n<li><strong>num_lags:</strong>  number of lags to shift back by</li>\n<li><strong>**kwargs:</strong>  additional arguments to pass to NDNLayer</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">num_lags</span><span class=\"o\">=</span><span class=\"mi\">1</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.timelayers.TimeShiftLayer.output_dims", "modulename": "NDNT.modules.layers.timelayers", "qualname": "TimeShiftLayer.output_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.timelayers.TimeShiftLayer.num_outputs", "modulename": "NDNT.modules.layers.timelayers", "qualname": "TimeShiftLayer.num_outputs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.timelayers.TimeShiftLayer.num_lags", "modulename": "NDNT.modules.layers.timelayers", "qualname": "TimeShiftLayer.num_lags", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.timelayers.TimeShiftLayer.forward", "modulename": "NDNT.modules.layers.timelayers", "qualname": "TimeShiftLayer.forward", "kind": "function", "doc": "<p>Shift in batch dimesnion by num_lags and pad by 0</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  torch.Tensor, input tensor</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>y: torch.Tensor, output tensor</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.timelayers.TimeShiftLayer.layer_dict", "modulename": "NDNT.modules.layers.timelayers", "qualname": "TimeShiftLayer.layer_dict", "kind": "function", "doc": "<p>This outputs a dictionary of parameters that need to input into the layer to completely specify.\nOutput is a dictionary with these keywords. \n-- All layer-specific inputs are included in the returned dict\n-- Values that must be set are set to empty lists\n-- Other values will be given their defaults</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">num_lags</span><span class=\"o\">=</span><span class=\"mi\">1</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.layers.timelayers.TimeLayer", "modulename": "NDNT.modules.layers.timelayers", "qualname": "TimeLayer", "kind": "class", "doc": "<p>Layer to track experiment time and a weighted output.</p>\n", "bases": "NDNT.modules.layers.ndnlayer.NDNLayer"}, {"fullname": "NDNT.modules.layers.timelayers.TimeLayer.__init__", "modulename": "NDNT.modules.layers.timelayers", "qualname": "TimeLayer.__init__", "kind": "function", "doc": "<p>TimeLayer: Layer to track experiment time and a weighted output.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>start_time:</strong>  float, start time of experiment</li>\n<li><strong>end_time:</strong>  float, end time of experiment</li>\n<li><strong>input_dims:</strong>  tuple or list of ints, (num_channels, height, width, lags)</li>\n<li><strong>num_bases:</strong>  number of tent basis functions</li>\n<li><strong>num_filters:</strong>  number of output filters</li>\n<li><strong>filter_dims:</strong>  tuple or list of ints, (num_channels, height, width, lags)</li>\n<li><strong>pos_constraint:</strong>  bool, whether to enforce non-negative weights</li>\n<li><strong>**kwargs:</strong>  additional arguments to pass to NDNLayer</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">start_time</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">end_time</span><span class=\"o\">=</span><span class=\"mi\">1000</span>,</span><span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_bases</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">filter_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">pos_constraint</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.layers.timelayers.TimeLayer.output_dims", "modulename": "NDNT.modules.layers.timelayers", "qualname": "TimeLayer.output_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.timelayers.TimeLayer.num_outputs", "modulename": "NDNT.modules.layers.timelayers", "qualname": "TimeLayer.num_outputs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.layers.timelayers.TimeLayer.forward", "modulename": "NDNT.modules.layers.timelayers", "qualname": "TimeLayer.forward", "kind": "function", "doc": "<p>Forward pass through the TimeLayer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  torch.Tensor, input tensor</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>y: torch.Tensor, output tensor</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.regularization", "modulename": "NDNT.modules.regularization", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.regularization.Regularization", "modulename": "NDNT.modules.regularization", "qualname": "Regularization", "kind": "class", "doc": "<p>Class for handling layer-wise regularization. </p>\n\n<p>This class stores all info for regularization, and sets up regularization modules for training, \nand returns reg_penalty from layer when passed in weights. Note that boundary conditions for a \ngiven regularization is specified by a regularization 'bc', and passing that a dictionary. By \ndefault, boundary conditions are on for a given regularization if not explicitly turned off.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>vals (dict):</strong>  values for different types of regularization stored as\nfloats</li>\n<li><strong>vals_ph (dict):</strong>  placeholders for different types of regularization to\nsimplify the tf Graph when experimenting with different reg vals</li>\n<li><strong>vals_var (dict):</strong>  values for different types of regularization stored as\n(un-trainable) tf.Variables</li>\n<li><strong>mats (dict):</strong>  matrices for different types of regularization stored as\ntf constants</li>\n<li><strong>penalties (dict):</strong>  tf ops for evaluating different regularization \npenalties</li>\n<li><strong>input_dims (list):</strong>  dimensions of layer input size; for constructing reg \nmatrices</li>\n<li><strong>num_outputs (int):</strong>  dimension of layer output size; for generating \ntarget weights in norm2</li>\n</ul>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "NDNT.modules.regularization.Regularization.__init__", "modulename": "NDNT.modules.regularization", "qualname": "Regularization.__init__", "kind": "function", "doc": "<p>Constructor for Regularization class. This stores all info for regularization, and \nsets up regularization modules for training, and returns reg_penalty from layer when passed in weights</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims (list of ints):</strong>  dimension of input size (for building reg mats)</li>\n<li><strong>vals (dict, optional):</strong>  key-value pairs specifying value for each type of regularization </li>\n<li><strong>Note:</strong>  to pass in boundary_condition information, use a dict in vals with the values corresponding to</li>\n<li><strong>particular regularization, e,g., 'BCs':</strong> {'d2t':1, 'd2x':0} (1=on, 0=off)</li>\n</ul>\n\n<h6 id=\"raises\">Raises:</h6>\n\n<ul>\n<li><strong>TypeError:</strong>  If <code>input_dims</code> is not specified</li>\n<li><strong>TypeError:</strong>  If <code>num_outputs</code> is not specified</li>\n</ul>\n\n<h6 id=\"notes\">Notes:</h6>\n\n<blockquote>\n  <p>I'm using my old regularization matrices, which is made for the following 3-dimensional \n  weights with dimensions ordered in [NX, NY, num_lags]. Currently, filter_dims is 4-d: \n  [num_filters, NX, NY, num_lags] so this will need to rearrage so num_filters gets folded into last \n  dimension so that it will work with d2t regularization, if reshape is necessary]</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">filter_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">vals</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_outputs</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">normalize</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">pos_constraint</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">folded_lags</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.regularization.Regularization.input_dims", "modulename": "NDNT.modules.regularization", "qualname": "Regularization.input_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.regularization.Regularization.vals", "modulename": "NDNT.modules.regularization", "qualname": "Regularization.vals", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.regularization.Regularization.reg_modules", "modulename": "NDNT.modules.regularization", "qualname": "Regularization.reg_modules", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.regularization.Regularization.normalize", "modulename": "NDNT.modules.regularization", "qualname": "Regularization.normalize", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.regularization.Regularization.folded_lags", "modulename": "NDNT.modules.regularization", "qualname": "Regularization.folded_lags", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.regularization.Regularization.num_outputs", "modulename": "NDNT.modules.regularization", "qualname": "Regularization.num_outputs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.regularization.Regularization.pos_constraint", "modulename": "NDNT.modules.regularization", "qualname": "Regularization.pos_constraint", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.regularization.Regularization.boundary_conditions", "modulename": "NDNT.modules.regularization", "qualname": "Regularization.boundary_conditions", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.regularization.Regularization.activity_regmodule", "modulename": "NDNT.modules.regularization", "qualname": "Regularization.activity_regmodule", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.regularization.Regularization.unit_reg", "modulename": "NDNT.modules.regularization", "qualname": "Regularization.unit_reg", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.regularization.Regularization.set_reg_val", "modulename": "NDNT.modules.regularization", "qualname": "Regularization.set_reg_val", "kind": "function", "doc": "<p>Set regularization value in self.vals dict. Secondarily, it will also determine whether unit-reg\napplies or not, based on whether any of the reg vals are lists or arrays.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>reg_type (str):</strong>  see <code>_allowed_reg_types</code> for options</li>\n<li><strong>reg_val (float or array):</strong>  value of regularization parameter, or list of values if unit_reg</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>bool: True if <code>reg_type</code> has not been previously set</p>\n</blockquote>\n\n<p>Note: can also pass in a dictionary with boundary condition information addressed by reg_type</p>\n\n<h6 id=\"raises\">Raises:</h6>\n\n<ul>\n<li><strong>ValueError:</strong>  If <code>reg_type</code> is not a valid regularization type</li>\n<li><strong>ValueError:</strong>  If <code>reg_val</code> is less than 0.0</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">reg_type</span>, </span><span class=\"param\"><span class=\"n\">reg_val</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.regularization.Regularization.unit_reg_convert", "modulename": "NDNT.modules.regularization", "qualname": "Regularization.unit_reg_convert", "kind": "function", "doc": "<p>Can convert reg object to turn on or off unit_reg (default turns it on</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">unit_reg</span><span class=\"o\">=</span><span class=\"kc\">True</span>, </span><span class=\"param\"><span class=\"n\">num_outputs</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.regularization.Regularization.build_reg_modules", "modulename": "NDNT.modules.regularization", "qualname": "Regularization.build_reg_modules", "kind": "function", "doc": "<p>Prepares regularization modules in train based on current regularization values</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">device</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.regularization.Regularization.compute_reg_loss", "modulename": "NDNT.modules.regularization", "qualname": "Regularization.compute_reg_loss", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">weights</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.regularization.Regularization.compute_activity_regularization", "modulename": "NDNT.modules.regularization", "qualname": "Regularization.compute_activity_regularization", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">layer_output</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.regularization.Regularization.reg_copy", "modulename": "NDNT.modules.regularization", "qualname": "Regularization.reg_copy", "kind": "function", "doc": "<p>Copy regularization to new structure</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.regularization.Regularization.get_reg_class", "modulename": "NDNT.modules.regularization", "qualname": "Regularization.get_reg_class", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">reg_type</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.regularization.RegModule", "modulename": "NDNT.modules.regularization", "qualname": "RegModule", "kind": "class", "doc": "<p>Base class for regularization modules</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "NDNT.modules.regularization.RegModule.__init__", "modulename": "NDNT.modules.regularization", "qualname": "RegModule.__init__", "kind": "function", "doc": "<p>Constructor for Reg_module class</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">reg_type</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">reg_val</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_dims</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">unit_reg</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">folded_lags</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">pos_constraint</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.regularization.RegModule.reg_type", "modulename": "NDNT.modules.regularization", "qualname": "RegModule.reg_type", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.regularization.RegModule.unit_reg", "modulename": "NDNT.modules.regularization", "qualname": "RegModule.unit_reg", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.regularization.RegModule.input_dims", "modulename": "NDNT.modules.regularization", "qualname": "RegModule.input_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.regularization.RegModule.num_dims", "modulename": "NDNT.modules.regularization", "qualname": "RegModule.num_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.regularization.RegModule.folded_lags", "modulename": "NDNT.modules.regularization", "qualname": "RegModule.folded_lags", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.regularization.RegModule.pos_constraint", "modulename": "NDNT.modules.regularization", "qualname": "RegModule.pos_constraint", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.regularization.RegModule.forward", "modulename": "NDNT.modules.regularization", "qualname": "RegModule.forward", "kind": "function", "doc": "<p>Defines the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"alert note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">weights</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.regularization.LocalityReg", "modulename": "NDNT.modules.regularization", "qualname": "LocalityReg", "kind": "class", "doc": "<p>Regularization to penalize locality separably for each dimension</p>\n", "bases": "RegModule"}, {"fullname": "NDNT.modules.regularization.LocalityReg.__init__", "modulename": "NDNT.modules.regularization", "qualname": "LocalityReg.__init__", "kind": "function", "doc": "<p>Constructor for LocalityReg class</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">reg_type</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">reg_val</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">num_dims</span><span class=\"o\">=</span><span class=\"mi\">0</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.regularization.LocalityReg.compute_reg_penalty", "modulename": "NDNT.modules.regularization", "qualname": "LocalityReg.compute_reg_penalty", "kind": "function", "doc": "<p>Compute regularization penalty for locality</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">weights</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.regularization.LocalityReg.build_reg_mats", "modulename": "NDNT.modules.regularization", "qualname": "LocalityReg.build_reg_mats", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.regularization.DiagonalReg", "modulename": "NDNT.modules.regularization", "qualname": "DiagonalReg", "kind": "class", "doc": "<p>Regularization module for diagonal penalties</p>\n", "bases": "RegModule"}, {"fullname": "NDNT.modules.regularization.DiagonalReg.__init__", "modulename": "NDNT.modules.regularization", "qualname": "DiagonalReg.__init__", "kind": "function", "doc": "<p>Constructor for Reg_module class</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">reg_type</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">reg_val</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.regularization.DiagonalReg.input_dims", "modulename": "NDNT.modules.regularization", "qualname": "DiagonalReg.input_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.regularization.DiagonalReg.compute_reg_penalty", "modulename": "NDNT.modules.regularization", "qualname": "DiagonalReg.compute_reg_penalty", "kind": "function", "doc": "<p>Compute regularization loss</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">weights</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.regularization.DiagonalReg.build_reg_mats", "modulename": "NDNT.modules.regularization", "qualname": "DiagonalReg.build_reg_mats", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">reg_type</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.regularization.InlineReg", "modulename": "NDNT.modules.regularization", "qualname": "InlineReg", "kind": "class", "doc": "<p>Regularization module for inline penalties</p>\n", "bases": "RegModule"}, {"fullname": "NDNT.modules.regularization.InlineReg.__init__", "modulename": "NDNT.modules.regularization", "qualname": "InlineReg.__init__", "kind": "function", "doc": "<p>Constructor for Reg_module class</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">reg_type</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">reg_val</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.regularization.InlineReg.compute_reg_penalty", "modulename": "NDNT.modules.regularization", "qualname": "InlineReg.compute_reg_penalty", "kind": "function", "doc": "<p>Calculate regularization penalty for various reg types</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">weights</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.regularization.ConvReg", "modulename": "NDNT.modules.regularization", "qualname": "ConvReg", "kind": "class", "doc": "<p>Regularization module for convolutional penalties</p>\n", "bases": "RegModule"}, {"fullname": "NDNT.modules.regularization.ConvReg.__init__", "modulename": "NDNT.modules.regularization", "qualname": "ConvReg.__init__", "kind": "function", "doc": "<p>Constructor for Reg_module class</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">reg_type</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">reg_val</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">bc_val</span><span class=\"o\">=</span><span class=\"mi\">1</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.regularization.ConvReg.BC", "modulename": "NDNT.modules.regularization", "qualname": "ConvReg.BC", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.regularization.ConvReg.compute_reg_penalty", "modulename": "NDNT.modules.regularization", "qualname": "ConvReg.compute_reg_penalty", "kind": "function", "doc": "<p>I'm separating the code for more complicated regularization penalties in the simplest possible way here, but\nthis can be done more (or less) elaborately in the future. The challenge here is that the dimension of the weight\nvector (1-D, 2-D, 3-D) determines what sort of Laplacian matrix, and convolution, to do\nNote that all convolutions are implicitly 'valid' so no boundary conditions</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">weights</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.regularization.Tikhanov", "modulename": "NDNT.modules.regularization", "qualname": "Tikhanov", "kind": "class", "doc": "<p>Regularization module for Tikhanov regularization</p>\n", "bases": "RegModule"}, {"fullname": "NDNT.modules.regularization.Tikhanov.__init__", "modulename": "NDNT.modules.regularization", "qualname": "Tikhanov.__init__", "kind": "function", "doc": "<p>Constructor for Tikhanov class</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">reg_type</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">reg_val</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">bc_val</span><span class=\"o\">=</span><span class=\"mi\">0</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.regularization.Tikhanov.input_dims", "modulename": "NDNT.modules.regularization", "qualname": "Tikhanov.input_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.regularization.Tikhanov.compute_reg_penalty", "modulename": "NDNT.modules.regularization", "qualname": "Tikhanov.compute_reg_penalty", "kind": "function", "doc": "<p>Calculate regularization penalty for Tikhanov reg types</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">weights</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.regularization.TikhanovC", "modulename": "NDNT.modules.regularization", "qualname": "TikhanovC", "kind": "class", "doc": "<p>Regularization module for Tikhanov-collapse regularization -- where all but one\ndimension is marginalized over first.</p>\n", "bases": "RegModule"}, {"fullname": "NDNT.modules.regularization.TikhanovC.__init__", "modulename": "NDNT.modules.regularization", "qualname": "TikhanovC.__init__", "kind": "function", "doc": "<p>Constructor for Tikhanov-Collapse class: where dimensions are summed over before Tikhanov\nmatrix is applied. In case where weights are not pos-constrained, be sure to pass in a \nboundary condition of '1' and it will square weights before summing. Otherwise, you will\nget a lot of -1s in the output.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">reg_type</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">reg_val</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">bc_val</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">pos_constraint</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.regularization.TikhanovC.input_dims", "modulename": "NDNT.modules.regularization", "qualname": "TikhanovC.input_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.regularization.TikhanovC.collapse_dims", "modulename": "NDNT.modules.regularization", "qualname": "TikhanovC.collapse_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.regularization.TikhanovC.compute_reg_penalty", "modulename": "NDNT.modules.regularization", "qualname": "TikhanovC.compute_reg_penalty", "kind": "function", "doc": "<p>Calculate regularization penalty for Tikhanov reg types</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">weights</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.regularization.ActivityReg", "modulename": "NDNT.modules.regularization", "qualname": "ActivityReg", "kind": "class", "doc": "<p>Regularization to penalize activity separably for each dimension\nNote that the penalty needs to be computed elsewhere and just stored here</p>\n", "bases": "RegModule"}, {"fullname": "NDNT.modules.regularization.ActivityReg.__init__", "modulename": "NDNT.modules.regularization", "qualname": "ActivityReg.__init__", "kind": "function", "doc": "<p>Constructor for LocalityReg class</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">reg_type</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">reg_val</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">num_dims</span><span class=\"o\">=</span><span class=\"mi\">0</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.modules.regularization.ActivityReg.activity_penalty", "modulename": "NDNT.modules.regularization", "qualname": "ActivityReg.activity_penalty", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.modules.regularization.ActivityReg.compute_activity_penalty", "modulename": "NDNT.modules.regularization", "qualname": "ActivityReg.compute_activity_penalty", "kind": "function", "doc": "<p>Computes activity penalty from activations -- called in layer forward</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">acts</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.modules.regularization.ActivityReg.compute_reg_penalty", "modulename": "NDNT.modules.regularization", "qualname": "ActivityReg.compute_reg_penalty", "kind": "function", "doc": "<p>Compute regularization penalty for locality</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">weights</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.networks", "modulename": "NDNT.networks", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.networks.FFnetwork", "modulename": "NDNT.networks", "qualname": "FFnetwork", "kind": "class", "doc": "<p>Initializes an instance of the network.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>layer_list (list, optional):</strong>  A list of dictionaries representing the layers of the network. Defaults to None.</li>\n<li><strong>ffnet_type (str, optional):</strong>  The type of the feedforward network. Defaults to 'normal'.</li>\n<li><strong>xstim_n (str, optional):</strong>  The name of the stimulus input. Defaults to 'stim'.</li>\n<li><strong>ffnet_n (list, optional):</strong>  A list of feedforward networks. Defaults to None.</li>\n<li><strong>input_dims_list (list, optional):</strong>  A list of input dimensions for each layer. Defaults to None.</li>\n<li><strong>reg_list (list, optional):</strong>  A list of regularization parameters. Defaults to None.</li>\n<li><strong>scaffold_levels (list, optional):</strong>  A list of scaffold levels. Defaults to None.</li>\n<li><strong>**kwargs:</strong>  Additional keyword arguments.</li>\n</ul>\n\n<h6 id=\"raises\">Raises:</h6>\n\n<ul>\n<li><strong>AssertionError:</strong>  If layer_list is not provided.</li>\n</ul>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "NDNT.networks.FFnetwork.__init__", "modulename": "NDNT.networks", "qualname": "FFnetwork.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">layer_list</span><span class=\"p\">:</span> <span class=\"nb\">list</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">ffnet_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;normal&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">xstim_n</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;stim&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">ffnet_n</span><span class=\"p\">:</span> <span class=\"nb\">list</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">input_dims_list</span><span class=\"p\">:</span> <span class=\"nb\">list</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">reg_list</span><span class=\"p\">:</span> <span class=\"nb\">list</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">scaffold_levels</span><span class=\"p\">:</span> <span class=\"nb\">list</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.networks.FFnetwork.LayerTypes", "modulename": "NDNT.networks", "qualname": "FFnetwork.LayerTypes", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.networks.FFnetwork.network_type", "modulename": "NDNT.networks", "qualname": "FFnetwork.network_type", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.networks.FFnetwork.layer_list", "modulename": "NDNT.networks", "qualname": "FFnetwork.layer_list", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.networks.FFnetwork.layer_types", "modulename": "NDNT.networks", "qualname": "FFnetwork.layer_types", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.networks.FFnetwork.xstim_n", "modulename": "NDNT.networks", "qualname": "FFnetwork.xstim_n", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.networks.FFnetwork.ffnets_in", "modulename": "NDNT.networks", "qualname": "FFnetwork.ffnets_in", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.networks.FFnetwork.shifter", "modulename": "NDNT.networks", "qualname": "FFnetwork.shifter", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.networks.FFnetwork.layers", "modulename": "NDNT.networks", "qualname": "FFnetwork.layers", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.networks.FFnetwork.output_dims", "modulename": "NDNT.networks", "qualname": "FFnetwork.output_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.networks.FFnetwork.num_outputs", "modulename": "NDNT.networks", "qualname": "FFnetwork.num_outputs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.networks.FFnetwork.determine_input_dims", "modulename": "NDNT.networks", "qualname": "FFnetwork.determine_input_dims", "kind": "function", "doc": "<p>Sets input_dims given network inputs. Can be overloaded depending on the network type. For this base class, there\nare two types of network input: external stimulus (xstim_n) or a list of internal (ffnet_in) networks:\n    For external inputs, it just uses the passed-in input_dims\n    For internal network inputs, it will concatenate inputs along the filter dimension, but MUST match other dims\nAs currently designed, this can either external or internal, but not both</p>\n\n<h6 id=\"this-sets-the-following-internal-ffnetwork-properties\">This sets the following internal FFnetwork properties:</h6>\n\n<blockquote>\n  <p>self.input_dims\n  self.input_dims_list</p>\n</blockquote>\n\n<p>and returns Boolean whether the passed in input dims are valid</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims_list (list):</strong>  A list of input dimensions for each layer.</li>\n<li><strong>ffnet_type (str):</strong>  The type of the feedforward network.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>valid_input_dims (bool): Whether the passed in input dims are valid.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">input_dims_list</span>, </span><span class=\"param\"><span class=\"n\">ffnet_type</span><span class=\"o\">=</span><span class=\"s1\">&#39;normal&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.networks.FFnetwork.preprocess_input", "modulename": "NDNT.networks", "qualname": "FFnetwork.preprocess_input", "kind": "function", "doc": "<p>Preprocess inputs to the ffnetwork according to the network type. If there\nis only one batch passed in, it does not matter what network type. But mutiple\ninputs (in a list) either:\n    'normal': concatenates\n    'add': adds inputs together (not must be same size or broadcastable)\n    'mult': multiplies x1<em>(1+x2)</em>(1+x3+...) with same size req as 'add' </p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>inputs (list, torch.Tensor):</strong>  The input to the network.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>x (torch.Tensor): The preprocessed input.</p>\n</blockquote>\n\n<h6 id=\"raises\">Raises:</h6>\n\n<ul>\n<li><strong>ValueError:</strong>  If no layers are defined.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">inputs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.networks.FFnetwork.forward", "modulename": "NDNT.networks", "qualname": "FFnetwork.forward", "kind": "function", "doc": "<p>Forward pass through the network.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>inputs (list, torch.Tensor):</strong>  The input to the network.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>x (torch.Tensor): The output of the network.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">inputs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.networks.FFnetwork.prepare_regularization", "modulename": "NDNT.networks", "qualname": "FFnetwork.prepare_regularization", "kind": "function", "doc": "<p>Makes regularization modules with current requested values.\nThis is done immediately before training, because it can change during training and tuning.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>device (str, optional):</strong>  The device to use. Defaults to None.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">device</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.networks.FFnetwork.compute_reg_loss", "modulename": "NDNT.networks", "qualname": "FFnetwork.compute_reg_loss", "kind": "function", "doc": "<p>Computes the regularization loss by summing reg_loss across layers.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li>None</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>rloss (torch.Tensor): The regularization loss.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.networks.FFnetwork.list_parameters", "modulename": "NDNT.networks", "qualname": "FFnetwork.list_parameters", "kind": "function", "doc": "<p>Lists the (fittable) parameters of the network, calling through each layer</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>layer_target (int, optional):</strong>  The layer to list the parameters for. Defaults to None.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n\n<h6 id=\"raises\">Raises:</h6>\n\n<ul>\n<li><strong>AssertionError:</strong>  If the layer target is invalid.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">layer_target</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.networks.FFnetwork.set_parameters", "modulename": "NDNT.networks", "qualname": "FFnetwork.set_parameters", "kind": "function", "doc": "<p>Sets the parameters as either fittable or not (depending on 'val' for the listed \nlayer, with the default being all layers.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>layer_target (int, optional):</strong>  The layer to set the parameters for. Defaults to None.</li>\n<li><strong>name (str):</strong>  The name of the parameter: default is all parameters.</li>\n<li><strong>val (bool):</strong>  Whether or not to fit (True) or not fit (False)</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n\n<h6 id=\"raises\">Raises:</h6>\n\n<ul>\n<li><strong>AssertionError:</strong>  If the layer target is invalid.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">layer_target</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">val</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.networks.FFnetwork.set_reg_val", "modulename": "NDNT.networks", "qualname": "FFnetwork.set_reg_val", "kind": "function", "doc": "<p>Set reg_values for listed layer or for all layers.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>reg_type (str):</strong>  The type of regularization to set.</li>\n<li><strong>reg_val (float):</strong>  The value to set the regularization to.</li>\n<li><strong>layer_target (int, optional):</strong>  The layer to set the regularization for. Defaults to None.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n\n<h6 id=\"raises\">Raises:</h6>\n\n<ul>\n<li><strong>AssertionError:</strong>  If the layer target is invalid.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">reg_type</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">reg_val</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">layer_target</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.networks.FFnetwork.plot_filters", "modulename": "NDNT.networks", "qualname": "FFnetwork.plot_filters", "kind": "function", "doc": "<p>Plots the filters for the listed layer, passed down to layer's call</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>layer_target (int, optional):</strong>  The layer to plot the filters for. Defaults to 0.</li>\n<li><strong>**kwargs:</strong>  Additional keyword arguments.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">layer_target</span><span class=\"o\">=</span><span class=\"mi\">0</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.networks.FFnetwork.get_weights", "modulename": "NDNT.networks", "qualname": "FFnetwork.get_weights", "kind": "function", "doc": "<p>Passed down to layer call, with optional arguments conveyed.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>layer_target (int):</strong>  The layer to get the weights for.</li>\n<li><strong>**kwargs:</strong>  Additional keyword arguments.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The weights for the specified layer.</p>\n</blockquote>\n\n<h6 id=\"raises\">Raises:</h6>\n\n<ul>\n<li><strong>AssertionError:</strong>  If the layer target is invalid.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">layer_target</span><span class=\"o\">=</span><span class=\"mi\">0</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.networks.FFnetwork.get_biases", "modulename": "NDNT.networks", "qualname": "FFnetwork.get_biases", "kind": "function", "doc": "<p>Passed down to layer call, with optional arguments conveyed.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>layer_target (int):</strong>  The layer to get the weights for.</li>\n<li><strong>**kwargs:</strong>  Additional keyword arguments.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The biases for the specified layer.</p>\n</blockquote>\n\n<h6 id=\"raises\">Raises:</h6>\n\n<ul>\n<li><strong>AssertionError:</strong>  If the layer target is invalid.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">layer_target</span><span class=\"o\">=</span><span class=\"mi\">0</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.networks.FFnetwork.get_network_info", "modulename": "NDNT.networks", "qualname": "FFnetwork.get_network_info", "kind": "function", "doc": "<p>Prints out a description of the network structure.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">abbrev</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.networks.FFnetwork.info", "modulename": "NDNT.networks", "qualname": "FFnetwork.info", "kind": "function", "doc": "<p>This outputs the network information in abbrev (default) or expanded format, including\ninformation from layers</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">ffnet_n</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">expand</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.networks.FFnetwork.generate_info_string", "modulename": "NDNT.networks", "qualname": "FFnetwork.generate_info_string", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.networks.FFnetwork.ffnet_dict", "modulename": "NDNT.networks", "qualname": "FFnetwork.ffnet_dict", "kind": "function", "doc": "<p>Returns a dictionary of the feedforward network.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>layer_list (list):</strong>  A list of dictionaries representing the layers of the network.</li>\n<li><strong>xstim_n (str):</strong>  The name of the stimulus input.</li>\n<li><strong>ffnet_n (list):</strong>  A list of feedforward networks.</li>\n<li><strong>ffnet_type (str):</strong>  The type of the feedforward network.</li>\n<li><strong>scaffold_levels (list):</strong>  A list of scaffold levels.</li>\n<li><strong>num_lags_out (int):</strong>  The number of lags out.</li>\n<li><strong>**kwargs:</strong>  Additional keyword arguments.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>ffnet_dict (dict): The dictionary of the feedforward network.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">layer_list</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">xstim_n</span><span class=\"o\">=</span><span class=\"s1\">&#39;stim&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">ffnet_n</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">ffnet_type</span><span class=\"o\">=</span><span class=\"s1\">&#39;normal&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">scaffold_levels</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_lags_out</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.networks.ScaffoldNetwork", "modulename": "NDNT.networks", "qualname": "ScaffoldNetwork", "kind": "class", "doc": "<p>Concatenates output of all layers together in filter dimension, preserving spatial dims.</p>\n\n<p>This essentially used the constructor for Point1DGaussian, with dicationary input.\nCurrently there is no extra code required at the network level. I think the constructor\ncan be left off entirely, but leaving in in case want to add something.</p>\n", "bases": "FFnetwork"}, {"fullname": "NDNT.networks.ScaffoldNetwork.__init__", "modulename": "NDNT.networks", "qualname": "ScaffoldNetwork.__init__", "kind": "function", "doc": "<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>scaffold_levels (list):</strong>  A list of scaffold levels.</li>\n<li><strong>num_lags_out (int):</strong>  The number of lags out.</li>\n</ul>\n\n<h6 id=\"raises\">Raises:</h6>\n\n<ul>\n<li><strong>AssertionError:</strong>  If the scaffold levels are invalid.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">scaffold_levels</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">num_lags_out</span><span class=\"o\">=</span><span class=\"mi\">1</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.networks.ScaffoldNetwork.network_type", "modulename": "NDNT.networks", "qualname": "ScaffoldNetwork.network_type", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.networks.ScaffoldNetwork.num_lags_out", "modulename": "NDNT.networks", "qualname": "ScaffoldNetwork.num_lags_out", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.networks.ScaffoldNetwork.spatial_dims", "modulename": "NDNT.networks", "qualname": "ScaffoldNetwork.spatial_dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.networks.ScaffoldNetwork.filter_count", "modulename": "NDNT.networks", "qualname": "ScaffoldNetwork.filter_count", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.networks.ScaffoldNetwork.forward", "modulename": "NDNT.networks", "qualname": "ScaffoldNetwork.forward", "kind": "function", "doc": "<p>Forward pass through the network: passes input sequentially through layers\nand concatenates the based on the self.scaffold_levels argument. Note that if\nthere are lags, it will either chomp to the last, or keep number specified\nby self.num_lags_out</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>inputs (list, torch.Tensor):</strong>  The input to the network.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>x (torch.Tensor): The output of the network.</p>\n</blockquote>\n\n<h6 id=\"raises\">Raises:</h6>\n\n<ul>\n<li><strong>ValueError:</strong>  If no layers are defined.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">inputs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.networks.ScaffoldNetwork.generate_info_string", "modulename": "NDNT.networks", "qualname": "ScaffoldNetwork.generate_info_string", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.networks.ScaffoldNetwork.ffnet_dict", "modulename": "NDNT.networks", "qualname": "ScaffoldNetwork.ffnet_dict", "kind": "function", "doc": "<p>Returns a dictionary of the scaffold network.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>scaffold_levels (list):</strong>  A list of scaffold levels.</li>\n<li><strong>num_lags_out (int):</strong>  The number of lags out.</li>\n<li><strong>**kwargs:</strong>  Additional keyword arguments.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>ffnet_dict (dict): The dictionary of the scaffold network.</p>\n</blockquote>\n\n<h6 id=\"raises\">Raises:</h6>\n\n<ul>\n<li><strong>AssertionError:</strong>  If the scaffold levels are invalid.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">scaffold_levels</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">num_lags_out</span><span class=\"o\">=</span><span class=\"mi\">1</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.networks.ScaffoldNetwork3D", "modulename": "NDNT.networks", "qualname": "ScaffoldNetwork3D", "kind": "class", "doc": "<p>Like scaffold network above, but preserves the third dimension so in order\nto have shaped filters designed to process in subsequent network components.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>num_lags_out (int):</strong>  The number of lags out.</li>\n</ul>\n\n<h6 id=\"raises\">Raises:</h6>\n\n<ul>\n<li><strong>AssertionError:</strong>  If the scaffold levels are invalid.</li>\n</ul>\n", "bases": "ScaffoldNetwork"}, {"fullname": "NDNT.networks.ScaffoldNetwork3D.__init__", "modulename": "NDNT.networks", "qualname": "ScaffoldNetwork3D.__init__", "kind": "function", "doc": "<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>scaffold_levels (list):</strong>  A list of scaffold levels.</li>\n<li><strong>num_lags_out (int):</strong>  The number of lags out.</li>\n</ul>\n\n<h6 id=\"raises\">Raises:</h6>\n\n<ul>\n<li><strong>AssertionError:</strong>  If the scaffold levels are invalid.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">layer_list</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">num_lags_out</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.networks.ScaffoldNetwork3D.network_type", "modulename": "NDNT.networks", "qualname": "ScaffoldNetwork3D.network_type", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.networks.ScaffoldNetwork3D.num_lags_out", "modulename": "NDNT.networks", "qualname": "ScaffoldNetwork3D.num_lags_out", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.networks.ScaffoldNetwork3D.forward", "modulename": "NDNT.networks", "qualname": "ScaffoldNetwork3D.forward", "kind": "function", "doc": "<p>Forward pass through the network.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>inputs (list, torch.Tensor):</strong>  The input to the network.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>x (torch.Tensor): The output of the network.</p>\n</blockquote>\n\n<h6 id=\"raises\">Raises:</h6>\n\n<ul>\n<li><strong>ValueError:</strong>  If no layers are defined.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">inputs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.networks.ScaffoldNetwork3D.ffnet_dict", "modulename": "NDNT.networks", "qualname": "ScaffoldNetwork3D.ffnet_dict", "kind": "function", "doc": "<p>Returns a dictionary of the scaffold network.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>**kwargs:</strong>  Additional keyword arguments.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>ffnet_dict (dict): The dictionary of the scaffold network.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.networks.ReadoutNetwork", "modulename": "NDNT.networks", "qualname": "ReadoutNetwork", "kind": "class", "doc": "<p>A readout using a spatial transformer layer whose positions are sampled from one Gaussian per neuron. Mean\nand covariance of that Gaussian are learned.</p>\n", "bases": "FFnetwork"}, {"fullname": "NDNT.networks.ReadoutNetwork.__init__", "modulename": "NDNT.networks", "qualname": "ReadoutNetwork.__init__", "kind": "function", "doc": "<p>Same as contructor for regular network, with extra argument to say if there is a shifter coming in. \nIf there is a shifter, it will interpret (in the forward) the last element routing towards the shifter</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">shifter</span><span class=\"o\">=</span><span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.networks.ReadoutNetwork.network_type", "modulename": "NDNT.networks", "qualname": "ReadoutNetwork.network_type", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.networks.ReadoutNetwork.shifter", "modulename": "NDNT.networks", "qualname": "ReadoutNetwork.shifter", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.networks.ReadoutNetwork.determine_input_dims", "modulename": "NDNT.networks", "qualname": "ReadoutNetwork.determine_input_dims", "kind": "function", "doc": "<p>Sets input_dims given network inputs. Can be overloaded depending on the network type. For this base class, there\nare two types of network input: external stimulus (xstim_n) or a list of internal (ffnet_in) networks:\n    For external inputs, it just uses the passed-in input_dims\n    For internal network inputs, it will concatenate inputs along the filter dimension, but MUST match other dims\nAs currently designed, this can either external or internal, but not both</p>\n\n<h6 id=\"this-sets-the-following-internal-ffnetwork-properties\">This sets the following internal FFnetwork properties:</h6>\n\n<blockquote>\n  <p>self.input_dims\n  self.input_dims_list</p>\n</blockquote>\n\n<p>and returns Boolean whether the passed in input dims are valid</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims_list (list):</strong>  A list of input dimensions for each layer.</li>\n<li><strong>**kwargs:</strong>  Additional keyword arguments.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>valid_input_dims (bool): Whether the passed in input dims are valid.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">input_dims_list</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.networks.ReadoutNetwork.forward", "modulename": "NDNT.networks", "qualname": "ReadoutNetwork.forward", "kind": "function", "doc": "<p>Network inputs correspond to output of conv layer, and (if it exists), a shifter.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>inputs (list, torch.Tensor):</strong>  The input to the network.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>y (torch.Tensor): The output of the network.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">inputs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.networks.ReadoutNetwork.get_readout_locations", "modulename": "NDNT.networks", "qualname": "ReadoutNetwork.get_readout_locations", "kind": "function", "doc": "<p>Returns the positions in the readout layer (within this network)</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li>None</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The readout locations</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.networks.ReadoutNetwork.set_readout_locations", "modulename": "NDNT.networks", "qualname": "ReadoutNetwork.set_readout_locations", "kind": "function", "doc": "<p>Sets the readout locations</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>locs:</strong>  the readout locations</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">locs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.networks.ReadoutNetwork.ffnet_dict", "modulename": "NDNT.networks", "qualname": "ReadoutNetwork.ffnet_dict", "kind": "function", "doc": "<p>Returns a dictionary of the readout network.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>ffnet_n (int):</strong>  The feedforward network.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>ffnet_dict (dict): The dictionary of the readout network.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">ffnet_n</span><span class=\"o\">=</span><span class=\"mi\">0</span>, </span><span class=\"param\"><span class=\"n\">shifter</span><span class=\"o\">=</span><span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.networks.FFnet_external", "modulename": "NDNT.networks", "qualname": "FFnet_external", "kind": "class", "doc": "<p>This is a 'shell' that lets an external network be plugged into the NDN. It establishes all the basics\nso that information requested to this network from other parts of the NDN will behave correctly.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>external_module_dict (dict):</strong>  A dictionary of external modules.</li>\n<li><strong>external_module_name (str):</strong>  The name of the external module.</li>\n<li><strong>input_dims_reshape (list):</strong>  A list of input dimensions to reshape.</li>\n<li><strong>**kwargs:</strong>  Additional keyword arguments.</li>\n</ul>\n\n<h6 id=\"raises\">Raises:</h6>\n\n<ul>\n<li><strong>AssertionError:</strong>  If the external module dictionary is invalid.</li>\n</ul>\n", "bases": "FFnetwork"}, {"fullname": "NDNT.networks.FFnet_external.__init__", "modulename": "NDNT.networks", "qualname": "FFnet_external.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">external_module_dict</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">external_module_name</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">input_dims_reshape</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.networks.FFnet_external.network_type", "modulename": "NDNT.networks", "qualname": "FFnet_external.network_type", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.networks.FFnet_external.input_dims_reshape", "modulename": "NDNT.networks", "qualname": "FFnet_external.input_dims_reshape", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.networks.FFnet_external.forward", "modulename": "NDNT.networks", "qualname": "FFnet_external.forward", "kind": "function", "doc": "<p>Forward pass through the network.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>inputs (list, torch.Tensor):</strong>  The input to the network.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>y (torch.Tensor): The output of the network.</p>\n</blockquote>\n\n<h6 id=\"raises\">Raises:</h6>\n\n<ul>\n<li><strong>ValueError:</strong>  If no layers are defined.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">inputs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.networks.FFnet_external.compute_reg_loss", "modulename": "NDNT.networks", "qualname": "FFnet_external.compute_reg_loss", "kind": "function", "doc": "<p>Computes the regularization loss.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li>None</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>0</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.networks.FFnet_external.list_params", "modulename": "NDNT.networks", "qualname": "FFnet_external.list_params", "kind": "function", "doc": "<p>Lists the parameters for the network.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>layer_target (int, optional):</strong>  The layer to list the parameters for. Defaults to None.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n\n<h6 id=\"raises\">Raises:</h6>\n\n<ul>\n<li><strong>AssertionError:</strong>  If the layer target is invalid.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">layer_target</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.networks.FFnet_external.set_params", "modulename": "NDNT.networks", "qualname": "FFnet_external.set_params", "kind": "function", "doc": "<p>Sets the parameters for the listed layer or for all layers.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>layer_target (int, optional):</strong>  The layer to set the parameters for. Defaults to None.</li>\n<li><strong>name (str):</strong>  The name of the parameter.</li>\n<li><strong>val (bool):</strong>  The value to set the parameter to.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n\n<h6 id=\"raises\">Raises:</h6>\n\n<ul>\n<li><strong>AssertionError:</strong>  If the layer target is invalid.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">layer_target</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">val</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.networks.FFnet_external.ffnet_dict", "modulename": "NDNT.networks", "qualname": "FFnet_external.ffnet_dict", "kind": "function", "doc": "<p>Returns a dictionary of the external network.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>**kwargs:</strong>  Additional keyword arguments.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>ffnet_dict (dict): The dictionary of the external network.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.networks.ScaffoldNetwork3d", "modulename": "NDNT.networks", "qualname": "ScaffoldNetwork3d", "kind": "class", "doc": "<p>Placeholder so old models are not lonely</p>\n", "bases": "ScaffoldNetwork3D"}, {"fullname": "NDNT.networks.ScaffoldNetwork3d.__init__", "modulename": "NDNT.networks", "qualname": "ScaffoldNetwork3d.__init__", "kind": "function", "doc": "<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>scaffold_levels (list):</strong>  A list of scaffold levels.</li>\n<li><strong>num_lags_out (int):</strong>  The number of lags out.</li>\n</ul>\n\n<h6 id=\"raises\">Raises:</h6>\n\n<ul>\n<li><strong>AssertionError:</strong>  If the scaffold levels are invalid.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">layer_list</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">num_lags_out</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.training", "modulename": "NDNT.training", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.ada_hessian", "modulename": "NDNT.training.ada_hessian", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.ada_hessian.AdaHessian", "modulename": "NDNT.training.ada_hessian", "qualname": "AdaHessian", "kind": "class", "doc": "<p>Implements the AdaHessian algorithm from \"ADAHESSIAN: An Adaptive Second OrderOptimizer for Machine Learning\".</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>params (iterable):</strong>  iterable of parameters to optimize or dicts defining parameter groups</li>\n<li><strong>lr (float, optional):</strong>  learning rate (default: 0.1)</li>\n<li><strong>betas ((float, float), optional):</strong>  coefficients used for computing running averages of gradient and the squared hessian trace (default: (0.9, 0.999))</li>\n<li><strong>eps (float, optional):</strong>  term added to the denominator to improve numerical stability (default: 1e-8)</li>\n<li><strong>weight_decay (float, optional):</strong>  weight decay (L2 penalty) (default: 0.0)</li>\n<li><strong>hessian_power (float, optional):</strong>  exponent of the hessian trace (default: 1.0)</li>\n<li><strong>update_each (int, optional):</strong>  compute the hessian trace approximation only after <em>this</em> number of steps (to save time) (default: 1)</li>\n<li><strong>n_samples (int, optional):</strong>  how many times to sample <code>z</code> for the approximation of the hessian trace (default: 1)</li>\n</ul>\n", "bases": "torch.optim.optimizer.Optimizer"}, {"fullname": "NDNT.training.ada_hessian.AdaHessian.__init__", "modulename": "NDNT.training.ada_hessian", "qualname": "AdaHessian.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">params</span>,</span><span class=\"param\">\t<span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.1</span>,</span><span class=\"param\">\t<span class=\"n\">betas</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"mf\">0.999</span><span class=\"p\">)</span>,</span><span class=\"param\">\t<span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mf\">1e-08</span>,</span><span class=\"param\">\t<span class=\"n\">weight_decay</span><span class=\"o\">=</span><span class=\"mf\">0.0</span>,</span><span class=\"param\">\t<span class=\"n\">hessian_power</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">update_each</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">n_samples</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">average_conv_kernel</span><span class=\"o\">=</span><span class=\"kc\">False</span></span>)</span>"}, {"fullname": "NDNT.training.ada_hessian.AdaHessian.n_samples", "modulename": "NDNT.training.ada_hessian", "qualname": "AdaHessian.n_samples", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.ada_hessian.AdaHessian.update_each", "modulename": "NDNT.training.ada_hessian", "qualname": "AdaHessian.update_each", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.ada_hessian.AdaHessian.average_conv_kernel", "modulename": "NDNT.training.ada_hessian", "qualname": "AdaHessian.average_conv_kernel", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.ada_hessian.AdaHessian.generator", "modulename": "NDNT.training.ada_hessian", "qualname": "AdaHessian.generator", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.ada_hessian.AdaHessian.get_params", "modulename": "NDNT.training.ada_hessian", "qualname": "AdaHessian.get_params", "kind": "function", "doc": "<p>Gets all parameters in all param_groups with gradients.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>generator: a generator that produces all parameters with gradients</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.training.ada_hessian.AdaHessian.zero_hessian", "modulename": "NDNT.training.ada_hessian", "qualname": "AdaHessian.zero_hessian", "kind": "function", "doc": "<p>Zeros out the accumalated hessian traces.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.training.ada_hessian.AdaHessian.set_hessian", "modulename": "NDNT.training.ada_hessian", "qualname": "AdaHessian.set_hessian", "kind": "function", "doc": "<p>Computes the Hutchinson approximation of the hessian trace and accumulates it for each trainable parameter.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.training.ada_hessian.AdaHessian.step", "modulename": "NDNT.training.ada_hessian", "qualname": "AdaHessian.step", "kind": "function", "doc": "<p>Performs a single optimization step.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>closure (callable, optional):</strong>  a closure that reevaluates the model and returns the loss (default: None)</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">closure</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.training.dual_trainer", "modulename": "NDNT.training.dual_trainer", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.dual_trainer.DualTrainer", "modulename": "NDNT.training.dual_trainer", "qualname": "DualTrainer", "kind": "class", "doc": "<p>This is the most basic trainer. There are fancier things we could add (hooks, callbacks, etc.), but I don't understand them well enough yet.</p>\n", "bases": "NDNT.training.trainer.Trainer"}, {"fullname": "NDNT.training.dual_trainer.DualTrainer.__init__", "modulename": "NDNT.training.dual_trainer", "qualname": "DualTrainer.__init__", "kind": "function", "doc": "<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model (nn.Module):</strong>  Pytorch Model. Needs training_step and validation_step defined.</li>\n<li><strong>optimizer (torch.optim):</strong>  Pytorch optimizer.</li>\n<li><strong>device (torch.device):</strong>  Device to train on\nDefault: will use CUDA if available</li>\n<li><strong>scheduler (torch.scheduler):</strong>  learning rate scheduler\nDefault: None</li>\n<li><strong>dirpath (str):</strong>  Path to save checkpoints\nDefault: current directory</li>\n<li><strong>multi_gpu (bool):</strong>  Whether to use multiple GPUs\nDefault: False</li>\n<li><strong>max_epochs (int):</strong>  Maximum number of epochs to train\nDefault: 100</li>\n<li><strong>early_stopping (EarlyStopping):</strong>  If not None, will use this as the early stopping callback.\nDefault: None</li>\n<li><strong>optimize_graph (bool):</strong>  Whether to optimize graph before training</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">optimizer2</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.training.dual_trainer.DualTrainer.epoch_optimizer", "modulename": "NDNT.training.dual_trainer", "qualname": "DualTrainer.epoch_optimizer", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.dual_trainer.DualTrainer.prepare_fit", "modulename": "NDNT.training.dual_trainer", "qualname": "DualTrainer.prepare_fit", "kind": "function", "doc": "<p>This is called before fit_loop, and is used to set up the model and optimizer(s).</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model (nn.Module):</strong>  Pytorch Model. Needs training_step and validation_step defined.</li>\n<li><strong>seed (int):</strong>  Random seed for reproducibility.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>model (nn.Module): Pytorch Model. Needs training_step and validation_step defined.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">model</span>, </span><span class=\"param\"><span class=\"n\">seed</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.training.dual_trainer.DualTrainer.train_one_epoch", "modulename": "NDNT.training.dual_trainer", "qualname": "DualTrainer.train_one_epoch", "kind": "function", "doc": "<p>Train for one epoch.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model (nn.Module):</strong>  Pytorch Model. Needs training_step and validation_step defined.</li>\n<li><strong>train_loader (DataLoader):</strong>  Training data.</li>\n<li><strong>epoch (int):</strong>  Current epoch.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>dict: training loss</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">model</span>, </span><span class=\"param\"><span class=\"n\">train_loader</span>, </span><span class=\"param\"><span class=\"n\">epoch</span><span class=\"o\">=</span><span class=\"mi\">0</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.training.dual_trainer.DualTrainer.train_one_step", "modulename": "NDNT.training.dual_trainer", "qualname": "DualTrainer.train_one_step", "kind": "function", "doc": "<p>Train for one step.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model (nn.Module):</strong>  Pytorch Model. Needs training_step and validation_step defined.</li>\n<li><strong>data (dict):</strong>  Training data.</li>\n<li><strong>batch_idx (int):</strong>  Current batch index.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>dict: training loss</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">model</span>, </span><span class=\"param\"><span class=\"n\">par_group1</span>, </span><span class=\"param\"><span class=\"n\">par_group2</span>, </span><span class=\"param\"><span class=\"n\">data</span>, </span><span class=\"param\"><span class=\"n\">batch_idx</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.training.dual_trainer.DualTrainer.checkpoint_model", "modulename": "NDNT.training.dual_trainer", "qualname": "DualTrainer.checkpoint_model", "kind": "function", "doc": "<p>Checkpoint the model.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model (nn.Module):</strong>  Pytorch Model. Needs training_step and validation_step defined.</li>\n<li><strong>epoch (int):</strong>  Current epoch.</li>\n<li><strong>is_best (bool):</strong>  Whether this is the best model.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">model</span>, </span><span class=\"param\"><span class=\"n\">epoch</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">is_best</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.training.dual_trainer.DualTrainer.graceful_exit", "modulename": "NDNT.training.dual_trainer", "qualname": "DualTrainer.graceful_exit", "kind": "function", "doc": "<p>Graceful exit. This is called at the end of training or it if gets interrupted.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model (nn.Module):</strong>  Pytorch Model. Needs training_step and validation_step defined.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">model</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.training.earlystopping", "modulename": "NDNT.training.earlystopping", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.earlystopping.EarlyStopping", "modulename": "NDNT.training.earlystopping", "qualname": "EarlyStopping", "kind": "class", "doc": "<p>Pytorch module that gets plugged into the trainer by NDN in order to early stops the training \nif validation loss doesn't improve after a given patience.</p>\n"}, {"fullname": "NDNT.training.earlystopping.EarlyStopping.__init__", "modulename": "NDNT.training.earlystopping", "qualname": "EarlyStopping.__init__", "kind": "function", "doc": "<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>patience (int):</strong>  How long to wait after last time validation loss improved. (Default: 7)</li>\n<li><strong>verbose (int):</strong>  If &gt;2, prints a message for each validation loss improvement. (Default: 0)</li>\n<li><strong>delta (float):</strong>  Minimum change in the monitored quantity to qualify as an improvement. (Default: 0)</li>\n<li><strong>trace_func (function):</strong>  trace print function. (Default: print)</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">patience</span><span class=\"o\">=</span><span class=\"mi\">7</span>, </span><span class=\"param\"><span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"mi\">0</span>, </span><span class=\"param\"><span class=\"n\">delta</span><span class=\"o\">=</span><span class=\"mi\">0</span>, </span><span class=\"param\"><span class=\"n\">trace_func</span><span class=\"o\">=&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">print</span><span class=\"o\">&gt;</span></span>)</span>"}, {"fullname": "NDNT.training.earlystopping.EarlyStopping.patience", "modulename": "NDNT.training.earlystopping", "qualname": "EarlyStopping.patience", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.earlystopping.EarlyStopping.verbose", "modulename": "NDNT.training.earlystopping", "qualname": "EarlyStopping.verbose", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.earlystopping.EarlyStopping.counter", "modulename": "NDNT.training.earlystopping", "qualname": "EarlyStopping.counter", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.earlystopping.EarlyStopping.best_score", "modulename": "NDNT.training.earlystopping", "qualname": "EarlyStopping.best_score", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.earlystopping.EarlyStopping.early_stop", "modulename": "NDNT.training.earlystopping", "qualname": "EarlyStopping.early_stop", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.earlystopping.EarlyStopping.val_loss_min", "modulename": "NDNT.training.earlystopping", "qualname": "EarlyStopping.val_loss_min", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.earlystopping.EarlyStopping.delta", "modulename": "NDNT.training.earlystopping", "qualname": "EarlyStopping.delta", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.earlystopping.EarlyStopping.trace_func", "modulename": "NDNT.training.earlystopping", "qualname": "EarlyStopping.trace_func", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.lbfgs", "modulename": "NDNT.training.lbfgs", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.lbfgs.is_legal", "modulename": "NDNT.training.lbfgs", "qualname": "is_legal", "kind": "function", "doc": "<p>Checks that tensor is not NaN or Inf.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>v (tensor):</strong>  tensor to be checked</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">v</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.training.lbfgs.polyinterp", "modulename": "NDNT.training.lbfgs", "qualname": "polyinterp", "kind": "function", "doc": "<p>Gives the minimizer and minimum of the interpolating polynomial over given points\nbased on function and derivative information. Defaults to bisection if no critical\npoints are valid.\nBased on polyinterp.m Matlab function in minFunc by Mark Schmidt with some slight\nmodifications.\nImplemented by: Hao-Jun Michael Shi and Dheevatsa Mudigere\nLast edited 12/6/18.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>points (nparray):</strong>  two-dimensional array with each point of form [x f g]</li>\n<li><strong>x_min_bound (float):</strong>  minimum value that brackets minimum (default: minimum of points)</li>\n<li><strong>x_max_bound (float):</strong>  maximum value that brackets minimum (default: maximum of points)</li>\n<li><strong>plot (bool):</strong>  plot interpolating polynomial</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>x_sol (float): minimizer of interpolating polynomial\n  F_min (float): minimum of interpolating polynomial</p>\n</blockquote>\n\n<h6 id=\"note\">Note:</h6>\n\n<blockquote>\n  <p>Set f or g to np.nan if they are unknown</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">points</span>, </span><span class=\"param\"><span class=\"n\">x_min_bound</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">x_max_bound</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">plot</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.training.lbfgs.LBFGS", "modulename": "NDNT.training.lbfgs", "qualname": "LBFGS", "kind": "class", "doc": "<p>Implements the L-BFGS algorithm. Compatible with multi-batch and full-overlap\nL-BFGS implementations and (stochastic) Powell damping. Partly based on the \noriginal L-BFGS implementation in PyTorch, Mark Schmidt's minFunc MATLAB code, \nand Michael Overton's weak Wolfe line search MATLAB code.\nImplemented by: Hao-Jun Michael Shi and Dheevatsa Mudigere\nLast edited 10/20/20.</p>\n\n<h6 id=\"warnings\">Warnings:</h6>\n\n<blockquote>\n  <p>. Does not support per-parameter options and parameter groups.\n  . All parameters have to be on a single device.</p>\n</blockquote>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>lr (float):</strong>  steplength or learning rate (default: 1)</li>\n<li><strong>history_size (int):</strong>  update history size (default: 10)</li>\n<li><strong>line_search (str):</strong>  designates line search to use (default: 'Wolfe')\nOptions:\n    'None': uses steplength designated in algorithm\n    'Armijo': uses Armijo backtracking line search\n    'Wolfe': uses Armijo-Wolfe bracketing line search</li>\n<li><strong>dtype:</strong>  data type (default: torch.float)</li>\n<li><strong>debug (bool):</strong>  debugging mode</li>\n</ul>\n\n<p>References:\n[1] Berahas, Albert S., Jorge Nocedal, and Martin Tak\u00e1c. \"A Multi-Batch L-BFGS \n    Method for Machine Learning.\" Advances in Neural Information Processing \n    Systems. 2016.\n[2] Bollapragada, Raghu, et al. \"A Progressive Batching L-BFGS Method for Machine \n    Learning.\" International Conference on Machine Learning. 2018.\n[3] Lewis, Adrian S., and Michael L. Overton. \"Nonsmooth Optimization via Quasi-Newton\n    Methods.\" Mathematical Programming 141.1-2 (2013): 135-163.\n[4] Liu, Dong C., and Jorge Nocedal. \"On the Limited Memory BFGS Method for \n    Large Scale Optimization.\" Mathematical Programming 45.1-3 (1989): 503-528.\n[5] Nocedal, Jorge. \"Updating Quasi-Newton Matrices With Limited Storage.\" \n    Mathematics of Computation 35.151 (1980): 773-782.\n[6] Nocedal, Jorge, and Stephen J. Wright. \"Numerical Optimization.\" Springer New York,\n    2006.\n[7] Schmidt, Mark. \"minFunc: Unconstrained Differentiable Multivariate Optimization \n    in Matlab.\" Software available at <a href=\"http://www.cs.ubc.ca/~schmidtm/Software/minFunc.html\">http://www.cs.ubc.ca/~schmidtm/Software/minFunc.html</a> \n    (2005).\n[8] Schraudolph, Nicol N., Jin Yu, and Simon G\u00fcnter. \"A Stochastic Quasi-Newton \n    Method for Online Convex Optimization.\" Artificial Intelligence and Statistics. \n    2007.\n[9] Wang, Xiao, et al. \"Stochastic Quasi-Newton Methods for Nonconvex Stochastic \n    Optimization.\" SIAM Journal on Optimization 27.2 (2017): 927-956.</p>\n", "bases": "torch.optim.optimizer.Optimizer"}, {"fullname": "NDNT.training.lbfgs.LBFGS.__init__", "modulename": "NDNT.training.lbfgs", "qualname": "LBFGS.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">params</span>,</span><span class=\"param\">\t<span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">history_size</span><span class=\"o\">=</span><span class=\"mi\">10</span>,</span><span class=\"param\">\t<span class=\"n\">line_search</span><span class=\"o\">=</span><span class=\"s1\">&#39;Wolfe&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span>,</span><span class=\"param\">\t<span class=\"n\">debug</span><span class=\"o\">=</span><span class=\"kc\">False</span></span>)</span>"}, {"fullname": "NDNT.training.lbfgs.LBFGS.line_search", "modulename": "NDNT.training.lbfgs", "qualname": "LBFGS.line_search", "kind": "function", "doc": "<p>Switches line search option.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>line_search (str):</strong>  designates line search to use\nOptions:\n    'None': uses steplength designated in algorithm\n    'Armijo': uses Armijo backtracking line search\n    'Wolfe': uses Armijo-Wolfe bracketing line search</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">line_search</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.training.lbfgs.LBFGS.two_loop_recursion", "modulename": "NDNT.training.lbfgs", "qualname": "LBFGS.two_loop_recursion", "kind": "function", "doc": "<p>Performs two-loop recursion on given vector to obtain Hv.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>vec (tensor):</strong>  1-D tensor to apply two-loop recursion to</li>\n</ul>\n\n<h6 id=\"output\">Output:</h6>\n\n<blockquote>\n  <p>r (tensor): matrix-vector product Hv</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">vec</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.training.lbfgs.LBFGS.curvature_update", "modulename": "NDNT.training.lbfgs", "qualname": "LBFGS.curvature_update", "kind": "function", "doc": "<p>Performs curvature update.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>flat_grad (tensor):</strong>  1-D tensor of flattened gradient for computing \ngradient difference with previously stored gradient</li>\n<li><strong>eps (float):</strong>  constant for curvature pair rejection or damping (default: 1e-2)</li>\n<li><strong>damping (bool):</strong>  flag for using Powell damping (default: False)</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">flat_grad</span>, </span><span class=\"param\"><span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mf\">0.01</span>, </span><span class=\"param\"><span class=\"n\">damping</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.training.lbfgs.LBFGS.step", "modulename": "NDNT.training.lbfgs", "qualname": "LBFGS.step", "kind": "function", "doc": "<p>Performs a single optimization step (parameter update).</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>closure (Callable):</strong>  A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers.</li>\n</ul>\n\n<div class=\"alert note\">\n\n<p>Unless otherwise specified, this function should not modify the\n<code>.grad</code> field of the parameters.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">p_k</span>, </span><span class=\"param\"><span class=\"n\">g_Ok</span>, </span><span class=\"param\"><span class=\"n\">g_Sk</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">options</span><span class=\"o\">=</span><span class=\"p\">{}</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.training.lbfgs.FullBatchLBFGS", "modulename": "NDNT.training.lbfgs", "qualname": "FullBatchLBFGS", "kind": "class", "doc": "<p>Implements full-batch or deterministic L-BFGS algorithm. Compatible with\nPowell damping. Can be used when evaluating a deterministic function and\ngradient. Wraps the LBFGS optimizer. Performs the two-loop recursion,\nupdating, and curvature updating in a single step.\nImplemented by: Hao-Jun Michael Shi and Dheevatsa Mudigere\nLast edited 11/15/18.</p>\n\n<h6 id=\"warnings\">Warnings:</h6>\n\n<blockquote>\n  <p>. Does not support per-parameter options and parameter groups.\n  . All parameters have to be on a single device.</p>\n</blockquote>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>lr (float):</strong>  steplength or learning rate (default: 1)</li>\n<li><strong>history_size (int):</strong>  update history size (default: 10)</li>\n<li><strong>line_search (str):</strong>  designates line search to use (default: 'Wolfe')\nOptions:\n    'None': uses steplength designated in algorithm\n    'Armijo': uses Armijo backtracking line search\n    'Wolfe': uses Armijo-Wolfe bracketing line search</li>\n<li><strong>dtype:</strong>  data type (default: torch.float)</li>\n<li><strong>debug (bool):</strong>  debugging mode</li>\n</ul>\n", "bases": "LBFGS"}, {"fullname": "NDNT.training.lbfgs.FullBatchLBFGS.__init__", "modulename": "NDNT.training.lbfgs", "qualname": "FullBatchLBFGS.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">params</span>,</span><span class=\"param\">\t<span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">history_size</span><span class=\"o\">=</span><span class=\"mi\">10</span>,</span><span class=\"param\">\t<span class=\"n\">line_search</span><span class=\"o\">=</span><span class=\"s1\">&#39;Wolfe&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span>,</span><span class=\"param\">\t<span class=\"n\">debug</span><span class=\"o\">=</span><span class=\"kc\">False</span></span>)</span>"}, {"fullname": "NDNT.training.lbfgs.FullBatchLBFGS.step", "modulename": "NDNT.training.lbfgs", "qualname": "FullBatchLBFGS.step", "kind": "function", "doc": "<p>Performs a single optimization step.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>options (dict):</strong>  contains options for performing line search (default: None)</li>\n</ul>\n\n<h6 id=\"general-options\">General Options:</h6>\n\n<blockquote>\n  <p>'eps' (float): constant for curvature pair rejection or damping (default: 1e-2)\n  'damping' (bool): flag for using Powell damping (default: False)</p>\n</blockquote>\n\n<h6 id=\"options-for-armijo-backtracking-line-search\">Options for Armijo backtracking line search:</h6>\n\n<blockquote>\n  <p>'closure' (callable): reevaluates model and returns function value\n  'current_loss' (tensor): objective value at current iterate (default: F(x_k))\n  'gtd' (tensor): inner product g_Ok'd in line search (default: g_Ok'd)\n  'eta' (tensor): factor for decreasing steplength &gt; 0 (default: 2)\n  'c1' (tensor): sufficient decrease constant in (0, 1) (default: 1e-4)\n  'max_ls' (int): maximum number of line search steps permitted (default: 10)\n  'interpolate' (bool): flag for using interpolation (default: True)\n  'inplace' (bool): flag for inplace operations (default: True)\n  'ls_debug' (bool): debugging mode for line search</p>\n</blockquote>\n\n<h6 id=\"options-for-wolfe-line-search\">Options for Wolfe line search:</h6>\n\n<blockquote>\n  <p>'closure' (callable): reevaluates model and returns function value\n  'current_loss' (tensor): objective value at current iterate (default: F(x_k))\n  'gtd' (tensor): inner product g_Ok'd in line search (default: g_Ok'd)\n  'eta' (float): factor for extrapolation (default: 2)\n  'c1' (float): sufficient decrease constant in (0, 1) (default: 1e-4)\n  'c2' (float): curvature condition constant in (0, 1) (default: 0.9)\n  'max_ls' (int): maximum number of line search steps permitted (default: 10)\n  'interpolate' (bool): flag for using interpolation (default: True)\n  'inplace' (bool): flag for inplace operations (default: True)\n  'ls_debug' (bool): debugging mode for line search</p>\n</blockquote>\n\n<p>Outputs (depends on line search):\n  . No line search:\n        t (float): steplength\n  . Armijo backtracking line search:\n        F_new (tensor): loss function at new iterate\n        t (tensor): final steplength\n        ls_step (int): number of backtracks\n        closure_eval (int): number of closure evaluations\n        desc_dir (bool): descent direction flag\n            True: p_k is descent direction with respect to the line search\n            function\n            False: p_k is not a descent direction with respect to the line\n            search function\n        fail (bool): failure flag\n            True: line search reached maximum number of iterations, failed\n            False: line search succeeded\n  . Wolfe line search:\n        F_new (tensor): loss function at new iterate\n        g_new (tensor): gradient at new iterate\n        t (float): final steplength\n        ls_step (int): number of backtracks\n        closure_eval (int): number of closure evaluations\n        grad_eval (int): number of gradient evaluations\n        desc_dir (bool): descent direction flag\n            True: p_k is descent direction with respect to the line search\n            function\n            False: p_k is not a descent direction with respect to the line\n            search function\n        fail (bool): failure flag\n            True: line search reached maximum number of iterations, failed\n            False: line search succeeded</p>\n\n<h6 id=\"notes\">Notes:</h6>\n\n<blockquote>\n  <p>. If encountering line search failure in the deterministic setting, one\n    should try increasing the maximum number of line search steps max_ls.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">options</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.training.lbfgsnew", "modulename": "NDNT.training.lbfgsnew", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.lbfgsnew.be_verbose", "modulename": "NDNT.training.lbfgsnew", "qualname": "be_verbose", "kind": "variable", "doc": "<p></p>\n", "default_value": "False"}, {"fullname": "NDNT.training.lbfgsnew.LBFGSNew", "modulename": "NDNT.training.lbfgsnew", "qualname": "LBFGSNew", "kind": "class", "doc": "<p>Implements L-BFGS algorithm.</p>\n\n<div class=\"alert warning\">\n\n<p>This optimizer doesn't support per-parameter options and parameter\ngroups (there can be only one).</p>\n\n</div>\n\n<div class=\"alert warning\">\n\n<p>Right now all parameters have to be on a single device. This will be\nimproved in the future.</p>\n\n</div>\n\n<div class=\"alert note\">\n\n<p>This is a very memory intensive optimizer (it requires additional\n<code>param_bytes * (history_size + 1)</code> bytes). If it doesn't fit in memory\ntry reducing the history size, or use a different algorithm.</p>\n\n</div>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>lr (float):</strong>  learning rate (fallback value when line search fails. not really needed) (default: 1)</li>\n<li><strong>max_iter (int):</strong>  maximal number of iterations per optimization step\n(default: 10)</li>\n<li><strong>max_eval (int):</strong>  maximal number of function evaluations per optimization\nstep (default: max_iter * 1.25).</li>\n<li><strong>tolerance_grad (float):</strong>  termination tolerance on first order optimality\n(default: 1e-5).</li>\n<li><strong>tolerance_change (float):</strong>  termination tolerance on function\nvalue/parameter changes (default: 1e-9).</li>\n<li><strong>history_size (int):</strong>  update history size (default: 7).</li>\n<li><strong>line_search_fn:</strong>  if True, use cubic interpolation to findstep size, if False: fixed step size</li>\n<li><strong>batch_mode:</strong>  True for stochastic version (default False)</li>\n<li><strong>Example usage for full batch mode:</strong>  optimizer = LBFGSNew(model.parameters(), history_size=7, max_iter=100, line_search_fn=True, batch_mode=False)</li>\n<li><strong>Example usage for batch mode (stochastic):</strong>  optimizer = LBFGSNew(net.parameters(), history_size=7, max_iter=4, line_search_fn=True,batch_mode=True)\nNote: when using a closure(), only do backward() after checking the gradient is available,\nEg: \n  def closure():\n   optimizer.zero_grad()\n   outputs=net(inputs)\n   loss=criterion(outputs,labels)\n   if loss.requires_grad:\n     loss.backward()\n   return loss</li>\n</ul>\n", "bases": "torch.optim.optimizer.Optimizer"}, {"fullname": "NDNT.training.lbfgsnew.LBFGSNew.__init__", "modulename": "NDNT.training.lbfgsnew", "qualname": "LBFGSNew.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">params</span>,</span><span class=\"param\">\t<span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">max_iter</span><span class=\"o\">=</span><span class=\"mi\">10</span>,</span><span class=\"param\">\t<span class=\"n\">max_eval</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">tolerance_grad</span><span class=\"o\">=</span><span class=\"mf\">1e-05</span>,</span><span class=\"param\">\t<span class=\"n\">tolerance_change</span><span class=\"o\">=</span><span class=\"mf\">1e-09</span>,</span><span class=\"param\">\t<span class=\"n\">history_size</span><span class=\"o\">=</span><span class=\"mi\">7</span>,</span><span class=\"param\">\t<span class=\"n\">line_search_fn</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">batch_mode</span><span class=\"o\">=</span><span class=\"kc\">False</span></span>)</span>"}, {"fullname": "NDNT.training.lbfgsnew.LBFGSNew.step", "modulename": "NDNT.training.lbfgsnew", "qualname": "LBFGSNew.step", "kind": "function", "doc": "<p>Performs a single optimization step.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>closure (callable):</strong>  A closure that reevaluates the model\nand returns the loss.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">closure</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.training.trainer", "modulename": "NDNT.training.trainer", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.trainer.Trainer", "modulename": "NDNT.training.trainer", "qualname": "Trainer", "kind": "class", "doc": "<p>This is the most basic trainer. There are fancier things we could add (hooks, callbacks, etc.), but this is bare-bones variable tracking.</p>\n"}, {"fullname": "NDNT.training.trainer.Trainer.__init__", "modulename": "NDNT.training.trainer", "qualname": "Trainer.__init__", "kind": "function", "doc": "<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>optimizer (torch.optim):</strong>  Pytorch optimizer to use</li>\n<li><strong>scheduler (torch.scheduler):</strong>  learning rate scheduler (Default: None)</li>\n<li><strong>device (torch.device):</strong>  Device to train on (Default: None, in which case it will use CUDA if available)</li>\n<li><strong>dirpath (str):</strong>  Path to save checkpoints (Default: current directory)</li>\n<li><strong>version (int):</strong>  Version of model to use (Default: None, in which case will create next version)</li>\n<li><strong>max_epochs (int):</strong>  Maximum number of epochs to train (Default: 100)</li>\n<li><strong>early_stopping (EarlyStopping):</strong>  If not None, will use this as the early stopping callback (Default: None)</li>\n<li><strong>accumulate_grad_batches (int):</strong>  How many batches to accumulate before taking optimizer step (Default: 1)</li>\n<li><strong>verbose:</strong>  degree of feedback to screen (int): 0=None, 1=epoch-level, 2=batch-level, 3=add early stopping info (Default 1)</li>\n<li><strong>save_epochs (bool):</strong>  whether to save checkpointed model at the end of every epoch (Default: False)</li>\n<li><strong>optimize_graph (bool):</strong>  whether to optimize graph before training</li>\n<li><strong>set_grad_to_none (bool):</strong>  option needed for optimizer (Default: False)</li>\n<li><strong>log_activations (bool):</strong>  whether to log activations (Default: False)</li>\n<li><strong>scheduler:</strong>  Currently not used, along with next two (Default: None)</li>\n<li><strong>scheduler_after:</strong>  (Default: 'batch')</li>\n<li><strong>scheduler_metric:</strong>  (Default: None)</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">optimizer</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">dirpath</span><span class=\"o\">=</span><span class=\"s1\">&#39;./checkpoints&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">version</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">max_epochs</span><span class=\"o\">=</span><span class=\"mi\">100</span>,</span><span class=\"param\">\t<span class=\"n\">early_stopping</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">accumulate_grad_batches</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">save_epochs</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">optimize_graph</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">set_grad_to_none</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">log_activations</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">scheduler</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">scheduler_after</span><span class=\"o\">=</span><span class=\"s1\">&#39;batch&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">scheduler_metric</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.training.trainer.Trainer.optimizer", "modulename": "NDNT.training.trainer", "qualname": "Trainer.optimizer", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.trainer.Trainer.scheduler", "modulename": "NDNT.training.trainer", "qualname": "Trainer.scheduler", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.trainer.Trainer.optimize_graph", "modulename": "NDNT.training.trainer", "qualname": "Trainer.optimize_graph", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.trainer.Trainer.log_activations", "modulename": "NDNT.training.trainer", "qualname": "Trainer.log_activations", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.trainer.Trainer.accumulate_grad_batches", "modulename": "NDNT.training.trainer", "qualname": "Trainer.accumulate_grad_batches", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.trainer.Trainer.verbose", "modulename": "NDNT.training.trainer", "qualname": "Trainer.verbose", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.trainer.Trainer.set_to_none", "modulename": "NDNT.training.trainer", "qualname": "Trainer.set_to_none", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.trainer.Trainer.fullbatch", "modulename": "NDNT.training.trainer", "qualname": "Trainer.fullbatch", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.trainer.Trainer.accum_loss", "modulename": "NDNT.training.trainer", "qualname": "Trainer.accum_loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.trainer.Trainer.accum_train_loss", "modulename": "NDNT.training.trainer", "qualname": "Trainer.accum_train_loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.trainer.Trainer.accum_reg", "modulename": "NDNT.training.trainer", "qualname": "Trainer.accum_reg", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.trainer.Trainer.accum_count", "modulename": "NDNT.training.trainer", "qualname": "Trainer.accum_count", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.trainer.Trainer.save_epochs", "modulename": "NDNT.training.trainer", "qualname": "Trainer.save_epochs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.trainer.Trainer.dirpath", "modulename": "NDNT.training.trainer", "qualname": "Trainer.dirpath", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.trainer.Trainer.early_stopping", "modulename": "NDNT.training.trainer", "qualname": "Trainer.early_stopping", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.trainer.Trainer.device", "modulename": "NDNT.training.trainer", "qualname": "Trainer.device", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.trainer.Trainer.logger", "modulename": "NDNT.training.trainer", "qualname": "Trainer.logger", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.trainer.Trainer.version", "modulename": "NDNT.training.trainer", "qualname": "Trainer.version", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.trainer.Trainer.epoch", "modulename": "NDNT.training.trainer", "qualname": "Trainer.epoch", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.trainer.Trainer.max_epochs", "modulename": "NDNT.training.trainer", "qualname": "Trainer.max_epochs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.trainer.Trainer.n_iter", "modulename": "NDNT.training.trainer", "qualname": "Trainer.n_iter", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.trainer.Trainer.val_loss_min", "modulename": "NDNT.training.trainer", "qualname": "Trainer.val_loss_min", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.trainer.Trainer.step_scheduler_after", "modulename": "NDNT.training.trainer", "qualname": "Trainer.step_scheduler_after", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.trainer.Trainer.step_scheduler_metric", "modulename": "NDNT.training.trainer", "qualname": "Trainer.step_scheduler_metric", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.trainer.Trainer.fit", "modulename": "NDNT.training.trainer", "qualname": "Trainer.fit", "kind": "function", "doc": "<p>Fit the model to the data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model (nn.Module):</strong>  Pytorch Model. Needs training_step and validation_step defined.</li>\n<li><strong>train_loader (DataLoader):</strong>  Training data.</li>\n<li><strong>val_loader (DataLoader):</strong>  Validation data.</li>\n<li><strong>seed (int):</strong>  Random seed for reproducibility.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">model</span>, </span><span class=\"param\"><span class=\"n\">train_loader</span>, </span><span class=\"param\"><span class=\"n\">val_loader</span>, </span><span class=\"param\"><span class=\"n\">seed</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.training.trainer.Trainer.prepare_fit", "modulename": "NDNT.training.trainer", "qualname": "Trainer.prepare_fit", "kind": "function", "doc": "<p>This is called before fit_loop, and is used to set up the model and optimizer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model (nn.Module):</strong>  Pytorch Model. Needs training_step and validation_step defined.</li>\n<li><strong>seed (int):</strong>  Random seed for reproducibility.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>model (nn.Module): Pytorch Model. Needs training_step and validation_step defined.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">model</span>, </span><span class=\"param\"><span class=\"n\">seed</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.training.trainer.Trainer.fit_loop", "modulename": "NDNT.training.trainer", "qualname": "Trainer.fit_loop", "kind": "function", "doc": "<p>Main training loop. This is where the model is trained.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model (nn.Module):</strong>  Pytorch Model. Needs training_step and validation_step defined.</li>\n<li><strong>epochs (int):</strong>  Number of epochs to train.</li>\n<li><strong>train_loader (DataLoader):</strong>  Training data.</li>\n<li><strong>val_loader (DataLoader):</strong>  Validation data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">model</span>, </span><span class=\"param\"><span class=\"n\">epochs</span>, </span><span class=\"param\"><span class=\"n\">train_loader</span>, </span><span class=\"param\"><span class=\"n\">val_loader</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.training.trainer.Trainer.validate_one_epoch", "modulename": "NDNT.training.trainer", "qualname": "Trainer.validate_one_epoch", "kind": "function", "doc": "<p>validation step for one epoch</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model (nn.Module):</strong>  Pytorch Model. Needs training_step and validation_step defined.</li>\n<li><strong>val_loader (DataLoader):</strong>  Validation data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>dict: validation loss</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">model</span>, </span><span class=\"param\"><span class=\"n\">val_loader</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.training.trainer.Trainer.train_one_epoch", "modulename": "NDNT.training.trainer", "qualname": "Trainer.train_one_epoch", "kind": "function", "doc": "<p>Train for one epoch.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model (nn.Module):</strong>  Pytorch Model. Needs training_step and validation_step defined.</li>\n<li><strong>train_loader (DataLoader):</strong>  Training data.</li>\n<li><strong>epoch (int):</strong>  Current epoch.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>dict: training loss</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">model</span>, </span><span class=\"param\"><span class=\"n\">train_loader</span>, </span><span class=\"param\"><span class=\"n\">epoch</span><span class=\"o\">=</span><span class=\"mi\">0</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.training.trainer.Trainer.train_one_step", "modulename": "NDNT.training.trainer", "qualname": "Trainer.train_one_step", "kind": "function", "doc": "<p>Train for one step.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model (nn.Module):</strong>  Pytorch Model. Needs training_step and validation_step defined.</li>\n<li><strong>data (dict):</strong>  Training data.</li>\n<li><strong>batch_idx (int):</strong>  Current batch index.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>dict: training loss</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">model</span>, </span><span class=\"param\"><span class=\"n\">data</span>, </span><span class=\"param\"><span class=\"n\">batch_idx</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.training.trainer.Trainer.checkpoint_model", "modulename": "NDNT.training.trainer", "qualname": "Trainer.checkpoint_model", "kind": "function", "doc": "<p>Checkpoint the model.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model (nn.Module):</strong>  Pytorch Model. Needs training_step and validation_step defined.</li>\n<li><strong>epoch (int):</strong>  Current epoch.</li>\n<li><strong>is_best (bool):</strong>  Whether this is the best model.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">model</span>, </span><span class=\"param\"><span class=\"n\">epoch</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">is_best</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.training.trainer.Trainer.graceful_exit", "modulename": "NDNT.training.trainer", "qualname": "Trainer.graceful_exit", "kind": "function", "doc": "<p>Graceful exit. This is called at the end of training or it if gets interrupted.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model (nn.Module):</strong>  Pytorch Model. Needs training_step and validation_step defined.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">model</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.training.trainer.LBFGSTrainer", "modulename": "NDNT.training.trainer", "qualname": "LBFGSTrainer", "kind": "class", "doc": "<p>This class is for training with the LBFGS optimizer. It is a subclass of Trainer.</p>\n", "bases": "Trainer"}, {"fullname": "NDNT.training.trainer.LBFGSTrainer.__init__", "modulename": "NDNT.training.trainer", "qualname": "LBFGSTrainer.__init__", "kind": "function", "doc": "<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>full_batch (bool):</strong>  Whether to use full batch optimization.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">full_batch</span><span class=\"o\">=</span><span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.training.trainer.LBFGSTrainer.fullbatch", "modulename": "NDNT.training.trainer", "qualname": "LBFGSTrainer.fullbatch", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.training.trainer.LBFGSTrainer.fit", "modulename": "NDNT.training.trainer", "qualname": "LBFGSTrainer.fit", "kind": "function", "doc": "<p>Fit the model to the data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model (nn.Module):</strong>  Pytorch Model. Needs training_step and validation_step defined.</li>\n<li><strong>train_loader (DataLoader):</strong>  Training data.</li>\n<li><strong>val_loader (DataLoader):</strong>  Validation data.</li>\n<li><strong>seed (int):</strong>  Random seed for reproducibility.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">model</span>, </span><span class=\"param\"><span class=\"n\">train_loader</span>, </span><span class=\"param\"><span class=\"n\">val_loader</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">seed</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.training.trainer.LBFGSTrainer.fit_data_dict", "modulename": "NDNT.training.trainer", "qualname": "LBFGSTrainer.fit_data_dict", "kind": "function", "doc": "<p>Fit data that is provided in a dictionary.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model (nn.Module):</strong>  Pytorch Model. Needs training_step and validation_step defined.</li>\n<li><strong>train_data (dict):</strong>  Training data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">model</span>, </span><span class=\"param\"><span class=\"n\">train_data</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.training.trainer.LBFGSTrainer.train_one_epoch", "modulename": "NDNT.training.trainer", "qualname": "LBFGSTrainer.train_one_epoch", "kind": "function", "doc": "<p>Train for one epoch.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model (nn.Module):</strong>  Pytorch Model. Needs training_step and validation_step defined.</li>\n<li><strong>train_loader (DataLoader):</strong>  Training data.</li>\n<li><strong>epoch (int):</strong>  Current epoch.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>dict: training loss</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">model</span>, </span><span class=\"param\"><span class=\"n\">train_loader</span>, </span><span class=\"param\"><span class=\"n\">epoch</span><span class=\"o\">=</span><span class=\"mi\">0</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.training.trainer.TemperatureCalibratedTrainer", "modulename": "NDNT.training.trainer", "qualname": "TemperatureCalibratedTrainer", "kind": "class", "doc": "<p>This class is for training with temperature calibration. It is a subclass of Trainer.</p>\n", "bases": "Trainer"}, {"fullname": "NDNT.training.trainer.TemperatureCalibratedTrainer.__init__", "modulename": "NDNT.training.trainer", "qualname": "TemperatureCalibratedTrainer.__init__", "kind": "function", "doc": "<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li>None</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.training.trainer.TemperatureCalibratedTrainer.validate_one_epoch", "modulename": "NDNT.training.trainer", "qualname": "TemperatureCalibratedTrainer.validate_one_epoch", "kind": "function", "doc": "<p>Validation step for one epoch.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model (nn.Module):</strong>  Pytorch Model. Needs training_step and validation_step defined.</li>\n<li><strong>val_loader (DataLoader):</strong>  Validation data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>dict: validation loss</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">model</span>, </span><span class=\"param\"><span class=\"n\">val_loader</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils", "modulename": "NDNT.utils", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.utils.DanUtils", "modulename": "NDNT.utils.DanUtils", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.utils.DanUtils.subplot_setup", "modulename": "NDNT.utils.DanUtils", "qualname": "subplot_setup", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">num_rows</span>, </span><span class=\"param\"><span class=\"n\">num_cols</span>, </span><span class=\"param\"><span class=\"n\">row_height</span><span class=\"o\">=</span><span class=\"mi\">3</span>, </span><span class=\"param\"><span class=\"n\">fig_width</span><span class=\"o\">=</span><span class=\"mi\">16</span>, </span><span class=\"param\"><span class=\"n\">fighandle</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.DanUtils.ss", "modulename": "NDNT.utils.DanUtils", "qualname": "ss", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">num_rows</span><span class=\"o\">=</span><span class=\"mi\">1</span>, </span><span class=\"param\"><span class=\"n\">num_cols</span><span class=\"o\">=</span><span class=\"mi\">1</span>, </span><span class=\"param\"><span class=\"n\">row_height</span><span class=\"o\">=</span><span class=\"mi\">4</span>, </span><span class=\"param\"><span class=\"n\">rh</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">fighandle</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.DanUtils.imagesc", "modulename": "NDNT.utils.DanUtils", "qualname": "imagesc", "kind": "function", "doc": "<p>Modifications of plt.imshow that choose reasonable defaults</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">img</span>,</span><span class=\"param\">\t<span class=\"n\">cmap</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">balanced</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">aspect</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"nb\">max</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">colrow</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">axis_labels</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">ax</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.DanUtils.scatterplot", "modulename": "NDNT.utils.DanUtils", "qualname": "scatterplot", "kind": "function", "doc": "<p>Generates scatter-plot of 2-d data (arr2) using the following options:</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>arr2 (array):</strong>  2-d array to plot (Nx2)</li>\n<li><strong>arrS2 (array):</strong>  second array if arr2 is 1-d</li>\n<li><strong>clr (str):</strong>  symbol to use (default 'b.')</li>\n<li><strong>alpha (float):</strong>  transparency level (default: 1)</li>\n<li><strong>diag (boolean):</strong>  whether to draw x-y diagonal line (default False)</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None, simply display to screen</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">arr2</span>, </span><span class=\"param\"><span class=\"n\">arrS2</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">clr</span><span class=\"o\">=</span><span class=\"s1\">&#39;b.&#39;</span>, </span><span class=\"param\"><span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>, </span><span class=\"param\"><span class=\"n\">diag</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.DanUtils.find_peaks", "modulename": "NDNT.utils.DanUtils", "qualname": "find_peaks", "kind": "function", "doc": "<p>Find maximum of peaks and then get rid of other points around it for plus/minus some amount</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">clearance</span><span class=\"o\">=</span><span class=\"mi\">10</span>, </span><span class=\"param\"><span class=\"n\">max_peaks</span><span class=\"o\">=</span><span class=\"mi\">10</span>, </span><span class=\"param\"><span class=\"n\">thresh</span><span class=\"o\">=</span><span class=\"mf\">13.0</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.DanUtils.regression2d", "modulename": "NDNT.utils.DanUtils", "qualname": "regression2d", "kind": "function", "doc": "<p>2-d regression on data with N examples, where Yobs is predicted by x2s, \nwhere x2s is Nx2 and Yobs is Nx1. Yobs can have multiple columns, with each column predicted\nindependently from x2s. Prediction(s) are y_i:</p>\n\n<p>y = a + b_1 x_1 + b_2 x_2 (where a, b1, b2 and y are i-specific)</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x2s:</strong>  2-d data inputs (Nx2)</li>\n<li><strong>Yobs:</strong>  target output(s): (NxM) will regress each column separately</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>regr_mat: matrix (2xM) for the regression slopes such that x2s@reg_mat + offset\n  regr_off: offsets (Mx0), see above</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x2s</span>, </span><span class=\"param\"><span class=\"n\">Yobs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.DanUtils.chunker", "modulename": "NDNT.utils.DanUtils", "qualname": "chunker", "kind": "function", "doc": "<p>This function chunks a sequence into chunks of size size. (from Matt J)</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>seq:</strong>  the sequence</li>\n<li><strong>size:</strong>  the size of the chunks</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>a list of chunks</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">seq</span>, </span><span class=\"param\"><span class=\"n\">size</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.DanUtils.filename_num2str", "modulename": "NDNT.utils.DanUtils", "qualname": "filename_num2str", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">n</span>, </span><span class=\"param\"><span class=\"n\">num_digits</span><span class=\"o\">=</span><span class=\"mi\">2</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.DanUtils.display_matrix", "modulename": "NDNT.utils.DanUtils", "qualname": "display_matrix", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">prec</span><span class=\"o\">=</span><span class=\"mi\">3</span>, </span><span class=\"param\"><span class=\"n\">spacing</span><span class=\"o\">=</span><span class=\"mi\">4</span>, </span><span class=\"param\"><span class=\"n\">number_rows</span><span class=\"o\">=</span><span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"n\">number_cols</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.DanUtils.figure_export", "modulename": "NDNT.utils.DanUtils", "qualname": "figure_export", "kind": "function", "doc": "<p>Usage: figure_export( fig_handle, filename, variable_list, bitmap=False, dpi=300)\nif bitmap, will use dpi and export as .png. Otherwise will export PDF</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">fig_handle</span>, </span><span class=\"param\"><span class=\"n\">filename</span>, </span><span class=\"param\"><span class=\"n\">bitmap</span><span class=\"o\">=</span><span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"n\">dpi</span><span class=\"o\">=</span><span class=\"mi\">300</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.DanUtils.matlab_export", "modulename": "NDNT.utils.DanUtils", "qualname": "matlab_export", "kind": "function", "doc": "<p>Export list of variables to .mat file</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">filename</span>, </span><span class=\"param\"><span class=\"n\">variable_list</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.DanUtils.save_python_data", "modulename": "NDNT.utils.DanUtils", "qualname": "save_python_data", "kind": "function", "doc": "<p>Saves python dictionary in standard binary file</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>filename:</strong>  name of the file to save</li>\n<li><strong>data:</strong>  dictionary to save</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">filename</span>, </span><span class=\"param\"><span class=\"n\">data</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.DanUtils.load_python_data", "modulename": "NDNT.utils.DanUtils", "qualname": "load_python_data", "kind": "function", "doc": "<p>Load python data from standard binary file</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>filename:</strong>  name of the file to save</li>\n<li><strong>show_keys:</strong>  to display components of dictionary (default=False)</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">filename</span>, </span><span class=\"param\"><span class=\"n\">show_keys</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.DanUtils.clean_jupyter_notebook", "modulename": "NDNT.utils.DanUtils", "qualname": "clean_jupyter_notebook", "kind": "function", "doc": "<p>Take jupyter notebook file (ipynb) and scrubs the output, in case it takes up too much memory or other\nerror running.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>notebook_filename:</strong>  filename of jupyter notebook to be cleaned</li>\n<li><strong>new_filename:</strong>  filename for new notebook</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Nothing, but writes new file to disk</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">notebook_filename</span>, </span><span class=\"param\"><span class=\"n\">new_filename</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.DanUtils.fold_sample", "modulename": "NDNT.utils.DanUtils", "qualname": "fold_sample", "kind": "function", "doc": "<p>Divide fold sample deterministically or randomly distributed over number of items. More options\ncan be added, but his captures the basics.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">num_items</span>, </span><span class=\"param\"><span class=\"n\">folds</span><span class=\"o\">=</span><span class=\"mi\">5</span>, </span><span class=\"param\"><span class=\"n\">random_gen</span><span class=\"o\">=</span><span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"n\">which_fold</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.DanUtils.time_embedding_simple", "modulename": "NDNT.utils.DanUtils", "qualname": "time_embedding_simple", "kind": "function", "doc": "<p>Simple time embedding: takes the stim and time-embeds with nlags\nIf stim is multi-dimensional, it flattens. This is a numpy function</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">stim</span>, </span><span class=\"param\"><span class=\"n\">nlags</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.DanUtils.design_matrix_drift", "modulename": "NDNT.utils.DanUtils", "qualname": "design_matrix_drift", "kind": "function", "doc": "<p>Produce a design matrix based on continuous data (s) and anchor points for a tent_basis.\nHere s is a continuous variable (e.g., a stimulus) that is function of time -- single dimension --\nand this will generate apply a tent basis set to s with a basis variable for each anchor point. \nThe end anchor points will be one-sided, but these can be dropped by changing \"zero_left\" and/or\n\"zero_right\" into \"True\".</p>\n\n<p>Inputs: \n    NT: length of design matrix\n    anchors: list or array of anchor points for tent-basis set\n    zero_left, zero_right: boolean whether to drop the edge bases (default for both is False)</p>\n\n<h6 id=\"outputs\">Outputs:</h6>\n\n<blockquote>\n  <p>X: design matrix that will be NT x the number of anchors left after zeroing out left and right</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">NT</span>,</span><span class=\"param\">\t<span class=\"n\">anchors</span>,</span><span class=\"param\">\t<span class=\"n\">zero_left</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">zero_right</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">const_right</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">to_plot</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.DanUtils.dist_mean", "modulename": "NDNT.utils.DanUtils", "qualname": "dist_mean", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">p</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.DanUtils.max_multiD", "modulename": "NDNT.utils.DanUtils", "qualname": "max_multiD", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">k</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.DanUtils.boxcar_smooth", "modulename": "NDNT.utils.DanUtils", "qualname": "boxcar_smooth", "kind": "function", "doc": "<p>Boxcar smoothing</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">s</span>, </span><span class=\"param\"><span class=\"n\">win_size</span><span class=\"o\">=</span><span class=\"mi\">5</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.DanUtils.median_smoothing", "modulename": "NDNT.utils.DanUtils", "qualname": "median_smoothing", "kind": "function", "doc": "<p>Median smoothing of a 1D signal (or multi-d signal where median smoothing in first dimension), \nusing padding on the ends</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>f:</strong>  input signal</li>\n<li><strong>L:</strong>  window radius (from each time point)</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>mout: smoothed signal</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">f</span>, </span><span class=\"param\"><span class=\"n\">L</span><span class=\"o\">=</span><span class=\"mi\">5</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.DanUtils.iterate_lbfgs", "modulename": "NDNT.utils.DanUtils", "qualname": "iterate_lbfgs", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">mod</span>,</span><span class=\"param\">\t<span class=\"n\">dat</span>,</span><span class=\"param\">\t<span class=\"n\">lbfgs_pars</span>,</span><span class=\"param\">\t<span class=\"n\">train_inds</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">val_inds</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">tol</span><span class=\"o\">=</span><span class=\"mf\">0.0001</span>,</span><span class=\"param\">\t<span class=\"n\">max_iter</span><span class=\"o\">=</span><span class=\"mi\">20</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.DanUtils.binocular_data_import", "modulename": "NDNT.utils.DanUtils", "qualname": "binocular_data_import", "kind": "function", "doc": "<p>Usage: stim, Robs, DFs, used_inds, Eadd_info = binocular_data_import( datadir, expt_num )</p>\n\n<h6 id=\"inputs\">Inputs:</h6>\n\n<blockquote>\n  <p>datadir: directory on local drive where datafiles are\n  expt_num: the experiment number (1-12) representing the binocular experiments we currently have. All\n              datafiles are called 'BS2expt?.mat'. Note that numbered from 1 (not zero)</p>\n</blockquote>\n\n<h6 id=\"outputs\">Outputs:</h6>\n\n<blockquote>\n  <p>stim: formatted as NT x 72 (stimuli in each eye are cropped to NX=36). It will be time-shifted by 1 to \n          eliminate 0-latency stim\n  Robs: response concatenating all SUs and MUs, NT x NC. NSU is saved as part of Eadd_info\n  DFs:  data_filters for experiment, also NT x NC (note MU datafilters are initialized to 1)\n  used_inds: indices overwhich data is valid according to initial data parsing (adjusted to python) \n  Eadd_info: dictionary containing all other relevant info for experiment</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">datadir</span>, </span><span class=\"param\"><span class=\"n\">expt_num</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.DanUtils.binocular_data_import_cell", "modulename": "NDNT.utils.DanUtils", "qualname": "binocular_data_import_cell", "kind": "function", "doc": "<p>Usage: stim, Robs, used_inds_cell, UiC, XiC = binocular_data_import_cell( datadir, expt_num, cell_num)</p>\n\n<p>Imports data for one cell: otherwise identitical to binocular_data_import. Takes data_filter for the \ncell into account so will not need datafilter. Also, all indices will be adjusted accordingly.</p>\n\n<h6 id=\"inputs\">Inputs:</h6>\n\n<blockquote>\n  <p>datadir: directory on local drive where datafiles are\n  expt_num: the experiment number (1-12) representing the binocular experiments we currently have. All\n              datafiles are called 'BS2expt?.mat'. Note that numbered from 1 (not zero)\n  cell_num: cell number to analyze</p>\n</blockquote>\n\n<h6 id=\"outputs\">Outputs:</h6>\n\n<blockquote>\n  <p>stim_all: formatted as NT x 72 (stimuli in each eye are cropped to NX=36). It will be time-shifted by 1 \n            to eliminate 0-latency stim. Note this is all stim in experiment, val_inds from used_inds...\n  Robs: response concatenating all SUs and MUs, NT x NC. NSU and full Robs is saved as part of Eadd_info.\n        This is already selected by used_inds_cell, so no need for further reduction\n  used_inds_cell: indices overwhich data is valid for that particular cell. Should be applied to stim only\n  UiC, XiC: cross-validation indices for that cell, based on used_inds_cell\n  Eadd_info: dictionary containing all other relevant info for experiment</p>\n</blockquote>\n\n<p>if the full experiment is NT long, stim and Robs are also NT long, \n    stim[used_inds_cell,:] and Robs[used_inds_cell] are the valid data for that cell, and UiC and XiC are\n    the proper train and test indices of used_inds_cell to access, i.e. \n_model.train( input_data=stim[used_inds_cell,:], output_data=Robs[used_inds_cell], train_indxs=UiC, ...)</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">datadir</span>, </span><span class=\"param\"><span class=\"n\">expt_num</span>, </span><span class=\"param\"><span class=\"n\">cell_num</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.DanUtils.monocular_data_import", "modulename": "NDNT.utils.DanUtils", "qualname": "monocular_data_import", "kind": "function", "doc": "<p>Usage: stim, Robs, DFs, used_inds, Eadd_info = binocular_data_import( datadir, expt_num )\nNote: expt num is starting with 1\nblock_output determines whether to fit using block info, or used_inds info (derived from blocks)\nnum_lags is for purposes of used_inds</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">datadir</span>, </span><span class=\"param\"><span class=\"n\">exptn</span>, </span><span class=\"param\"><span class=\"n\">time_shift</span><span class=\"o\">=</span><span class=\"mi\">1</span>, </span><span class=\"param\"><span class=\"n\">num_lags</span><span class=\"o\">=</span><span class=\"mi\">20</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.MattUtils", "modulename": "NDNT.utils.MattUtils", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.utils.MattUtils.load_losses", "modulename": "NDNT.utils.MattUtils", "qualname": "load_losses", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ckpts_directory</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.MattUtils.smooth_ema", "modulename": "NDNT.utils.MattUtils", "qualname": "smooth_ema", "kind": "function", "doc": "<p>EMA implementation according to\n<a href=\"https://github.com/tensorflow/tensorboard/blob/34877f15153e1a2087316b9952c931807a122aa7/tensorboard/components/vz_line_chart2/line-chart.ts#L699\">https://github.com/tensorflow/tensorboard/blob/34877f15153e1a2087316b9952c931807a122aa7/tensorboard/components/vz_line_chart2/line-chart.ts#L699</a></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">scalars</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">weight</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.MattUtils.smooth_conv", "modulename": "NDNT.utils.MattUtils", "qualname": "smooth_conv", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">scalars</span>, </span><span class=\"param\"><span class=\"n\">smoothing</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.MattUtils.plot_losses", "modulename": "NDNT.utils.MattUtils", "qualname": "plot_losses", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ckpts_directory</span>, </span><span class=\"param\"><span class=\"n\">smoothing</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">figsize</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">)</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.NDNutils", "modulename": "NDNT.utils.NDNutils", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.utils.NDNutils.fit_lbfgs", "modulename": "NDNT.utils.NDNutils", "qualname": "fit_lbfgs", "kind": "function", "doc": "<p>Runs fullbatch LBFGS on a Pytorch model and data dictionary</p>\n\n<h6 id=\"inputs\">Inputs:</h6>\n\n<blockquote>\n  <p>Model: Pytorch model\n  data: Dictionary to used with Model.training_step(data)</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model</span>,</span><span class=\"param\">\t<span class=\"n\">data</span>,</span><span class=\"param\">\t<span class=\"n\">parameters</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">optimizer</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">max_iter</span><span class=\"o\">=</span><span class=\"mi\">1000</span>,</span><span class=\"param\">\t<span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">line_search</span><span class=\"o\">=</span><span class=\"s1\">&#39;strong_wolfe&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">history_size</span><span class=\"o\">=</span><span class=\"mi\">100</span>,</span><span class=\"param\">\t<span class=\"n\">tolerance_change</span><span class=\"o\">=</span><span class=\"mf\">1e-07</span>,</span><span class=\"param\">\t<span class=\"n\">tolerance_grad</span><span class=\"o\">=</span><span class=\"mf\">1e-07</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.NDNutils.fit_lbfgs_batch", "modulename": "NDNT.utils.NDNutils", "qualname": "fit_lbfgs_batch", "kind": "function", "doc": "<p>Runs LBFGS on a Pytorch model with batching a dataset into chunks. Both model and \ndata should be on the cpu, and it copies the model and individual batches to the GPU,\naccumulating full-dataset gradients for each epoch.</p>\n\n<p>Note that in this version, the loss accumulates the gradient, but itself only represents\nthe loss over a batch. Does this screw LBFGS up since it is looking for loss decrease? Probably!</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>Model:</strong>  Pytorch model</li>\n<li><strong>dataset:</strong>  Dataset that can be sampled</li>\n<li><strong>batch_size:</strong>  size of chunk to sample; superceded by num_chunks</li>\n<li><strong>num_chunks:</strong>  divide dataset into number of chunks -> sets batch_size (def: None)</li>\n<li><strong>train_inds:</strong>  indices to train over (default=None, meaning full dataset)</li>\n<li><strong>speckledXV:</strong>  whether to use speckled training</li>\n<li><strong>device:</strong>  GPU device to fit model on</li>\n<li><strong>verbose:</strong>  output to screen: 0 is None, 1 (default) is end of each iteration, 2 is updated per batch</li>\n<li><strong>parameters:</strong>  model parameters to fit, Default (None) all model parameters </li>\n<li><strong>max_iter:</strong>  max iterations (passes through full dataset), Default=1000</li>\n<li><strong>lr:</strong>  learning rate (default=1)</li>\n<li><strong>history_size:</strong>  LBFGS history size in approximating Jacobian (Default: 100)</li>\n<li><strong>tolerance_change:</strong>  minimum change in loss criteria (Default: 1e-8)</li>\n<li><strong>tolerance_grad:</strong>  minimum size of gradient criteria (Default: 1e-8)</li>\n<li><strong>line_search:</strong>  whether to use strong-wolfe line search (True, Default) or not </li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None, but model will reflect fit parameters</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model</span>,</span><span class=\"param\">\t<span class=\"n\">dataset</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">1000</span>,</span><span class=\"param\">\t<span class=\"n\">num_chunks</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">train_inds</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">speckledXV</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">parameters</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">max_iter</span><span class=\"o\">=</span><span class=\"mi\">1000</span>,</span><span class=\"param\">\t<span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">history_size</span><span class=\"o\">=</span><span class=\"mi\">100</span>,</span><span class=\"param\">\t<span class=\"n\">tolerance_change</span><span class=\"o\">=</span><span class=\"mf\">1e-08</span>,</span><span class=\"param\">\t<span class=\"n\">tolerance_grad</span><span class=\"o\">=</span><span class=\"mf\">1e-08</span>,</span><span class=\"param\">\t<span class=\"n\">line_search</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.NDNutils.create_optimizer_params", "modulename": "NDNT.utils.NDNutils", "qualname": "create_optimizer_params", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">optimizer_type</span><span class=\"o\">=</span><span class=\"s1\">&#39;AdamW&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">1000</span>,</span><span class=\"param\">\t<span class=\"n\">accumulate_grad_batches</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">max_iter</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">max_epochs</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">num_gpus</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">optimize_graph</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">log_activations</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">progress_bar_refresh</span><span class=\"o\">=</span><span class=\"mi\">20</span>,</span><span class=\"param\">\t<span class=\"n\">early_stopping</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">early_stopping_patience</span><span class=\"o\">=</span><span class=\"mi\">4</span>,</span><span class=\"param\">\t<span class=\"n\">early_stopping_delta</span><span class=\"o\">=</span><span class=\"mf\">0.0</span>,</span><span class=\"param\">\t<span class=\"n\">weight_decay</span><span class=\"o\">=</span><span class=\"mf\">0.01</span>,</span><span class=\"param\">\t<span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"mf\">0.001</span>,</span><span class=\"param\">\t<span class=\"n\">betas</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"mf\">0.999</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">auto_lr</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">full_batch</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">tolerance_change</span><span class=\"o\">=</span><span class=\"mf\">1e-08</span>,</span><span class=\"param\">\t<span class=\"n\">tolerance_grad</span><span class=\"o\">=</span><span class=\"mf\">1e-10</span>,</span><span class=\"param\">\t<span class=\"n\">history_size</span><span class=\"o\">=</span><span class=\"mi\">10</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">line_search_fn</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.NDNutils.CPU_Unpickler", "modulename": "NDNT.utils.NDNutils", "qualname": "CPU_Unpickler", "kind": "class", "doc": "<p>This takes a binary file for reading a pickle data stream.</p>\n\n<p>The protocol version of the pickle is detected automatically, so no\nprotocol argument is needed.  Bytes past the pickled object's\nrepresentation are ignored.</p>\n\n<p>The argument <em>file</em> must have two methods, a read() method that takes\nan integer argument, and a readline() method that requires no\narguments.  Both methods should return bytes.  Thus <em>file</em> can be a\nbinary file object opened for reading, an io.BytesIO object, or any\nother custom object that meets this interface.</p>\n\n<p>Optional keyword arguments are <em>fix_imports</em>, <em>encoding</em> and <em>errors</em>,\nwhich are used to control compatibility support for pickle stream\ngenerated by Python 2.  If <em>fix_imports</em> is True, pickle will try to\nmap the old Python 2 names to the new names used in Python 3.  The\n<em>encoding</em> and <em>errors</em> tell pickle how to decode 8-bit string\ninstances pickled by Python 2; these default to 'ASCII' and 'strict',\nrespectively.  The <em>encoding</em> can be 'bytes' to read these 8-bit\nstring instances as bytes objects.</p>\n", "bases": "_pickle.Unpickler"}, {"fullname": "NDNT.utils.NDNutils.CPU_Unpickler.find_class", "modulename": "NDNT.utils.NDNutils", "qualname": "CPU_Unpickler.find_class", "kind": "function", "doc": "<p>Return an object from a specified module.</p>\n\n<p>If necessary, the module will be imported. Subclasses may override\nthis method (e.g. to restrict unpickling of arbitrary classes and\nfunctions).</p>\n\n<p>This method is called whenever a class or a function object is\nneeded.  Both arguments passed are str objects.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">module</span>, </span><span class=\"param\"><span class=\"n\">name</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.NDNutils.default_save_dir", "modulename": "NDNT.utils.NDNutils", "qualname": "default_save_dir", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.NDNutils.create_time_embedding", "modulename": "NDNT.utils.NDNutils", "qualname": "create_time_embedding", "kind": "function", "doc": "<p>Takes a Txd stimulus matrix and creates a time-embedded matrix of size \nTx(d*L), where L is the desired number of time lags, included as last (folded) dimension. \nIf stim is a 3d array, its dimensions are folded into the 2nd dimension. </p>\n\n<p>Assumes zero-padding.</p>\n\n<p>Optional up-sampling of stimulus and tent-basis representation for filter \nestimation.</p>\n\n<p>Note that xmatrix is formatted so that adjacent time lags are adjacent \nwithin a time-slice of the xmatrix, thus x(t, 1:nLags) gives all time lags \nof the first spatial pixel at time t.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>stim (type):</strong>  simulus matrix (time must be in the first dim).</li>\n<li>num_lags</li>\n<li><strong>up_fac (type):</strong>  description</li>\n<li><strong>tent_spacing (type):</strong>  description</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>numpy array: time-embedded stim matrix</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">stim</span>, </span><span class=\"param\"><span class=\"n\">num_lags</span>, </span><span class=\"param\"><span class=\"n\">up_fac</span><span class=\"o\">=</span><span class=\"mi\">1</span>, </span><span class=\"param\"><span class=\"n\">tent_spacing</span><span class=\"o\">=</span><span class=\"mi\">1</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.NDNutils.create_time_embedding_NIM", "modulename": "NDNT.utils.NDNutils", "qualname": "create_time_embedding_NIM", "kind": "function", "doc": "<p>All the arguments starting with a p are part of params structure which I \nwill fix later.</p>\n\n<p>Takes a Txd stimulus matrix and creates a time-embedded matrix of size \nTx(d*L), where L is the desired number of time lags. If stim is a 3d array, \nthe spatial dimensions are folded into the 2nd dimension. </p>\n\n<p>Assumes zero-padding.</p>\n\n<p>Optional up-sampling of stimulus and tent-basis representation for filter \nestimation.</p>\n\n<p>Note that xmatrix is formatted so that adjacent time lags are adjacent \nwithin a time-slice of the xmatrix, thus x(t, 1:nLags) gives all time lags \nof the first spatial pixel at time t.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>stim (type):</strong>  simulus matrix (time must be in the first dim).</li>\n<li><strong>pdims (list/array):</strong>  length(3) list of stimulus dimensions</li>\n<li><strong>up_fac (type):</strong>  description</li>\n<li><strong>tent_spacing (type):</strong>  description</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>numpy array: time-embedded stim matrix</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">stim</span>, </span><span class=\"param\"><span class=\"n\">pdims</span>, </span><span class=\"param\"><span class=\"n\">up_fac</span><span class=\"o\">=</span><span class=\"mi\">1</span>, </span><span class=\"param\"><span class=\"n\">tent_spacing</span><span class=\"o\">=</span><span class=\"mi\">1</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.NDNutils.is_int", "modulename": "NDNT.utils.NDNutils", "qualname": "is_int", "kind": "function", "doc": "<p>returns Boolean as to whether val is one of many types of integers</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">val</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.NDNutils.design_matrix_tent_basis", "modulename": "NDNT.utils.NDNutils", "qualname": "design_matrix_tent_basis", "kind": "function", "doc": "<p>Produce a design matrix based on continuous data (s) and anchor points for a tent_basis.\nHere s is a continuous variable (e.g., a stimulus) that is function of time -- single dimension --\nand this will generate apply a tent basis set to s with a basis variable for each anchor point. \nThe end anchor points will be one-sided, but these can be dropped by changing \"zero_left\" and/or\n\"zero_right\" into \"True\".</p>\n\n<p>Inputs: \n    s: continuous one-dimensional variable with NT time points\n    anchors: list or array of anchor points for tent-basis set\n    zero_left, zero_right: boolean whether to drop the edge bases (default for both is False)</p>\n\n<h6 id=\"outputs\">Outputs:</h6>\n\n<blockquote>\n  <p>X: design matrix that will be NT x the number of anchors left after zeroing out left and right</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">s</span>, </span><span class=\"param\"><span class=\"n\">anchors</span>, </span><span class=\"param\"><span class=\"n\">zero_left</span><span class=\"o\">=</span><span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"n\">zero_right</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.NDNutils.tent_basis_generate", "modulename": "NDNT.utils.NDNutils", "qualname": "tent_basis_generate", "kind": "function", "doc": "<p>Computes tent-bases over the range of 'xs', with center points at each value of 'xs'.\nAlternatively (if xs=None), will generate a list with init_space and doubling_time up to\nthe total number of parameters. Must specify xs OR num_params. \nNote this assumes discrete (binned) variables to be acted on.</p>\n\n<h6 id=\"defaults\">Defaults:</h6>\n\n<blockquote>\n  <p>doubling_time = num_params\n  init_space = 1</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">xs</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_params</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">doubling_time</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">init_spacing</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">first_lag</span><span class=\"o\">=</span><span class=\"mi\">0</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.NDNutils.shift_mat_zpad", "modulename": "NDNT.utils.NDNutils", "qualname": "shift_mat_zpad", "kind": "function", "doc": "<p>Takes a vector or matrix and shifts it along dimension dim by amount \nshift using zero-padding. Positive shifts move the matrix right or down.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x (type):</strong>  description</li>\n<li><strong>shift (type):</strong>  description</li>\n<li><strong>dim (type):</strong>  description</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>type: description</p>\n</blockquote>\n\n<p>Raises:</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">shift</span>, </span><span class=\"param\"><span class=\"n\">dim</span><span class=\"o\">=</span><span class=\"mi\">0</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.NDNutils.generate_xv_folds", "modulename": "NDNT.utils.NDNutils", "qualname": "generate_xv_folds", "kind": "function", "doc": "<p>Will generate unique and cross-validation indices, but subsample in each block\nNT = number of time steps\nnum_folds = fraction of data (1/fold) to set aside for cross-validation\nwhich_fold = which fraction of data to set aside for cross-validation (default: middle of each block)\nnum_blocks = how many blocks to sample fold validation from</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">nt</span>, </span><span class=\"param\"><span class=\"n\">num_folds</span><span class=\"o\">=</span><span class=\"mi\">5</span>, </span><span class=\"param\"><span class=\"n\">num_blocks</span><span class=\"o\">=</span><span class=\"mi\">3</span>, </span><span class=\"param\"><span class=\"n\">which_fold</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.NDNutils.initialize_gaussian_envelope", "modulename": "NDNT.utils.NDNutils", "qualname": "initialize_gaussian_envelope", "kind": "function", "doc": "<p>This assumes a set of filters is passed in, and windows by Gaussian along each non-singleton dimension\nws is all filters (ndims x nfilters)\nwshape is individual filter shape</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ws</span>, </span><span class=\"param\"><span class=\"n\">w_shape</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.NDNutils.pixel2mu", "modulename": "NDNT.utils.NDNutils", "qualname": "pixel2mu", "kind": "function", "doc": "<p>Converts from pixel coordinates to mu values, used by grid_sample. The default will flip\nhorizontal and vertical axes automatically, which is what grid_sample requires\nPixels are starting with number 0 up to L-1, converted to range of -1 to 1</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>p:</strong>  list of pixel locations, presumably num_locations x 2 (horizontal and vertical coords)</li>\n<li><strong>L:</strong>  size of grid (number of pixels), assuming square (default 60)</li>\n<li><strong>flip_axes:</strong>  whether to swap horizontal and vertical axes as grid_sample needs (default: True)</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>x: mu values</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">p</span>, </span><span class=\"param\"><span class=\"n\">L</span><span class=\"o\">=</span><span class=\"mi\">60</span>, </span><span class=\"param\"><span class=\"n\">flip_axes</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.NDNutils.mu2pixel", "modulename": "NDNT.utils.NDNutils", "qualname": "mu2pixel", "kind": "function", "doc": "<p>Converts from mu values back into pixel coordinates, where mus are coordinates used by grid_sample.\nCan be continuous (fractional pixels), but default is rounding to nearest int</p>\n\n<p>Args: \n    x: mu values, assuming num_locations x 2 (although can be 1-dimensional)\n    L: size of grid (number of pixels), assuming square (default 60)\n    force_int: force to be integer (versus continuous-valued)\n    enforce bounds: go to edges if answer ends up bigger than L-1 or smaller than 0\n    flip_axes: whether to swap horizontal and vertical axes as grid_sample needs (default: True)</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>p: pixel values</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">L</span><span class=\"o\">=</span><span class=\"mi\">60</span>, </span><span class=\"param\"><span class=\"n\">force_int</span><span class=\"o\">=</span><span class=\"kc\">True</span>, </span><span class=\"param\"><span class=\"n\">enforce_bounds</span><span class=\"o\">=</span><span class=\"kc\">True</span>, </span><span class=\"param\"><span class=\"n\">flip_axes</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.NDNutils.pixel2grid", "modulename": "NDNT.utils.NDNutils", "qualname": "pixel2grid", "kind": "function", "doc": "<p>Pixels are starting with number 0 up to L-1, converted to range of -1 to 1. This is the\nold function, to be replaced by pixel2mu</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">p</span>, </span><span class=\"param\"><span class=\"n\">L</span><span class=\"o\">=</span><span class=\"mi\">60</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.NDNutils.grid2pixel", "modulename": "NDNT.utils.NDNutils", "qualname": "grid2pixel", "kind": "function", "doc": "<p>Older function: replaced by mu2pixel</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">L</span><span class=\"o\">=</span><span class=\"mi\">60</span>, </span><span class=\"param\"><span class=\"n\">force_int</span><span class=\"o\">=</span><span class=\"kc\">True</span>, </span><span class=\"param\"><span class=\"n\">enforce_bounds</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.NDNutils.set_scaffold_level_reg", "modulename": "NDNT.utils.NDNutils", "qualname": "set_scaffold_level_reg", "kind": "function", "doc": "<p>Sets up regularization for scaffold_level, which requires pulling scaffold structure from \ncore network and passing it into the readout layer at beginning of readout_net</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>ndn:</strong>  model that has scaffold and readout ffnetworks</li>\n<li><strong>reg_val:</strong>  scaffold_level reg value to set. None (default) resets</li>\n<li><strong>level_exponent:</strong>  how much to weight each level, default 1</li>\n<li><strong>core_net:</strong>  which ffnetwork is the core (default 0)</li>\n<li><strong>readout_net:</strong>  which ffnetwork is the readout (default 1)</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ndn</span>, </span><span class=\"param\"><span class=\"n\">reg_val</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">level_exponent</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>, </span><span class=\"param\"><span class=\"n\">core_net</span><span class=\"o\">=</span><span class=\"mi\">0</span>, </span><span class=\"param\"><span class=\"n\">readout_net</span><span class=\"o\">=</span><span class=\"mi\">1</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.NDNutils.save_checkpoint", "modulename": "NDNT.utils.NDNutils", "qualname": "save_checkpoint", "kind": "function", "doc": "<p>Saves torch model to checkpoint file.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>state (torch model state):</strong>  State of a torch Neural Network (use model.state_dict() to get it)</li>\n<li><strong>save_path (str):</strong>  Destination path for saving checkpoint</li>\n<li><strong>is_best (bool):</strong>  If <code>True</code> creates additional copy\n<code>best_model.ckpt</code></li>\n<li><strong>max_keep (int):</strong>  Specifies the max amount of checkpoints to keep</li>\n</ul>\n\n<p>credit: pulled from <a href=\"https://github.com/IgorSusmelj/pytorch-styleguide\">https://github.com/IgorSusmelj/pytorch-styleguide</a></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">state</span>, </span><span class=\"param\"><span class=\"n\">save_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">is_best</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"n\">max_keep</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.NDNutils.load_checkpoint", "modulename": "NDNT.utils.NDNutils", "qualname": "load_checkpoint", "kind": "function", "doc": "<p>Loads torch model from checkpoint file.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>ckpt_dir_or_file (str):</strong>  Path to checkpoint directory or filename</li>\n<li><strong>map_location:</strong>  Can be used to directly load to specific device</li>\n<li><strong>load_best (bool):</strong>  If True loads <code>best_model.ckpt</code> if exists.</li>\n</ul>\n\n<p>credit: pulled from <a href=\"https://github.com/IgorSusmelj/pytorch-styleguide\">https://github.com/IgorSusmelj/pytorch-styleguide</a></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ckpt_dir_or_file</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">map_location</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">load_best</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.NDNutils.ensure_dir", "modulename": "NDNT.utils.NDNutils", "qualname": "ensure_dir", "kind": "function", "doc": "<p>Creates folder if not exists.</p>\n\n<p>credit: pulled from <a href=\"https://github.com/IgorSusmelj/pytorch-styleguide\">https://github.com/IgorSusmelj/pytorch-styleguide</a></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">dir_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.NDNutils.ModelSummary", "modulename": "NDNT.utils.NDNutils", "qualname": "ModelSummary", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model</span>,</span><span class=\"param\">\t<span class=\"n\">input_size</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"o\">=-</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">(</span><span class=\"nb\">type</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">index</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>,</span><span class=\"param\">\t<span class=\"n\">dtypes</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.NDNutils.summary_string", "modulename": "NDNT.utils.NDNutils", "qualname": "summary_string", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model</span>,</span><span class=\"param\">\t<span class=\"n\">input_size</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"o\">=-</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">(</span><span class=\"nb\">type</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">index</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>,</span><span class=\"param\">\t<span class=\"n\">dtypes</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.NDNutils.get_fit_versions", "modulename": "NDNT.utils.NDNutils", "qualname": "get_fit_versions", "kind": "function", "doc": "<p>This looks in the checkpoint_dir and finds all the versions of the fit model\nand returns a dictionary with the version number, events file, model file, and\nbest validation loss</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>checkpoint_dir:</strong>  directory where the checkpoints are stored</li>\n<li><strong>model_name:</strong>  name of the model (as it would be in the checkpoint_dir)</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>outdict: dictionary with the following keys:\n      'version_num': list of version numbers\n      'events_file': list of events files\n      'model_file': list of model files\n      'val_loss': list of best validation losses\n      'val_loss_steps': list of validation losses at each epoch</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">checkpoint_dir</span>, </span><span class=\"param\"><span class=\"n\">model_name</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.NDNutils.load_model_from_checkpoint", "modulename": "NDNT.utils.NDNutils", "qualname": "load_model_from_checkpoint", "kind": "function", "doc": "<p>Loads model from checkpoint information. Mostly made for the checkpoint files save as default\nfrom the trainer-fit, but could in principle be used for other checkpoint files.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>checkpoint_path:</strong>  directory of where all the checkpoint files are</li>\n<li><strong>model_name:</strong>  name of the model (sub-directory of checkpoint file), default=''</li>\n<li><strong>version:</strong>  version number of the model, subdirectory within the model (Default: None)</li>\n<li><strong>verbose:</strong>  output to screen (default True)</li>\n<li><strong>filename:</strong>  filenname of the particular model to load (default None)</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>model: NDN model</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">checkpoint_path</span>,</span><span class=\"param\">\t<span class=\"n\">model_name</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">version</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">filename</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.NDNutils.get_max_samples", "modulename": "NDNT.utils.NDNutils", "qualname": "get_max_samples", "kind": "function", "doc": "<p>get the maximum number of samples that fit in memory</p>\n\n<h6 id=\"inputs\">Inputs:</h6>\n\n<blockquote>\n  <p>dataset: the dataset to get the samples from\n  device: the device to put the samples on</p>\n</blockquote>\n\n<h6 id=\"optional\">Optional:</h6>\n\n<blockquote>\n  <p>history_size: the history size parameter for LBFGS (scales memory usage)\n  nquad: the number of quadratic kernels for a gqm (adds # parameters for every new quad filter)\n  num_cells: the number of cells in model (n cells * n parameters)\n  buffer: extra memory to keep free (in GB)</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">dataset</span>,</span><span class=\"param\">\t<span class=\"n\">device</span>,</span><span class=\"param\">\t<span class=\"n\">history_size</span><span class=\"o\">=</span><span class=\"mi\">10</span>,</span><span class=\"param\">\t<span class=\"n\">nquad</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">num_cells</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">buffer</span><span class=\"o\">=</span><span class=\"mf\">1.2</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.NDNutils.NpEncoder", "modulename": "NDNT.utils.NDNutils", "qualname": "NpEncoder", "kind": "class", "doc": "<p>Extensible JSON <a href=\"https://json.org\">https://json.org</a> encoder for Python data structures.</p>\n\n<p>Supports the following objects and types by default:</p>\n\n<p>+-------------------+---------------+\n| Python            | JSON          |\n+===================+===============+\n| dict              | object        |\n+-------------------+---------------+\n| list, tuple       | array         |\n+-------------------+---------------+\n| str               | string        |\n+-------------------+---------------+\n| int, float        | number        |\n+-------------------+---------------+\n| True              | true          |\n+-------------------+---------------+\n| False             | false         |\n+-------------------+---------------+\n| None              | null          |\n+-------------------+---------------+</p>\n\n<p>To extend this to recognize other objects, subclass and implement a\n<code>.default()</code> method with another method that returns a serializable\nobject for <code>o</code> if possible, otherwise it should call the superclass\nimplementation (to raise <code>TypeError</code>).</p>\n", "bases": "json.encoder.JSONEncoder"}, {"fullname": "NDNT.utils.NDNutils.NpEncoder.default", "modulename": "NDNT.utils.NDNutils", "qualname": "NpEncoder.default", "kind": "function", "doc": "<p>Implement this method in a subclass such that it returns\na serializable object for <code>o</code>, or calls the base implementation\n(to raise a <code>TypeError</code>).</p>\n\n<p>For example, to support arbitrary iterators, you could\nimplement default like this::</p>\n\n<pre><code>def default(self, o):\n    try:\n        iterable = iter(o)\n    except TypeError:\n        pass\n    else:\n        return list(iterable)\n    # Let the base class default method raise the TypeError\n    return JSONEncoder.default(self, o)\n</code></pre>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">obj</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.create_reg_matrices", "modulename": "NDNT.utils.create_reg_matrices", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.utils.create_reg_matrices.create_tikhonov_matrix", "modulename": "NDNT.utils.create_reg_matrices", "qualname": "create_tikhonov_matrix", "kind": "function", "doc": "<p>Usage: Tmat = create_Tikhonov_matrix(stim_dims, reg_type, boundary_cond)</p>\n\n<p>Creates a matrix specifying a an L2-regularization operator of the form\n||T*k||^2, where T is a matrix and k is a vector of parameters. Currently \nonly supports second derivative/Laplacian operations</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>stim_dims (list of ints):</strong>  dimensions associated with the target \nstimulus, in the form [num_lags, num_x_pix, num_y_pix]</li>\n<li><strong>reg_type (str):</strong>  specify form of the regularization matrix\n'd2xt' | 'd2x' | 'd2t'</li>\n<li><strong>boundary_conditions (None):</strong>  is a list corresponding to all dimensions\n[default is [True,True,True]\nwould ideally be a dictionary with each reg\ntype listed; currently unused</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>scipy array: matrix specifying the desired Tikhonov operator</p>\n</blockquote>\n\n<h6 id=\"notes\">Notes:</h6>\n\n<blockquote>\n  <p>The method of computing sparse differencing matrices used here is \n  adapted from Bryan C. Smith's and Andrew V. Knyazev's function \n  \"laplacian\", available here: \n  <a href=\"http://www.mathworks.com/matlabcentral/fileexchange/27279-laplacian-in-1d-2d-or-3d\">http://www.mathworks.com/matlabcentral/fileexchange/27279-laplacian-in-1d-2d-or-3d</a>\n  Written in Matlab by James McFarland, adapted into python by Dan Butts</p>\n  \n  <p>Currently, the no-boundary condition case for all but temporal dimension alone is untested and possibly wrong\n  due to the fact that it <em>seems</em> that the indexing in python flips the first and second dimensions and a\n  transpose is thus necessary at the early stage. Not a problem (it seems) because boundary conditions are\n  currently applied by default, which makes the edges zero....</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">stim_dims</span>, </span><span class=\"param\"><span class=\"n\">reg_type</span>, </span><span class=\"param\"><span class=\"n\">boundary_conditions</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.create_reg_matrices.create_maxpenalty_matrix", "modulename": "NDNT.utils.create_reg_matrices", "qualname": "create_maxpenalty_matrix", "kind": "function", "doc": "<p>Usage: Tmat = create_maxpenalty_matrix(input_dims, reg_type)</p>\n\n<p>Creates a matrix specifying a an L2-regularization operator of the form\n||T*k||^2, where T is a matrix and k is a vector of parameters. Currently \nonly supports second derivative/Laplacian operations</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims (list of ints):</strong>  dimensions associated with the target input, \nin the form [num_lags, num_x_pix, num_y_pix]</li>\n<li><strong>reg_type (str):</strong>  specify form of the regularization matrix\n'max' | 'max_filt' | 'max_space'</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>numpy array: matrix specifying the desired Tikhonov operator</p>\n</blockquote>\n\n<h6 id=\"notes\">Notes:</h6>\n\n<blockquote>\n  <p>Adapted from create_Tikhonov_matrix function above.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_dims</span>, </span><span class=\"param\"><span class=\"n\">reg_type</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.create_reg_matrices.create_localpenalty_matrix", "modulename": "NDNT.utils.create_reg_matrices", "qualname": "create_localpenalty_matrix", "kind": "function", "doc": "<p>Usage: Tmat = create_maxpenalty_matrix(input_dims, reg_type)</p>\n\n<p>Creates a matrix specifying a an L2-regularization operator of the form\n||T*k||^2, where T is a matrix and k is a vector of parameters. Currently\nonly supports second derivative/Laplacian operations</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_dims (list of ints):</strong>  dimensions associated with the target input,\nin the form [num_lags, num_x_pix, num_y_pix]</li>\n<li><strong>reg_type (str):</strong>  specify form of the regularization matrix\n'max' | 'max_filt' | 'max_space'</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>numpy array: matrix specifying the desired Tikhonov operator</p>\n</blockquote>\n\n<h6 id=\"notes\">Notes:</h6>\n\n<blockquote>\n  <p>Adapted from create_Tikhonov_matrix function above.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_dims</span>, </span><span class=\"param\"><span class=\"n\">separable</span><span class=\"o\">=</span><span class=\"kc\">True</span>, </span><span class=\"param\"><span class=\"n\">spatial_global</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.ffnet_dicts", "modulename": "NDNT.utils.ffnet_dicts", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.utils.ffnet_dicts.layer_dict", "modulename": "NDNT.utils.ffnet_dicts", "qualname": "layer_dict", "kind": "function", "doc": "<p>input dims are [num_filters, space1, space2, num_lags]</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">NLtype</span><span class=\"o\">=</span><span class=\"s1\">&#39;relu&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">norm_type</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">pos_constraint</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">num_inh</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">output_norm</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">conv</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">conv_width</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">stride</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">dilation</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.ffnet_dicts.ffnet_params_default", "modulename": "NDNT.utils.ffnet_dicts", "qualname": "ffnet_params_default", "kind": "function", "doc": "<p>This creates a ffnetwork_params object that specifies details of ffnetwork\nffnetwork dicts have these fields:\n    ffnet_type: string specifying network type (default = 'normal')\n    layer_list: defaults to None, to be set in internal function\n    input_dims: defaults to None, but will be set when network made, if not sooner\n    xstim_n: external input (or None). Note can only be from one source\n    ffnet_n: list of internal network inputs (has to be a list, or None)\n    conv: [boolean] whether ffnetwork is convolutional or not, defaults to False but to be set</p>\n\n<p>-- Note xstim_n and ffnet_n are created/formatted here. \n-- If xstim_n is specified, it must specify input dimensions\n-- This should set all required fields as needed (even if none)</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">xstim_n</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">ffnet_n</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.ffnet_dicts.ffnet_dict_NIM", "modulename": "NDNT.utils.ffnet_dicts", "qualname": "ffnet_dict_NIM", "kind": "function", "doc": "<p>This creates will make a list of layer dicts corresponding to a non-convolutional NIM].\nNote that input_dims can be set to none</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">layer_sizes</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">layer_types</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">act_funcs</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">ei_layers</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">conv_widths</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">norm_list</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">reg_list</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">xstim_n</span><span class=\"o\">=</span><span class=\"s1\">&#39;stim&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">ffnet_n</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">ffnet_type</span><span class=\"o\">=</span><span class=\"s1\">&#39;normal&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.ffnet_dicts.ffnet_dict_readout", "modulename": "NDNT.utils.ffnet_dicts", "qualname": "ffnet_dict_readout", "kind": "function", "doc": "<p>This sets up dictionary parameters for readout ffnetwork, establishing all the relevant info. Note that the shifter\nis designated as the second of two ffnet_n inputs listed, so is not separately specified.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">ffnet_n</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_cells</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">act_func</span><span class=\"o\">=</span><span class=\"s1\">&#39;softplus&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">init_mu_range</span><span class=\"o\">=</span><span class=\"mf\">0.1</span>,</span><span class=\"param\">\t<span class=\"n\">init_sigma</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">batch_sample</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">align_corners</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">gauss_type</span><span class=\"o\">=</span><span class=\"s1\">&#39;uncorrelated&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">pos_constraint</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">reg_list</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.ffnet_dicts.ffnet_dict_external", "modulename": "NDNT.utils.ffnet_dicts", "qualname": "ffnet_dict_external", "kind": "function", "doc": "<p>The network information passed in must simply be:</p>\n\n<ol>\n<li>The name of the network in the network dictionary that is passed into the constructor.</li>\n<li>The source of network input (external stim or otherwise).</li>\n<li>If external input, its input dims must also be specified.</li>\n<li>If the network takes input that needs to be reshaped, pass in 'input_dims_reshaped' that adds the \nbatch dimension and then reshapes before passing into external network. Note this will be the dimensionality\nthat the network takes as input, so does not need to be the NDN convention [CWHT]</li>\n<li>The output dims that are passed to the rest of the network (needs to be [CWHT]</li>\n</ol>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">&#39;external&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">xstim_n</span><span class=\"o\">=</span><span class=\"s1\">&#39;stim&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">ffnet_n</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">input_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">input_dims_reshape</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">output_dims</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.ffnet_dicts.list_complete", "modulename": "NDNT.utils.ffnet_dicts", "qualname": "list_complete", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">fauxlist</span>, </span><span class=\"param\"><span class=\"n\">L</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">null_val</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.plotting", "modulename": "NDNT.utils.plotting", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.utils.plotting.plot_filters_1D", "modulename": "NDNT.utils.plotting", "qualname": "plot_filters_1D", "kind": "function", "doc": "<p>function to plot 1-D spatiotemporal filters (so, 2-d images) by passing in weights of multiple filters</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ws</span>, </span><span class=\"param\"><span class=\"n\">num_cols</span><span class=\"o\">=</span><span class=\"mi\">8</span>, </span><span class=\"param\"><span class=\"n\">row_height</span><span class=\"o\">=</span><span class=\"mi\">2</span>, </span><span class=\"param\"><span class=\"n\">fix_scale</span><span class=\"o\">=</span><span class=\"kc\">True</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.plotting.plot_filters_2D", "modulename": "NDNT.utils.plotting", "qualname": "plot_filters_2D", "kind": "function", "doc": "<p>function to plot 1-D spatiotemporal filters (so, 2-d images) by passing in weights of multiple filters</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">ws</span>,</span><span class=\"param\">\t<span class=\"n\">num_cols</span><span class=\"o\">=</span><span class=\"mi\">8</span>,</span><span class=\"param\">\t<span class=\"n\">row_height</span><span class=\"o\">=</span><span class=\"mi\">2</span>,</span><span class=\"param\">\t<span class=\"n\">fix_scale</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">cmap</span><span class=\"o\">=</span><span class=\"s1\">&#39;viridis&#39;</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.plotting.plot_filters_ST1D", "modulename": "NDNT.utils.plotting", "qualname": "plot_filters_ST1D", "kind": "function", "doc": "<p>function to plot 1-D spatiotemporal filters (so, 2-d images) by passing in weights of multiple filters</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">ws</span>,</span><span class=\"param\">\t<span class=\"n\">cmap</span><span class=\"o\">=</span><span class=\"s1\">&#39;viridis&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">num_cols</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">row_height</span><span class=\"o\">=</span><span class=\"mi\">2</span>,</span><span class=\"param\">\t<span class=\"n\">fix_scale</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.plotting.plot_filters_ST2D", "modulename": "NDNT.utils.plotting", "qualname": "plot_filters_ST2D", "kind": "function", "doc": "<p>Stolen directly from neureye -> core.plot_filters, and modified</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ws</span>, </span><span class=\"param\"><span class=\"n\">sort</span><span class=\"o\">=</span><span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.plotting.plot_filters_ST3D", "modulename": "NDNT.utils.plotting", "qualname": "plot_filters_ST3D", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ws</span>, </span><span class=\"param\"><span class=\"n\">sort</span><span class=\"o\">=</span><span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.plotting.plot_internal_convlayer", "modulename": "NDNT.utils.plotting", "qualname": "plot_internal_convlayer", "kind": "function", "doc": "<p>Display weights of filter layer with NC x NX x NY (and no 4th dimension)</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">wP</span>, </span><span class=\"param\"><span class=\"n\">cmap</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.plotting.plot_scatter", "modulename": "NDNT.utils.plotting", "qualname": "plot_scatter", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">xs</span>, </span><span class=\"param\"><span class=\"n\">ys</span>, </span><span class=\"param\"><span class=\"n\">clr</span><span class=\"o\">=</span><span class=\"s1\">&#39;g&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.utils.plotting.plot_internal_weights", "modulename": "NDNT.utils.plotting", "qualname": "plot_internal_weights", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ws</span>, </span><span class=\"param\"><span class=\"n\">num_inh</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.version", "modulename": "NDNT.version", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.version.version", "modulename": "NDNT.version", "qualname": "version", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;0.1.0&#x27;"}, {"fullname": "NDNT.metrics", "modulename": "NDNT.metrics", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.metrics.mse_loss", "modulename": "NDNT.metrics.mse_loss", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.metrics.mse_loss.MseLoss_datafilter", "modulename": "NDNT.metrics.mse_loss", "qualname": "MseLoss_datafilter", "kind": "class", "doc": "<p>This is initialized with default behavior that requires no knowledge of the dataset:\nunit_weighting = False, meaning no external weighting of units, such as by firing rate)\n    True corresponds to weighting being stored in buffer unit_weights<br />\nbatch_weighting = 0: 'batch_size', 1: 'data_filter', 2: 'av_batch_size', -1: \"unnormalized\"  default 0\n    Default requires no info from dataset: LL is normalized by time steps in batch, but otherwise\n    could use average batch size (for consistency across epoch), or datafilter (to get correct per-neuron) \nFor example using batch size and unit_weights corresponding to reciprocal of probability of spike per bin will\ngive standard LL/spk. \nNote that default is using batch_size, and info must be oassed in using 'set_loss_weighting' to alter behavior.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "NDNT.metrics.mse_loss.MseLoss_datafilter.__init__", "modulename": "NDNT.metrics.mse_loss", "qualname": "MseLoss_datafilter.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.metrics.mse_loss.MseLoss_datafilter.loss_name", "modulename": "NDNT.metrics.mse_loss", "qualname": "MseLoss_datafilter.loss_name", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.metrics.mse_loss.MseLoss_datafilter.loss", "modulename": "NDNT.metrics.mse_loss", "qualname": "MseLoss_datafilter.loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.metrics.mse_loss.MseLoss_datafilter.lossNR", "modulename": "NDNT.metrics.mse_loss", "qualname": "MseLoss_datafilter.lossNR", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.metrics.mse_loss.MseLoss_datafilter.unit_weighting", "modulename": "NDNT.metrics.mse_loss", "qualname": "MseLoss_datafilter.unit_weighting", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.metrics.mse_loss.MseLoss_datafilter.batch_weighting", "modulename": "NDNT.metrics.mse_loss", "qualname": "MseLoss_datafilter.batch_weighting", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.metrics.mse_loss.MseLoss_datafilter.set_loss_weighting", "modulename": "NDNT.metrics.mse_loss", "qualname": "MseLoss_datafilter.set_loss_weighting", "kind": "function", "doc": "<h6 id=\"this-changes-default-loss-function-weights-to-adjust-for-the-dataset-by-setting-two-flags\">This changes default loss function weights to adjust for the dataset by setting two flags:</h6>\n\n<blockquote>\n  <p>unit_weighting: whether to weight neurons by different amounts in loss function (e.g., av spike rate)\n  batch_weighting: how much to weight each batch, dividing by following quantity\n      0 (default) 'batch\n      1 'data_filters': weight each neuron individually by the amount of data\n      2 'av_batch_size': weight by average batch size. Needs av_batch_size set/initialized from dataset info\n      -1 'unnormalized': no weighing at all. this will implicitly increase with batch size</p>\n</blockquote>\n\n<p>Args\n    batch_weighting (int): 0, 1, 2, -1\n    unit_weighting (bool): whether to weight units\n    unit_weights (torch.tensor): weights for each unit\n    av_batch_size (int): average batch size</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">batch_weighting</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">unit_weighting</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">unit_weights</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">av_batch_size</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.metrics.mse_loss.MseLoss_datafilter.forward", "modulename": "NDNT.metrics.mse_loss", "qualname": "MseLoss_datafilter.forward", "kind": "function", "doc": "<p>This is the forward function for the loss function. It calculates the loss based on the input and target data.\nThe loss is calculated as the mean squared error between the prediction and target data.</p>\n\n<p>Args\n    pred (torch.tensor): prediction data\n    target (torch.tensor): target data\n    data_filters (torch.tensor): data filters for each unit</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>loss (torch.tensor): loss value</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">pred</span>, </span><span class=\"param\"><span class=\"n\">target</span>, </span><span class=\"param\"><span class=\"n\">data_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.metrics.mse_loss.MseLoss_datafilter.unit_loss", "modulename": "NDNT.metrics.mse_loss", "qualname": "MseLoss_datafilter.unit_loss", "kind": "function", "doc": "<p>This should be equivalent of forward, without sum over units\nCurrently only true if batch_weighting = 'data_filter'.</p>\n\n<p>Args\n    pred (torch.tensor): prediction data\n    target (torch.tensor): target data\n    data_filters (torch.tensor): data filters for each unit\n    temporal_normalize (bool): whether to normalize by time steps</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>unitloss (torch.tensor): loss value</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">pred</span>, </span><span class=\"param\"><span class=\"n\">target</span>, </span><span class=\"param\"><span class=\"n\">data_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">temporal_normalize</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.metrics.poisson_loss", "modulename": "NDNT.metrics.poisson_loss", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.metrics.poisson_loss.PoissonLoss_datafilter", "modulename": "NDNT.metrics.poisson_loss", "qualname": "PoissonLoss_datafilter", "kind": "class", "doc": "<p>This is initialized with default behavior that requires no knowledge of the dataset:\nunit_weighting = False, meaning no external weighting of units, such as by firing rate)\n    True corresponds to weighting being stored in buffer unit_weights<br />\nbatch_weighting = 0: 'batch_size', 1: 'data_filter', 2: 'av_batch_size', -1: \"unnormalized\"  default 0\n    Default requires no info from dataset: LL is normalized by time steps in batch, but otherwise\n    could use average batch size (for consistency across epoch), or datafilter (to get correct per-neuron) \nFor example using batch size and unit_weights corresponding to reciprocal of probability of spike per bin will\ngive standard LL/spk. \nNote that default is using batch_size, and info must be oassed in using 'set_loss_weighting' to alter behavior</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "NDNT.metrics.poisson_loss.PoissonLoss_datafilter.__init__", "modulename": "NDNT.metrics.poisson_loss", "qualname": "PoissonLoss_datafilter.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.metrics.poisson_loss.PoissonLoss_datafilter.loss_name", "modulename": "NDNT.metrics.poisson_loss", "qualname": "PoissonLoss_datafilter.loss_name", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.metrics.poisson_loss.PoissonLoss_datafilter.loss", "modulename": "NDNT.metrics.poisson_loss", "qualname": "PoissonLoss_datafilter.loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.metrics.poisson_loss.PoissonLoss_datafilter.lossNR", "modulename": "NDNT.metrics.poisson_loss", "qualname": "PoissonLoss_datafilter.lossNR", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.metrics.poisson_loss.PoissonLoss_datafilter.unit_weighting", "modulename": "NDNT.metrics.poisson_loss", "qualname": "PoissonLoss_datafilter.unit_weighting", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.metrics.poisson_loss.PoissonLoss_datafilter.batch_weighting", "modulename": "NDNT.metrics.poisson_loss", "qualname": "PoissonLoss_datafilter.batch_weighting", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.metrics.poisson_loss.PoissonLoss_datafilter.set_log_epsilon", "modulename": "NDNT.metrics.poisson_loss", "qualname": "PoissonLoss_datafilter.set_log_epsilon", "kind": "function", "doc": "<p>Changes the floor of the poisson loss function (the epsilon in the logarithm). Leave epsilon\nblank or None if want to reset to default (1e-8).</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>epsilon (float):</strong>  floor of the poisson loss function</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">epsilon</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.metrics.poisson_loss.PoissonLoss_datafilter.set_loss_weighting", "modulename": "NDNT.metrics.poisson_loss", "qualname": "PoissonLoss_datafilter.set_loss_weighting", "kind": "function", "doc": "<h6 id=\"this-changes-default-loss-function-weights-to-adjust-for-the-dataset-by-setting-two-flags\">This changes default loss function weights to adjust for the dataset by setting two flags:</h6>\n\n<blockquote>\n  <p>unit_weighting: whether to weight neurons by different amounts in loss function (e.g., av spike rate), which\n      access self.unit_weights, which should be passed in if turning this on\n  batch_weighting: how much to weight each batch, dividing by following quantity\n      0 (default) 'batch_size': weight by length of batch, regardless of dfs\n      1 'data_filters': weight each neuron individually by the amount of data in the particular batch\n      2 'av_batch_size': weight by average batch size. Needs av_batch_size set/initialized from dataset info\n      -1 'unnormalized': no weighing at all. this will implicitly increase with batch size</p>\n</blockquote>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>batch_weighting (int):</strong>  0, 1, 2, -1</li>\n<li><strong>unit_weighting (bool):</strong>  whether to weight units</li>\n<li><strong>unit_weights (torch.tensor):</strong>  weights for each unit</li>\n<li><strong>av_batch_size (int):</strong>  average batch size</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None, but sets self-variables as described</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">batch_weighting</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">unit_weighting</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">unit_weights</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">av_batch_size</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.metrics.poisson_loss.PoissonLoss_datafilter.forward", "modulename": "NDNT.metrics.poisson_loss", "qualname": "PoissonLoss_datafilter.forward", "kind": "function", "doc": "<p>This is the forward function for the loss function. It calculates the loss based on the input and target data.\nThe loss is calculated as the Poisson negative log likelihood between the prediction and target data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>pred (torch.tensor):</strong>  prediction data</li>\n<li><strong>target (torch.tensor):</strong>  target data</li>\n<li><strong>data_filters (torch.tensor):</strong>  data filters for each unit</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>loss (torch.tensor): loss value</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">pred</span>, </span><span class=\"param\"><span class=\"n\">target</span>, </span><span class=\"param\"><span class=\"n\">data_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.metrics.poisson_loss.PoissonLoss_datafilter.unit_loss", "modulename": "NDNT.metrics.poisson_loss", "qualname": "PoissonLoss_datafilter.unit_loss", "kind": "function", "doc": "<p>This should be equivalent of forward, without sum over units, and no unit-specific weighting.\nCurrently only true if batch_weighting = 'data_filter'.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>pred (torch.tensor):</strong>  prediction data</li>\n<li><strong>target (torch.tensor):</strong>  target data</li>\n<li><strong>data_filters (torch.tensor):</strong>  data filters for each unit</li>\n<li><strong>temporal_normalize (bool):</strong>  whether to normalize by time steps</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>unitloss (torch.tensor): loss value for each unit</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">pred</span>, </span><span class=\"param\"><span class=\"n\">target</span>, </span><span class=\"param\"><span class=\"n\">data_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">temporal_normalize</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.metrics.poisson_loss.PoissonLossLagged", "modulename": "NDNT.metrics.poisson_loss", "qualname": "PoissonLossLagged", "kind": "class", "doc": "<p>This is initialized with default behavior that requires no knowledge of the dataset:\nunit_weighting = False, meaning no external weighting of units, such as by firing rate)\n    True corresponds to weighting being stored in buffer unit_weights<br />\nbatch_weighting = 0: 'batch_size', 1: 'data_filter', 2: 'av_batch_size', -1: \"unnormalized\"  default 0\n    Default requires no info from dataset: LL is normalized by time steps in batch, but otherwise\n    could use average batch size (for consistency across epoch), or datafilter (to get correct per-neuron) \nFor example using batch size and unit_weights corresponding to reciprocal of probability of spike per bin will\ngive standard LL/spk. \nNote that default is using batch_size, and info must be oassed in using 'set_loss_weighting' to alter behavior</p>\n", "bases": "PoissonLoss_datafilter"}, {"fullname": "NDNT.metrics.poisson_loss.PoissonLossLagged.__init__", "modulename": "NDNT.metrics.poisson_loss", "qualname": "PoissonLossLagged.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.metrics.poisson_loss.PoissonLossLagged.loss_name", "modulename": "NDNT.metrics.poisson_loss", "qualname": "PoissonLossLagged.loss_name", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.metrics.poisson_loss.PoissonLossLagged.num_lags", "modulename": "NDNT.metrics.poisson_loss", "qualname": "PoissonLossLagged.num_lags", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.metrics.poisson_loss.PoissonLossLagged.forward", "modulename": "NDNT.metrics.poisson_loss", "qualname": "PoissonLossLagged.forward", "kind": "function", "doc": "<p>This is the forward function for the loss function. It calculates the loss based on the input and target data.\nThe loss is calculated as the Poisson negative log likelihood between the prediction and target data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>pred (torch.tensor):</strong>  prediction data</li>\n<li><strong>target (torch.tensor):</strong>  target data</li>\n<li><strong>data_filters (torch.tensor):</strong>  data filters for each unit</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>loss (torch.tensor): loss value</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">pred</span>, </span><span class=\"param\"><span class=\"n\">target</span>, </span><span class=\"param\"><span class=\"n\">data_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.metrics.poisson_loss.PoissonLossLagged.unit_loss", "modulename": "NDNT.metrics.poisson_loss", "qualname": "PoissonLossLagged.unit_loss", "kind": "function", "doc": "<h4 id=\"note-this-might-not-be-updated-for-lagged-version\">NOTE THIS MIGHT NOT BE UPDATED FOR Lagged version</h4>\n\n<p>This should be equivalent of forward, without sum over units\nCurrently only true if batch_weighting = 'data_filter'.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>pred (torch.tensor):</strong>  prediction data</li>\n<li><strong>target (torch.tensor):</strong>  target data</li>\n<li><strong>data_filters (torch.tensor):</strong>  data filters for each unit</li>\n<li><strong>temporal_normalize (bool):</strong>  whether to normalize by time steps</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>unitloss (torch.tensor): loss value for each unit</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">pred</span>, </span><span class=\"param\"><span class=\"n\">target</span>, </span><span class=\"param\"><span class=\"n\">data_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">temporal_normalize</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.metrics.poisson_loss.SimplePoissonLoss", "modulename": "NDNT.metrics.poisson_loss", "qualname": "SimplePoissonLoss", "kind": "class", "doc": "<p>Modified Poisson loss to involve minimal extra calcs\nThis is initialized with default behavior that requires no knowledge of the dataset:\nunit_weighting = False, meaning no external weighting of units, such as by firing rate)\n    True corresponds to weighting being stored in buffer unit_weights<br />\nbatch_weighting = 0: 'batch_size', 1: 'data_filter', 2: 'av_batch_size', -1: \"unnormalized\"  default 0\n    Default requires no info from dataset: LL is normalized by time steps in batch, but otherwise\n    could use average batch size (for consistency across epoch), or datafilter (to get correct per-neuron) \nFor example using batch size and unit_weights corresponding to reciprocal of probability of spike per bin will\ngive standard LL/spk. \nNote that default is using batch_size, and info must be oassed in using 'set_loss_weighting' to alter behavior</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "NDNT.metrics.poisson_loss.SimplePoissonLoss.__init__", "modulename": "NDNT.metrics.poisson_loss", "qualname": "SimplePoissonLoss.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.metrics.poisson_loss.SimplePoissonLoss.loss_name", "modulename": "NDNT.metrics.poisson_loss", "qualname": "SimplePoissonLoss.loss_name", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.metrics.poisson_loss.SimplePoissonLoss.loss", "modulename": "NDNT.metrics.poisson_loss", "qualname": "SimplePoissonLoss.loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.metrics.poisson_loss.SimplePoissonLoss.lossNR", "modulename": "NDNT.metrics.poisson_loss", "qualname": "SimplePoissonLoss.lossNR", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.metrics.poisson_loss.SimplePoissonLoss.unit_weighting", "modulename": "NDNT.metrics.poisson_loss", "qualname": "SimplePoissonLoss.unit_weighting", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.metrics.poisson_loss.SimplePoissonLoss.batch_weighting", "modulename": "NDNT.metrics.poisson_loss", "qualname": "SimplePoissonLoss.batch_weighting", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.metrics.poisson_loss.SimplePoissonLoss.set_loss_weighting", "modulename": "NDNT.metrics.poisson_loss", "qualname": "SimplePoissonLoss.set_loss_weighting", "kind": "function", "doc": "<h6 id=\"this-changes-default-loss-function-weights-to-adjust-for-the-dataset-by-setting-two-flags\">This changes default loss function weights to adjust for the dataset by setting two flags:</h6>\n\n<blockquote>\n  <p>unit_weighting: whether to weight neurons by different amounts in loss function (e.g., av spike rate)\n  batch_weighting: how much to weight each batch, dividing by following quantity\n      0 (default) 'batch_size': weight by length of batch\n      1 'data_filters': weight each neuron individually by the amount of data\n      2 'av_batch_size': weight by average batch size. Needs av_batch_size set/initialized from dataset info\n      -1 'unnormalized': no weighing at all. this will implicitly increase with batch size</p>\n</blockquote>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>batch_weighting (int):</strong>  0, 1, 2, -1</li>\n<li><strong>unit_weighting (bool):</strong>  whether to weight units</li>\n<li><strong>unit_weights (torch.tensor):</strong>  weights for each unit</li>\n<li><strong>av_batch_size (int):</strong>  average batch size</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">batch_weighting</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">unit_weighting</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">unit_weights</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">av_batch_size</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.metrics.poisson_loss.SimplePoissonLoss.forward", "modulename": "NDNT.metrics.poisson_loss", "qualname": "SimplePoissonLoss.forward", "kind": "function", "doc": "<p>This is the forward function for the loss function. It calculates the loss based on the input and target data.\nThe loss is calculated as the Poisson negative log likelihood between the prediction and target data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>pred (torch.tensor):</strong>  prediction data</li>\n<li><strong>target (torch.tensor):</strong>  target data</li>\n<li><strong>data_filters (torch.tensor):</strong>  data filters for each unit</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>loss (torch.tensor): loss value</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">pred</span>, </span><span class=\"param\"><span class=\"n\">target</span>, </span><span class=\"param\"><span class=\"n\">data_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.metrics.poisson_loss.SimplePoissonLoss.unit_loss", "modulename": "NDNT.metrics.poisson_loss", "qualname": "SimplePoissonLoss.unit_loss", "kind": "function", "doc": "<p>This should be equivalent of forward, without sum over units\nCurrently only true if batch_weighting = 'data_filter'.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>pred (torch.tensor):</strong>  prediction data</li>\n<li><strong>target (torch.tensor):</strong>  target data</li>\n<li><strong>data_filters (torch.tensor):</strong>  data filters for each unit</li>\n<li><strong>temporal_normalize (bool):</strong>  whether to normalize by time steps</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>unitloss (torch.tensor): loss value for each unit</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">pred</span>, </span><span class=\"param\"><span class=\"n\">target</span>, </span><span class=\"param\"><span class=\"n\">data_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">temporal_normalize</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.metrics.rmse_loss", "modulename": "NDNT.metrics.rmse_loss", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "NDNT.metrics.rmse_loss.RmseLoss", "modulename": "NDNT.metrics.rmse_loss", "qualname": "RmseLoss", "kind": "class", "doc": "<p>This is initialized with default behavior that requires no knowledge of the dataset:\nunit_weighting = False, meaning no external weighting of units, such as by firing rate)\n    True corresponds to weighting being stored in buffer unit_weights<br />\nbatch_weighting = 0: 'batch_size', 1: 'data_filter', 2: 'av_batch_size', -1: \"unnormalized\"  default 0\n    Default requires no info from dataset: LL is normalized by time steps in batch, but otherwise\n    could use average batch size (for consistency across epoch), or datafilter (to get correct per-neuron) \nFor example using batch size and unit_weights corresponding to reciprocal of probability of spike per bin will\ngive standard LL/spk. \nNote that default is using batch_size, and info must be oassed in using 'set_loss_weighting' to alter behavior</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "NDNT.metrics.rmse_loss.RmseLoss.__init__", "modulename": "NDNT.metrics.rmse_loss", "qualname": "RmseLoss.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "NDNT.metrics.rmse_loss.RmseLoss.loss_name", "modulename": "NDNT.metrics.rmse_loss", "qualname": "RmseLoss.loss_name", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.metrics.rmse_loss.RmseLoss.loss", "modulename": "NDNT.metrics.rmse_loss", "qualname": "RmseLoss.loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.metrics.rmse_loss.RmseLoss.lossNR", "modulename": "NDNT.metrics.rmse_loss", "qualname": "RmseLoss.lossNR", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.metrics.rmse_loss.RmseLoss.unit_weighting", "modulename": "NDNT.metrics.rmse_loss", "qualname": "RmseLoss.unit_weighting", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.metrics.rmse_loss.RmseLoss.batch_weighting", "modulename": "NDNT.metrics.rmse_loss", "qualname": "RmseLoss.batch_weighting", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "NDNT.metrics.rmse_loss.RmseLoss.set_loss_weighting", "modulename": "NDNT.metrics.rmse_loss", "qualname": "RmseLoss.set_loss_weighting", "kind": "function", "doc": "<h6 id=\"this-changes-default-loss-function-weights-to-adjust-for-the-dataset-by-setting-two-flags\">This changes default loss function weights to adjust for the dataset by setting two flags:</h6>\n\n<blockquote>\n  <p>unit_weighting: whether to weight neurons by different amounts in loss function (e.g., av spike rate)\n  batch_weighting: how much to weight each batch, dividing by following quantity\n      0 (default) 'batch\n      1 'data_filters': weight each neuron individually by the amount of data\n      2 'av_batch_size': weight by average batch size. Needs av_batch_size set/initialized from dataset info\n      -1 'unnormalized': no weighing at all. this will implicitly increase with batch size</p>\n</blockquote>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>batch_weighting (int):</strong>  0, 1, 2, -1</li>\n<li><strong>unit_weighting (bool):</strong>  whether to weight units</li>\n<li><strong>unit_weights (torch.tensor):</strong>  weights for each unit</li>\n<li><strong>av_batch_size (int):</strong>  average batch size</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">batch_weighting</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">unit_weighting</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">unit_weights</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">av_batch_size</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.metrics.rmse_loss.RmseLoss.forward", "modulename": "NDNT.metrics.rmse_loss", "qualname": "RmseLoss.forward", "kind": "function", "doc": "<p>This is the forward function for the loss function. It calculates the loss based on the input and target data.\nThe loss is calculated as the mean squared error between the prediction and target data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>pred (torch.tensor):</strong>  predicted data</li>\n<li><strong>target (torch.tensor):</strong>  target data</li>\n<li><strong>data_filters (torch.tensor):</strong>  data filters for each unit</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>loss (torch.tensor): mean squared error loss</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">pred</span>, </span><span class=\"param\"><span class=\"n\">target</span>, </span><span class=\"param\"><span class=\"n\">data_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "NDNT.metrics.rmse_loss.RmseLoss.unit_loss", "modulename": "NDNT.metrics.rmse_loss", "qualname": "RmseLoss.unit_loss", "kind": "function", "doc": "<p>This should be equivalent of forward, without sum over units\nCurrently only true if batch_weighting = 'data_filter'.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>pred (torch.tensor):</strong>  predicted data</li>\n<li><strong>target (torch.tensor):</strong>  target data</li>\n<li><strong>data_filters (torch.tensor):</strong>  data filters for each unit</li>\n<li><strong>temporal_normalize (bool):</strong>  whether to normalize by time steps</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>unitloss (torch.tensor): mean squared error loss</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">pred</span>, </span><span class=\"param\"><span class=\"n\">target</span>, </span><span class=\"param\"><span class=\"n\">data_filters</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">temporal_normalize</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}];

    // mirrored in build-search-index.js (part 1)
    // Also split on html tags. this is a cheap heuristic, but good enough.
    elasticlunr.tokenizer.setSeperator(/[\s\-.;&_'"=,()]+|<[^>]*>/);

    let searchIndex;
    if (docs._isPrebuiltIndex) {
        console.info("using precompiled search index");
        searchIndex = elasticlunr.Index.load(docs);
    } else {
        console.time("building search index");
        // mirrored in build-search-index.js (part 2)
        searchIndex = elasticlunr(function () {
            this.pipeline.remove(elasticlunr.stemmer);
            this.pipeline.remove(elasticlunr.stopWordFilter);
            this.addField("qualname");
            this.addField("fullname");
            this.addField("annotation");
            this.addField("default_value");
            this.addField("signature");
            this.addField("bases");
            this.addField("doc");
            this.setRef("fullname");
        });
        for (let doc of docs) {
            searchIndex.addDoc(doc);
        }
        console.timeEnd("building search index");
    }

    return (term) => searchIndex.search(term, {
        fields: {
            qualname: {boost: 4},
            fullname: {boost: 2},
            annotation: {boost: 2},
            default_value: {boost: 2},
            signature: {boost: 2},
            bases: {boost: 2},
            doc: {boost: 1},
        },
        expand: true
    });
})();